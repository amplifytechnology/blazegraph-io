<?xml version="1.0" encoding="UTF-8"?>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta name="pdf:PDFVersion" content="1.2" />
<meta name="pdf:docinfo:title" content="shannon1948.dvi" />
<meta name="xmp:CreatorTool" content="dvipsk 5.58f Copyright 1986, 1994 Radical Eye Software" />
<meta name="pdf:hasXFA" content="false" />
<meta name="access_permission:modify_annotations" content="true" />
<meta name="pdf:incrementalUpdateCount" content="0" />
<meta name="dcterms:created" content="1998-07-16T10:14:40Z" />
<meta name="dc:format" content="application/pdf; version=1.2" />
<meta name="pdf:docinfo:creator_tool" content="dvipsk 5.58f Copyright 1986, 1994 Radical Eye Software" />
<meta name="access_permission:fill_in_form" content="true" />
<meta name="pdf:hasCollection" content="false" />
<meta name="pdf:encrypted" content="false" />
<meta name="dc:title" content="shannon1948.dvi" />
<meta name="X-TIKA:versionCount" content="0" />
<meta name="pdf:hasMarkedContent" content="false" />
<meta name="Content-Type" content="application/pdf" />
<meta name="access_permission:can_print_faithful" content="true" />
<meta name="pdf:producer" content="Acrobat Distiller Command 3.01 for Solaris 2.3 and later (SPARC)" />
<meta name="access_permission:extract_for_accessibility" content="true" />
<meta name="access_permission:assemble_document" content="true" />
<meta name="xmpTPg:NPages" content="55" />
<meta name="pdf:hasXMP" content="false" />
<meta name="access_permission:extract_content" content="true" />
<meta name="access_permission:can_print" content="true" />
<meta name="pdf:eofOffsets" content="366296" />
<meta name="access_permission:can_modify" content="true" />
<meta name="pdf:docinfo:producer" content="Acrobat Distiller Command 3.01 for Solaris 2.3 and later (SPARC)" />
<meta name="pdf:docinfo:created" content="1998-07-16T10:14:40Z" />
<title>shannon1948.dvi</title>
</head>
<body><div class="page">
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         <p>
<span class="f1" data-bbox="56.0,56.5,216.2,4.5" data-line="0" data-segment="0">Reprinted with corrections from The Bell System Technical Journal,</span>
<span class="f1" data-bbox="56.0,65.1,166.4,4.5" data-line="1" data-segment="0">Vol. 27, pp. 379–423, 623–656, July, October, 1948.</span>
</p>
<p>
<span class="f3" data-bbox="181.0,127.9,249.3,8.0" data-line="0" data-segment="0">A Mathematical Theory of Communication</span>
</p>
<p>
<span class="f4" data-bbox="262.9,154.7,85.6,5.6" data-line="0" data-segment="0">By C. E. SHANNON</span>
</p>
<p>
<span class="f4" data-bbox="272.6,176.7,65.9,5.6" data-line="0" data-segment="0">INTRODUCTION</span>
</p>
<p>
<span class="f4" data-bbox="111.7,195.3,408.0,5.6" data-line="0" data-segment="0">HE recent development of various methods of modulation such as PCM and PPM which exchange</span>
<span class="f5" data-bbox="91.9,207.3,426.7,9.4" data-line="1" data-segment="0">Tbandwidth for signal-to-noise ratio has intensiﬁed the interest in a general theory of communication. A</span>
<span class="f6" data-bbox="372.6,215.7,57.2,4.1" data-line="2" data-segment="0">1 2</span>
<span class="f4" data-bbox="91.9,219.3,427.5,5.6" data-line="3" data-segment="0">basis for such a theory is contained in the important papers of Nyquist and Hartley on this subject. In the</span>
<span class="f4" data-bbox="91.9,231.2,427.5,5.6" data-line="4" data-segment="0">present paper we will extend the theory to include a number of new factors, in particular the effect of noise</span>
<span class="f4" data-bbox="91.9,243.2,427.4,5.6" data-line="5" data-segment="0">in the channel, and the savings possible due to the statistical structure of the original message and due to the</span>
<span class="f4" data-bbox="91.9,255.2,194.5,5.6" data-line="6" data-segment="0">nature of the ﬁnal destination of the information.</span>
<span class="f4" data-bbox="106.9,267.1,412.7,5.6" data-line="7" data-segment="0">The fundamental problem of communication is that of reproducing at one point either exactly or ap-</span>
<span class="f4" data-bbox="91.9,279.1,427.5,5.6" data-line="8" data-segment="0">proximately a message selected at another point. Frequently the messages have meaning; that is they refer</span>
<span class="f4" data-bbox="91.9,290.9,427.6,5.6" data-line="9" data-segment="0">to or are correlated according to some system with certain physical or conceptual entities. These semantic</span>
<span class="f4" data-bbox="91.9,302.9,427.5,5.6" data-line="10" data-segment="0">aspects of communication are irrelevant to the engineering problem. The signiﬁcant aspect is that the actual</span>
<span class="f4" data-bbox="91.9,314.9,427.4,5.6" data-line="11" data-segment="0">message is one selected from a set of possible messages. The system must be designed to operate for each</span>
<span class="f4" data-bbox="91.9,326.8,427.4,5.6" data-line="12" data-segment="0">possible selection, not just the one which will actually be chosen since this is unknown at the time of design.</span>
<span class="f4" data-bbox="106.9,338.8,412.7,5.6" data-line="13" data-segment="0">If the number of messages in the set is ﬁnite then this number or any monotonic function of this number</span>
<span class="f4" data-bbox="91.9,350.8,427.3,5.6" data-line="14" data-segment="0">can be regarded as a measure of the information produced when one message is chosen from the set, all</span>
<span class="f4" data-bbox="91.9,362.7,427.8,5.6" data-line="15" data-segment="0">choices being equally likely. As was pointed out by Hartley the most natural choice is the logarithmic</span>
<span class="f4" data-bbox="91.9,374.7,427.3,5.6" data-line="16" data-segment="0">function. Although this deﬁnition must be generalized considerably when we consider the inﬂuence of the</span>
<span class="f4" data-bbox="91.9,386.6,427.4,5.6" data-line="17" data-segment="0">statistics of the message and when we have a continuous range of messages, we will in all cases use an</span>
<span class="f4" data-bbox="91.9,398.6,128.8,5.6" data-line="18" data-segment="0">essentially logarithmic measure.</span>
<span class="f4" data-bbox="106.9,410.6,258.3,5.6" data-line="19" data-segment="0">The logarithmic measure is more convenient for various reasons:</span>
</p>
<p>
<span class="f4" data-bbox="104.4,430.5,415.9,5.6" data-line="0" data-segment="0">1. It is practically more useful. Parameters of engineering importance such as time, bandwidth, number</span>
<span class="f4" data-bbox="116.9,442.4,402.7,5.6" data-line="1" data-segment="0">of relays, etc., tend to vary linearly with the logarithm of the number of possibilities. For example,</span>
<span class="f4" data-bbox="116.9,454.4,402.4,5.6" data-line="2" data-segment="0">adding one relay to a group doubles the number of possible states of the relays. It adds 1 to the base 2</span>
<span class="f4" data-bbox="116.9,466.4,402.5,5.6" data-line="3" data-segment="0">logarithm of this number. Doubling the time roughly squares the number of possible messages, or</span>
<span class="f4" data-bbox="116.9,478.3,106.1,5.6" data-line="4" data-segment="0">doubles the logarithm, etc.</span>
</p>
<p>
<span class="f4" data-bbox="104.4,498.2,414.8,5.6" data-line="0" data-segment="0">2. It is nearer to our intuitive feeling as to the proper measure. This is closely related to (1) since we in-</span>
<span class="f4" data-bbox="116.9,510.2,402.4,5.6" data-line="1" data-segment="0">tuitively measures entities by linear comparison with common standards. One feels, for example, that</span>
<span class="f4" data-bbox="116.9,522.2,402.3,5.6" data-line="2" data-segment="0">two punched cards should have twice the capacity of one for information storage, and two identical</span>
<span class="f4" data-bbox="116.9,534.1,253.9,5.6" data-line="3" data-segment="0">channels twice the capacity of one for transmitting information.</span>
</p>
<p>
<span class="f4" data-bbox="104.4,554.0,415.1,5.6" data-line="0" data-segment="0">3. It is mathematically more suitable. Many of the limiting operations are simple in terms of the loga-</span>
<span class="f4" data-bbox="116.9,566.0,330.9,5.6" data-line="1" data-segment="0">rithm but would require clumsy restatement in terms of the number of possibilities.</span>
</p>
<p>
<span class="f4" data-bbox="106.9,585.9,412.5,5.6" data-line="0" data-segment="0">The choice of a logarithmic base corresponds to the choice of a unit for measuring information. If the</span>
<span class="f4" data-bbox="91.9,597.8,427.5,5.6" data-line="1" data-segment="0">base 2 is used the resulting units may be called binary digits, or more brieﬂy bits, a word suggested by</span>
<span class="f4" data-bbox="91.9,609.8,427.4,5.6" data-line="2" data-segment="0">J. W. Tukey. A device with two stable positions, such as a relay or a ﬂip-ﬂop circuit, can store one bit of</span>
<span class="f8" data-bbox="444.7,618.2,52.5,4.1" data-line="3" data-segment="0">N N</span>
<span class="f4" data-bbox="91.9,621.8,427.4,5.6" data-line="4" data-segment="0">information. N such devices can store N bits, since the total number of possible states is 2 and log 2 = N.</span>
<span class="f6" data-bbox="481.9,624.3,3.7,4.1" data-line="5" data-segment="0">2</span>
<span class="f4" data-bbox="91.9,633.7,264.6,5.6" data-line="6" data-segment="0">If the base 10 is used the units may be called decimal digits. Since</span>
</p>
<p>
<span class="f4" data-bbox="254.6,655.6,102.1,5.6" data-line="0" data-segment="0">log M = log M=log 2</span>
<span class="f6" data-bbox="267.4,658.1,82.8,4.1" data-line="1" data-segment="0">2 10 10</span>
<span class="f9" data-bbox="283.6,670.6,61.8,5.6" data-line="2" data-segment="0">= 3:32 log M;</span>
<span class="f6" data-bbox="325.0,673.1,7.4,4.1" data-line="3" data-segment="0">10</span>
<span class="f11" data-bbox="102.8,687.2,3.0,3.3" data-line="4" data-segment="0">1</span>
<span class="f1" data-bbox="106.3,690.1,413.0,4.5" data-line="5" data-segment="0">Nyquist, H., “Certain Factors Affecting Telegraph Speed,” Bell System Technical Journal, April 1924, p. 324; “Certain Topics in</span>
<span class="f1" data-bbox="91.9,699.5,239.0,4.5" data-line="6" data-segment="0">Telegraph Transmission Theory,” A.I.E.E. Trans., v. 47, April 1928, p. 617.</span>
<span class="f11" data-bbox="102.8,706.5,3.0,3.3" data-line="7" data-segment="0">2</span>
<span class="f1" data-bbox="106.3,709.4,315.1,4.5" data-line="8" data-segment="0">Hartley, R. V. L., “Transmission of Information,” Bell System Technical Journal, July 1928, p. 535.</span>
</p>
<p>
<span class="f4" data-bbox="303.1,744.5,5.0,5.6" data-line="0" data-segment="0">1</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      <p>
<span class="f6" data-bbox="145.2,100.4,54.2,4.0" data-line="0" data-segment="0">INFORMATION</span>
<span class="f6" data-bbox="156.8,108.3,311.3,4.0" data-line="1" data-segment="0">SOURCE TRANSMITTER RECEIVER DESTINATION</span>
</p>
<p>
<span class="f6" data-bbox="263.0,140.3,93.9,4.0" data-line="0" data-segment="0">SIGNAL RECEIVED</span>
<span class="f6" data-bbox="323.0,147.3,28.4,4.0" data-line="1" data-segment="0">SIGNAL</span>
<span class="f6" data-bbox="182.3,161.3,248.0,4.0" data-line="2" data-segment="0">MESSAGE MESSAGE</span>
</p>
<p>
<span class="f6" data-bbox="297.7,226.4,23.0,4.0" data-line="0" data-segment="0">NOISE</span>
<span class="f6" data-bbox="293.9,233.3,30.7,4.0" data-line="1" data-segment="0">SOURCE</span>
<span class="f12" data-bbox="189.2,248.9,232.6,5.0" data-line="2" data-segment="0">Fig. 1 — Schematic diagram of a general communication system.</span>
</p>
<p>
<span class="f6" data-bbox="196.3,276.8,3.7,4.1" data-line="0" data-segment="0">1</span>
<span class="f4" data-bbox="91.9,280.7,427.6,5.6" data-line="1" data-segment="0">a decimal digit is about 3 bits. A digit wheel on a desk computing machine has ten stable positions and</span>
<span class="f6" data-bbox="196.3,284.2,3.7,4.1" data-line="2" data-segment="0">3</span>
<span class="f4" data-bbox="91.9,292.7,427.7,5.6" data-line="3" data-segment="0">therefore has a storage capacity of one decimal digit. In analytical work where integration and differentiation</span>
<span class="f4" data-bbox="91.9,304.6,427.3,5.6" data-line="4" data-segment="0">are involved the base e is sometimes useful. The resulting units of information will be called natural units.</span>
<span class="f4" data-bbox="91.9,316.6,296.1,5.6" data-line="5" data-segment="0">Change from the base a to base b merely requires multiplication by log a.</span>
<span class="f8" data-bbox="375.2,319.1,3.7,4.1" data-line="6" data-segment="0">b</span>
<span class="f4" data-bbox="106.9,328.6,412.4,5.6" data-line="7" data-segment="0">By a communication system we will mean a system of the type indicated schematically in Fig. 1. It</span>
<span class="f4" data-bbox="91.9,340.5,128.6,5.6" data-line="8" data-segment="0">consists of essentially ﬁve parts:</span>
</p>
<p>
<span class="f4" data-bbox="104.4,360.4,414.9,5.6" data-line="0" data-segment="0">1. An information source which produces a message or sequence of messages to be communicated to the</span>
<span class="f4" data-bbox="116.9,372.4,402.8,5.6" data-line="1" data-segment="0">receiving terminal. The message may be of various types: (a) A sequence of letters as in a telegraph</span>
<span class="f4" data-bbox="116.9,384.4,402.5,5.6" data-line="2" data-segment="0">of teletype system; (b) A single function of time f (t) as in radio or telephony; (c) A function of</span>
<span class="f4" data-bbox="116.9,396.3,401.8,5.6" data-line="3" data-segment="0">time and other variables as in black and white television — here the message may be thought of as a</span>
<span class="f4" data-bbox="116.9,408.3,402.5,5.6" data-line="4" data-segment="0">function f (x;y;t) of two space coordinates and time, the light intensity at point (x;y) and time t on a</span>
<span class="f4" data-bbox="116.9,420.2,402.6,5.6" data-line="5" data-segment="0">pickup tube plate; (d) Two or more functions of time, say f (t), g(t), h(t) — this is the case in “three-</span>
<span class="f4" data-bbox="116.9,432.2,402.5,5.6" data-line="6" data-segment="0">dimensional” sound transmission or if the system is intended to service several individual channels in</span>
<span class="f4" data-bbox="116.9,444.2,402.5,5.6" data-line="7" data-segment="0">multiplex; (e) Several functions of several variables — in color television the message consists of three</span>
<span class="f4" data-bbox="116.9,456.1,402.4,5.6" data-line="8" data-segment="0">functions f (x;y;t), g(x;y;t), h(x;y;t) deﬁned in a three-dimensional continuum — we may also think</span>
<span class="f4" data-bbox="116.9,468.1,402.5,5.6" data-line="9" data-segment="0">of these three functions as components of a vector ﬁeld deﬁned in the region — similarly, several</span>
<span class="f4" data-bbox="116.9,480.1,402.1,5.6" data-line="10" data-segment="0">black and white television sources would produce “messages” consisting of a number of functions</span>
<span class="f4" data-bbox="116.9,491.9,402.3,5.6" data-line="11" data-segment="0">of three variables; (f) Various combinations also occur, for example in television with an associated</span>
<span class="f4" data-bbox="116.9,503.9,58.3,5.6" data-line="12" data-segment="0">audio channel.</span>
</p>
<p>
<span class="f4" data-bbox="104.4,523.9,415.1,5.6" data-line="0" data-segment="0">2. A transmitter which operates on the message in some way to produce a signal suitable for trans-</span>
<span class="f4" data-bbox="116.9,535.9,402.7,5.6" data-line="1" data-segment="0">mission over the channel. In telephony this operation consists merely of changing sound pressure</span>
<span class="f4" data-bbox="116.9,547.7,403.4,5.6" data-line="2" data-segment="0">into a proportional electrical current. In telegraphy we have an encoding operation which produces</span>
<span class="f4" data-bbox="116.9,559.7,403.6,5.6" data-line="3" data-segment="0">a sequence of dots, dashes and spaces on the channel corresponding to the message. In a multiplex</span>
<span class="f4" data-bbox="116.9,571.6,402.6,5.6" data-line="4" data-segment="0">PCM system the different speech functions must be sampled, compressed, quantized and encoded,</span>
<span class="f4" data-bbox="116.9,583.6,402.9,5.6" data-line="5" data-segment="0">and ﬁnally interleaved properly to construct the signal. Vocoder systems, television and frequency</span>
<span class="f4" data-bbox="116.9,595.6,388.6,5.6" data-line="6" data-segment="0">modulation are other examples of complex operations applied to the message to obtain the signal.</span>
</p>
<p>
<span class="f4" data-bbox="104.4,615.5,415.1,5.6" data-line="0" data-segment="0">3. The channel is merely the medium used to transmit the signal from transmitter to receiver. It may be</span>
<span class="f4" data-bbox="116.9,627.4,318.9,5.6" data-line="1" data-segment="0">a pair of wires, a coaxial cable, a band of radio frequencies, a beam of light, etc.</span>
</p>
<p>
<span class="f4" data-bbox="104.4,647.3,415.4,5.6" data-line="0" data-segment="0">4. The receiver ordinarily performs the inverse operation of that done by the transmitter, reconstructing</span>
<span class="f4" data-bbox="116.9,659.3,113.5,5.6" data-line="1" data-segment="0">the message from the signal.</span>
</p>
<p>
<span class="f4" data-bbox="104.4,679.3,308.5,5.6" data-line="0" data-segment="0">5. The destination is the person (or thing) for whom the message is intended.</span>
</p>
<p>
<span class="f4" data-bbox="106.9,699.2,412.6,5.6" data-line="0" data-segment="0">We wish to consider certain general problems involving communication systems. To do this it is ﬁrst</span>
<span class="f4" data-bbox="91.9,711.2,427.3,5.6" data-line="1" data-segment="0">necessary to represent the various elements involved as mathematical entities, suitably idealized from their</span>
</p>
<p>
<span class="f4" data-bbox="303.1,744.5,5.0,5.6" data-line="0" data-segment="0">2</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         <p>
<span class="f4" data-bbox="91.9,99.9,427.3,5.6" data-line="0" data-segment="0">physical counterparts. We may roughly classify communication systems into three main categories: discrete,</span>
<span class="f4" data-bbox="91.9,111.9,427.3,5.6" data-line="1" data-segment="0">continuous and mixed. By a discrete system we will mean one in which both the message and the signal</span>
<span class="f4" data-bbox="91.9,123.9,427.4,5.6" data-line="2" data-segment="0">are a sequence of discrete symbols. A typical case is telegraphy where the message is a sequence of letters</span>
<span class="f4" data-bbox="91.9,135.8,427.6,5.6" data-line="3" data-segment="0">and the signal a sequence of dots, dashes and spaces. A continuous system is one in which the message and</span>
<span class="f4" data-bbox="91.9,147.8,427.3,5.6" data-line="4" data-segment="0">signal are both treated as continuous functions, e.g., radio or television. A mixed system is one in which</span>
<span class="f4" data-bbox="91.9,159.8,322.6,5.6" data-line="5" data-segment="0">both discrete and continuous variables appear, e.g., PCM transmission of speech.</span>
<span class="f4" data-bbox="106.9,171.7,412.7,5.6" data-line="6" data-segment="0">We ﬁrst consider the discrete case. This case has applications not only in communication theory, but</span>
<span class="f4" data-bbox="91.9,183.7,427.8,5.6" data-line="7" data-segment="0">also in the theory of computing machines, the design of telephone exchanges and other ﬁelds. In addition</span>
<span class="f4" data-bbox="91.9,195.5,427.5,5.6" data-line="8" data-segment="0">the discrete case forms a foundation for the continuous and mixed cases which will be treated in the second</span>
<span class="f4" data-bbox="91.9,207.5,67.6,5.6" data-line="9" data-segment="0">half of the paper.</span>
</p>
<p>
<span class="f13" data-bbox="194.6,233.5,222.2,6.7" data-line="0" data-segment="0">PART I: DISCRETE NOISELESS SYSTEMS</span>
</p>
<p>
<span class="f4" data-bbox="219.2,257.3,172.8,5.6" data-line="0" data-segment="0">1. THE DISCRETE NOISELESS CHANNEL</span>
</p>
<p>
<span class="f4" data-bbox="91.9,276.1,427.3,5.6" data-line="0" data-segment="0">Teletype and telegraphy are two simple examples of a discrete channel for transmitting information. Gen-</span>
<span class="f4" data-bbox="91.9,288.1,427.7,5.6" data-line="1" data-segment="0">erally, a discrete channel will mean a system whereby a sequence of choices from a ﬁnite set of elementary</span>
<span class="f4" data-bbox="91.9,299.9,427.6,5.6" data-line="2" data-segment="0">symbols S1;:::;Sn can be transmitted from one point to another. Each of the symbols Si is assumed to have</span>
<span class="f4" data-bbox="91.9,311.9,427.6,5.6" data-line="3" data-segment="0">a certain duration in time ti seconds (not necessarily the same for different Si, for example the dots and</span>
<span class="f4" data-bbox="91.9,323.8,427.6,5.6" data-line="4" data-segment="0">dashes in telegraphy). It is not required that all possible sequences of the Si be capable of transmission on</span>
<span class="f4" data-bbox="91.9,335.8,427.6,5.6" data-line="5" data-segment="0">the system; certain sequences only may be allowed. These will be possible signals for the channel. Thus</span>
<span class="f4" data-bbox="91.9,347.8,427.3,5.6" data-line="6" data-segment="0">in telegraphy suppose the symbols are: (1) A dot, consisting of line closure for a unit of time and then line</span>
<span class="f4" data-bbox="91.9,359.7,427.3,5.6" data-line="7" data-segment="0">open for a unit of time; (2) A dash, consisting of three time units of closure and one unit open; (3) A letter</span>
<span class="f4" data-bbox="91.9,371.7,427.6,5.6" data-line="8" data-segment="0">space consisting of, say, three units of line open; (4) A word space of six units of line open. We might place</span>
<span class="f4" data-bbox="91.9,383.7,427.1,5.6" data-line="9" data-segment="0">the restriction on allowable sequences that no spaces follow each other (for if two letter spaces are adjacent,</span>
<span class="f4" data-bbox="91.9,395.6,427.6,5.6" data-line="10" data-segment="0">it is identical with a word space). The question we now consider is how one can measure the capacity of</span>
<span class="f4" data-bbox="91.9,407.6,156.5,5.6" data-line="11" data-segment="0">such a channel to transmit information.</span>
<span class="f4" data-bbox="106.9,419.5,412.7,5.6" data-line="12" data-segment="0">In the teletype case where all symbols are of the same duration, and any sequence of the 32 symbols</span>
<span class="f4" data-bbox="91.9,431.5,427.4,5.6" data-line="13" data-segment="0">is allowed the answer is easy. Each symbol represents ﬁve bits of information. If the system transmits n</span>
<span class="f4" data-bbox="91.9,443.5,427.6,5.6" data-line="14" data-segment="0">symbols per second it is natural to say that the channel has a capacity of 5n bits per second. This does not</span>
<span class="f4" data-bbox="91.9,455.3,427.5,5.6" data-line="15" data-segment="0">mean that the teletype channel will always be transmitting information at this rate — this is the maximum</span>
<span class="f4" data-bbox="91.9,467.3,427.6,5.6" data-line="16" data-segment="0">possible rate and whether or not the actual rate reaches this maximum depends on the source of information</span>
<span class="f4" data-bbox="91.9,479.3,178.4,5.6" data-line="17" data-segment="0">which feeds the channel, as will appear later.</span>
<span class="f4" data-bbox="106.9,491.2,412.3,5.6" data-line="18" data-segment="0">In the more general case with different lengths of symbols and constraints on the allowed sequences, we</span>
<span class="f4" data-bbox="91.9,503.2,120.8,5.6" data-line="19" data-segment="0">make the following deﬁnition:</span>
<span class="f4" data-bbox="91.9,515.1,236.8,5.6" data-line="20" data-segment="0">Deﬁnition: The capacity C of a discrete channel is given by</span>
</p>
<p>
<span class="f4" data-bbox="306.5,535.7,35.5,5.6" data-line="0" data-segment="0">log N(T )</span>
<span class="f7" data-bbox="267.5,542.5,36.2,5.6" data-line="1" data-segment="0">C = Lim</span>
<span class="f8" data-bbox="286.6,548.9,39.9,5.9" data-line="2" data-segment="0">T !∞ T</span>
</p>
<p>
<span class="f4" data-bbox="91.9,568.4,238.3,5.6" data-line="0" data-segment="0">where N(T ) is the number of allowed signals of duration T .</span>
<span class="f4" data-bbox="106.9,580.3,412.4,5.6" data-line="1" data-segment="0">It is easily seen that in the teletype case this reduces to the previous result. It can be shown that the limit</span>
<span class="f4" data-bbox="91.9,592.3,427.4,5.6" data-line="2" data-segment="0">in question will exist as a ﬁnite number in most cases of interest. Suppose all sequences of the symbols</span>
<span class="f7" data-bbox="91.9,604.3,427.3,5.6" data-line="3" data-segment="0">S1;:::;Sn are allowed and these symbols have durations t1;:::;tn. What is the channel capacity? If N(t)</span>
<span class="f4" data-bbox="91.9,616.1,229.1,5.6" data-line="4" data-segment="0">represents the number of sequences of duration t we have</span>
</p>
<p>
<span class="f7" data-bbox="215.0,638.1,181.2,8.5" data-line="0" data-segment="0">N(t) = N(t  t1) + N(t  t2) +    + N(t  tn):</span>
</p>
<p>
<span class="f4" data-bbox="91.9,660.1,427.4,5.6" data-line="0" data-segment="0">The total number is equal to the sum of the numbers of sequences ending in S1;S2;:::;Sn and these are</span>
<span class="f7" data-bbox="91.9,671.9,427.3,8.5" data-line="1" data-segment="0">N(t  t1);N(t  t2);:::;N(t  tn), respectively. According to a well-known result in ﬁnite differences, N(t)</span>
<span class="f8" data-bbox="226.1,680.3,2.1,4.1" data-line="2" data-segment="0">t</span>
<span class="f4" data-bbox="91.9,683.9,402.2,5.6" data-line="3" data-segment="0">is then asymptotic for large t to X where X0 is the largest real solution of the characteristic equation:</span>
<span class="f6" data-bbox="225.1,686.9,3.7,4.1" data-line="4" data-segment="0">0</span>
<span class="f14" data-bbox="255.2,701.7,89.9,4.8" data-line="5" data-segment="0"> t1  t2  tn</span>
<span class="f7" data-bbox="248.2,705.9,114.9,8.5" data-line="6" data-segment="0">X + X +    + X = 1</span>
</p>
<p>
<span class="f4" data-bbox="303.1,744.5,5.0,5.6" data-line="0" data-segment="0">3</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          <p>
<span class="f4" data-bbox="91.9,99.9,53.1,5.6" data-line="0" data-segment="0">and therefore</span>
<span class="f7" data-bbox="282.6,111.9,45.6,5.6" data-line="1" data-segment="0">C = log X0:</span>
</p>
<p>
<span class="f4" data-bbox="106.9,129.7,412.3,5.6" data-line="0" data-segment="0">In case there are restrictions on allowed sequences we may still often obtain a difference equation of this</span>
<span class="f4" data-bbox="91.9,141.5,353.1,5.6" data-line="1" data-segment="0">type and ﬁnd C from the characteristic equation. In the telegraphy case mentioned above</span>
</p>
<p>
<span class="f7" data-bbox="160.7,163.1,290.0,8.5" data-line="0" data-segment="0">N(t) = N(t  2) + N(t  4) + N(t  5) + N(t  7) + N(t  8) + N(t  10)</span>
</p>
<p>
<span class="f4" data-bbox="91.9,184.6,427.5,5.6" data-line="0" data-segment="0">as we see by counting sequences of symbols according to the last or next to the last symbol occurring.</span>
<span class="f6" data-bbox="322.6,193.0,110.7,4.1" data-line="1" data-segment="0">2 4 5 7 8 10</span>
<span class="f4" data-bbox="91.9,196.6,427.5,8.5" data-line="2" data-segment="0">Hence C is  log  0 where  0 is the positive root of 1 =   +  +  +  +  +  . Solving this we ﬁnd</span>
<span class="f7" data-bbox="91.4,208.6,44.4,5.6" data-line="3" data-segment="0">C = 0:539.</span>
<span class="f4" data-bbox="106.9,220.5,412.5,5.6" data-line="4" data-segment="0">A very general type of restriction which may be placed on allowed sequences is the following: We</span>
<span class="f4" data-bbox="91.9,232.5,426.9,5.6" data-line="5" data-segment="0">imagine a number of possible states a1;a2;:::;am. For each state only certain symbols from the set S1;:::;Sn</span>
<span class="f4" data-bbox="91.9,244.4,427.2,5.6" data-line="6" data-segment="0">can be transmitted (different subsets for the different states). When one of these has been transmitted the</span>
<span class="f4" data-bbox="91.9,256.4,427.5,5.6" data-line="7" data-segment="0">state changes to a new state depending both on the old state and the particular symbol transmitted. The</span>
<span class="f4" data-bbox="91.9,268.4,427.2,5.6" data-line="8" data-segment="0">telegraph case is a simple example of this. There are two states depending on whether or not a space was</span>
<span class="f4" data-bbox="91.9,280.3,427.7,5.6" data-line="9" data-segment="0">the last symbol transmitted. If so, then only a dot or a dash can be sent next and the state always changes.</span>
<span class="f4" data-bbox="91.9,292.3,427.3,5.6" data-line="10" data-segment="0">If not, any symbol can be transmitted and the state changes if a space is sent, otherwise it remains the same.</span>
<span class="f4" data-bbox="91.9,304.3,427.5,5.6" data-line="11" data-segment="0">The conditions can be indicated in a linear graph as shown in Fig. 2. The junction points correspond to the</span>
<span class="f6" data-bbox="273.4,319.5,20.7,4.0" data-line="12" data-segment="0">DASH</span>
</p>
<p>
<span class="f6" data-bbox="276.0,339.5,77.4,5.0" data-line="0" data-segment="0">DOT DOT</span>
</p>
<p>
<span class="f6" data-bbox="256.3,379.5,99.8,5.0" data-line="0" data-segment="0">LETTER SPACE DASH</span>
</p>
<p>
<span class="f6" data-bbox="259.1,400.5,49.3,4.0" data-line="0" data-segment="0">WORD SPACE</span>
<span class="f12" data-bbox="172.9,415.1,265.2,5.0" data-line="1" data-segment="0">Fig. 2 — Graphical representation of the constraints on telegraph symbols.</span>
</p>
<p>
<span class="f4" data-bbox="91.9,438.7,427.4,5.6" data-line="0" data-segment="0">states and the lines indicate the symbols possible in a state and the resulting state. In Appendix 1 it is shown</span>
<span class="f4" data-bbox="91.9,450.7,427.5,5.6" data-line="1" data-segment="0">that if the conditions on allowed sequences can be described in this form C will exist and can be calculated</span>
<span class="f4" data-bbox="91.9,462.5,158.4,5.6" data-line="2" data-segment="0">in accordance with the following result:</span>
<span class="f18" data-bbox="178.8,474.2,110.9,6.0" data-line="3" data-segment="0">(s) th</span>
<span class="f7" data-bbox="106.9,479.6,412.3,5.7" data-line="4" data-segment="0">Theorem 1: Let b be the duration of the s symbol which is allowable in state i and leads to state j.</span>
<span class="f8" data-bbox="178.8,482.5,5.2,4.1" data-line="5" data-segment="0">i j</span>
<span class="f4" data-bbox="91.9,491.6,424.2,5.7" data-line="6" data-segment="0">Then the channel capacity C is equal to logW where W is the largest real root of the determinant equation:</span>
<span class="f19" data-bbox="264.5,506.2,65.2,14.9" data-line="7" data-segment="0">   </span>
<span class="f21" data-bbox="297.6,508.7,6.8,3.3" data-line="8" data-segment="0">(s)</span>
<span class="f19" data-bbox="264.5,512.2,65.2,14.9" data-line="9" data-segment="0">   b  </span>
<span class="f17" data-bbox="297.6,515.3,4.2,3.3" data-line="10" data-segment="0">i j</span>
<span class="f19" data-bbox="264.5,517.6,82.3,15.4" data-line="11" data-segment="0"> ∑W   i j   = 0</span>
<span class="f8" data-bbox="271.4,527.3,2.9,4.1" data-line="12" data-segment="0">s</span>
</p>
<p>
<span class="f4" data-bbox="91.9,545.8,174.3,5.7" data-line="0" data-segment="0">where  i j = 1 if i = j and is zero otherwise.</span>
<span class="f4" data-bbox="106.9,560.8,245.1,5.6" data-line="1" data-segment="0">For example, in the telegraph case (Fig. 2) the determinant is:</span>
<span class="f19" data-bbox="223.6,573.8,144.4,14.9" data-line="2" data-segment="0">   </span>
<span class="f19" data-bbox="223.6,578.5,144.4,16.3" data-line="3" data-segment="0">   2  4  </span>
<span class="f16" data-bbox="248.5,582.1,108.2,8.5" data-line="4" data-segment="0"> 1 (W +W )</span>
<span class="f19" data-bbox="223.6,585.8,144.4,14.9" data-line="5" data-segment="0">   </span>
<span class="f9" data-bbox="370.1,588.2,17.8,5.6" data-line="6" data-segment="0">= 0:</span>
<span class="f19" data-bbox="223.6,590.7,144.4,15.9" data-line="7" data-segment="0">   3  6  2  4  </span>
<span class="f9" data-bbox="226.8,594.3,137.9,8.5" data-line="8" data-segment="0">(W +W ) (W +W  1)</span>
</p>
<p>
<span class="f4" data-bbox="91.9,615.3,259.7,5.6" data-line="0" data-segment="0">On expansion this leads to the equation given above for this case.</span>
</p>
<p>
<span class="f4" data-bbox="209.5,636.2,192.3,5.6" data-line="0" data-segment="0">2. THE DISCRETE SOURCE OF INFORMATION</span>
</p>
<p>
<span class="f4" data-bbox="91.9,654.9,427.1,5.6" data-line="0" data-segment="0">We have seen that under very general conditions the logarithm of the number of possible signals in a discrete</span>
<span class="f4" data-bbox="91.9,666.8,427.3,5.6" data-line="1" data-segment="0">channel increases linearly with time. The capacity to transmit information can be speciﬁed by giving this</span>
<span class="f4" data-bbox="91.9,678.8,366.0,5.6" data-line="2" data-segment="0">rate of increase, the number of bits per second required to specify the particular signal used.</span>
<span class="f4" data-bbox="106.9,690.8,412.5,5.6" data-line="3" data-segment="0">We now consider the information source. How is an information source to be described mathematically,</span>
<span class="f4" data-bbox="91.9,702.7,427.3,5.6" data-line="4" data-segment="0">and how much information in bits per second is produced in a given source? The main point at issue is the</span>
<span class="f4" data-bbox="91.9,714.7,427.3,5.6" data-line="5" data-segment="0">effect of statistical knowledge about the source in reducing the required capacity of the channel, by the use</span>
</p>
<p>
<span class="f4" data-bbox="303.1,744.5,5.0,5.6" data-line="0" data-segment="0">4</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         <p>
<span class="f4" data-bbox="91.9,99.9,427.4,5.6" data-line="0" data-segment="0">of proper encoding of the information. In telegraphy, for example, the messages to be transmitted consist of</span>
<span class="f4" data-bbox="91.9,111.9,427.4,5.6" data-line="1" data-segment="0">sequences of letters. These sequences, however, are not completely random. In general, they form sentences</span>
<span class="f4" data-bbox="91.9,123.9,427.9,5.6" data-line="2" data-segment="0">and have the statistical structure of, say, English. The letter E occurs more frequently than Q, the sequence</span>
<span class="f4" data-bbox="91.9,135.8,427.6,5.6" data-line="3" data-segment="0">TH more frequently than XP, etc. The existence of this structure allows one to make a saving in time (or</span>
<span class="f4" data-bbox="91.9,147.8,427.4,5.6" data-line="4" data-segment="0">channel capacity) by properly encoding the message sequences into signal sequences. This is already done</span>
<span class="f4" data-bbox="91.9,159.8,426.6,5.6" data-line="5" data-segment="0">to a limited extent in telegraphy by using the shortest channel symbol, a dot, for the most common English</span>
<span class="f4" data-bbox="91.9,171.7,426.7,5.6" data-line="6" data-segment="0">letter E; while the infrequent letters, Q, X, Z are represented by longer sequences of dots and dashes. This</span>
<span class="f4" data-bbox="91.9,183.7,427.6,5.6" data-line="7" data-segment="0">idea is carried still further in certain commercial codes where common words and phrases are represented</span>
<span class="f4" data-bbox="91.9,195.5,427.4,5.6" data-line="8" data-segment="0">by four- or ﬁve-letter code groups with a considerable saving in average time. The standardized greeting</span>
<span class="f4" data-bbox="91.9,207.5,427.4,5.6" data-line="9" data-segment="0">and anniversary telegrams now in use extend this to the point of encoding a sentence or two into a relatively</span>
<span class="f4" data-bbox="91.9,219.5,109.1,5.6" data-line="10" data-segment="0">short sequence of numbers.</span>
<span class="f4" data-bbox="106.9,231.4,412.1,5.6" data-line="11" data-segment="0">We can think of a discrete source as generating the message, symbol by symbol. It will choose succes-</span>
<span class="f4" data-bbox="91.9,243.4,427.4,5.6" data-line="12" data-segment="0">sive symbols according to certain probabilities depending, in general, on preceding choices as well as the</span>
<span class="f4" data-bbox="91.9,255.4,427.6,5.6" data-line="13" data-segment="0">particular symbols in question. A physical system, or a mathematical model of a system which produces</span>
<span class="f6" data-bbox="478.3,263.7,3.7,4.1" data-line="14" data-segment="0">3</span>
<span class="f4" data-bbox="91.9,267.3,427.5,5.6" data-line="15" data-segment="0">such a sequence of symbols governed by a set of probabilities, is known as a stochastic process. We may</span>
<span class="f4" data-bbox="91.9,279.3,427.2,5.6" data-line="16" data-segment="0">consider a discrete source, therefore, to be represented by a stochastic process. Conversely, any stochastic</span>
<span class="f4" data-bbox="91.9,291.2,427.3,5.6" data-line="17" data-segment="0">process which produces a discrete sequence of symbols chosen from a ﬁnite set may be considered a discrete</span>
<span class="f4" data-bbox="91.9,303.2,156.9,5.6" data-line="18" data-segment="0">source. This will include such cases as:</span>
</p>
<p>
<span class="f4" data-bbox="104.4,323.1,256.4,5.6" data-line="0" data-segment="0">1. Natural written languages such as English, German, Chinese.</span>
</p>
<p>
<span class="f4" data-bbox="104.4,343.0,414.8,5.6" data-line="0" data-segment="0">2. Continuous information sources that have been rendered discrete by some quantizing process. For</span>
<span class="f4" data-bbox="116.9,355.0,350.1,5.6" data-line="1" data-segment="0">example, the quantized speech from a PCM transmitter, or a quantized television signal.</span>
</p>
<p>
<span class="f4" data-bbox="104.4,374.9,414.8,5.6" data-line="0" data-segment="0">3. Mathematical cases where we merely deﬁne abstractly a stochastic process which generates a se-</span>
<span class="f4" data-bbox="116.9,386.8,297.0,5.6" data-line="1" data-segment="0">quence of symbols. The following are examples of this last type of source.</span>
</p>
<p>
<span class="f4" data-bbox="120.0,406.9,399.5,5.6" data-line="0" data-segment="0">(A) Suppose we have ﬁve letters A, B, C, D, E which are chosen each with probability .2, successive</span>
<span class="f4" data-bbox="138.7,418.7,380.7,5.6" data-line="1" data-segment="0">choices being independent. This would lead to a sequence of which the following is a typical</span>
<span class="f4" data-bbox="138.7,430.7,36.3,5.6" data-line="2" data-segment="0">example.</span>
<span class="f4" data-bbox="138.7,444.7,200.6,5.6" data-line="3" data-segment="0">B D C B C E C C C A D C B D D A A E C E E A</span>
<span class="f4" data-bbox="138.7,456.7,200.9,5.6" data-line="4" data-segment="0">A B B D A E E C A C E E B A E E C B C E A D.</span>
<span class="f6" data-bbox="396.2,467.0,3.7,4.1" data-line="5" data-segment="0">4</span>
<span class="f4" data-bbox="138.7,470.6,257.6,5.6" data-line="6" data-segment="0">This was constructed with the use of a table of random numbers.</span>
</p>
<p>
<span class="f4" data-bbox="120.5,486.5,399.3,5.6" data-line="0" data-segment="0">(B) Using the same ﬁve letters let the probabilities be .4, .1, .2, .2, .1, respectively, with successive</span>
<span class="f4" data-bbox="138.7,498.4,258.5,5.6" data-line="1" data-segment="0">choices independent. A typical message from this source is then:</span>
<span class="f4" data-bbox="138.7,512.5,186.3,5.6" data-line="2" data-segment="0">A A A C D C B D C E A A D A D A C E D A</span>
<span class="f4" data-bbox="138.7,524.3,188.4,5.6" data-line="3" data-segment="0">E A D C A B E D A D D C E C A A A A A D.</span>
</p>
<p>
<span class="f4" data-bbox="120.5,540.3,399.0,5.6" data-line="0" data-segment="0">(C) A more complicated structure is obtained if successive symbols are not chosen independently</span>
<span class="f4" data-bbox="138.7,552.3,380.4,5.6" data-line="1" data-segment="0">but their probabilities depend on preceding letters. In the simplest case of this type a choice</span>
<span class="f4" data-bbox="138.7,564.2,380.5,5.6" data-line="2" data-segment="0">depends only on the preceding letter and not on ones before that. The statistical structure can</span>
<span class="f4" data-bbox="138.7,576.2,380.8,5.6" data-line="3" data-segment="0">then be described by a set of transition probabilities pi( j), the probability that letter i is followed</span>
<span class="f4" data-bbox="138.7,588.2,380.7,5.6" data-line="4" data-segment="0">by letter j. The indices i and j range over all the possible symbols. A second equivalent way of</span>
<span class="f4" data-bbox="138.7,600.1,380.7,5.6" data-line="5" data-segment="0">specifying the structure is to give the “digram” probabilities p(i; j), i.e., the relative frequency of</span>
<span class="f4" data-bbox="138.7,612.1,380.8,5.6" data-line="6" data-segment="0">the digram i j. The letter frequencies p(i), (the probability of letter i), the transition probabilities</span>
</p>
<p>
<span class="f11" data-bbox="102.8,627.5,3.0,3.3" data-line="0" data-segment="0">3</span>
<span class="f1" data-bbox="106.3,630.4,413.0,4.5" data-line="1" data-segment="0">See, for example, S. Chandrasekhar, “Stochastic Problems in Physics and Astronomy,” Reviews of Modern Physics, v. 15, No. 1,</span>
<span class="f1" data-bbox="91.9,639.9,60.8,4.5" data-line="2" data-segment="0">January 1943, p. 1.</span>
<span class="f11" data-bbox="102.8,646.9,3.0,3.3" data-line="3" data-segment="0">4</span>
<span class="f1" data-bbox="106.3,649.7,244.8,4.5" data-line="4" data-segment="0">Kendall and Smith, Tables of Random Sampling Numbers, Cambridge, 1939.</span>
</p>
<p>
<span class="f4" data-bbox="303.1,744.5,5.0,5.6" data-line="0" data-segment="0">5</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              <p>
<span class="f7" data-bbox="139.4,99.9,314.8,5.6" data-line="0" data-segment="0">pi( j) and the digram probabilities p(i; j) are related by the following formulas:</span>
</p>
<p>
<span class="f7" data-bbox="250.6,121.9,173.2,5.5" data-line="0" data-segment="0">p(i) = p(i; j) = p( j;i) = p( j)p j (i)</span>
<span class="f20" data-bbox="278.3,124.0,106.5,9.4" data-line="1" data-segment="0">∑ ∑ ∑</span>
<span class="f8" data-bbox="282.7,131.9,98.3,4.1" data-line="2" data-segment="0">j j j</span>
</p>
<p>
<span class="f7" data-bbox="242.2,146.9,72.7,5.5" data-line="0" data-segment="0">p(i; j) = p(i)pi( j)</span>
</p>
<p>
<span class="f7" data-bbox="246.2,170.6,127.7,5.6" data-line="0" data-segment="0">p (j) = p(i) = p(i; j) = 1:</span>
<span class="f20" data-bbox="234.2,172.0,94.0,10.1" data-line="1" data-segment="0">∑ i ∑ ∑</span>
<span class="f8" data-bbox="238.8,180.7,87.7,4.1" data-line="2" data-segment="0">j i i; j</span>
</p>
<p>
<span class="f4" data-bbox="138.7,203.0,350.7,5.6" data-line="0" data-segment="0">As a speciﬁc example suppose there are three letters A, B, C with the probability tables:</span>
</p>
<p>
<span class="f7" data-bbox="200.6,226.1,231.3,5.7" data-line="0" data-segment="0">pi( j) j i p(i) p(i; j) j</span>
<span class="f4" data-bbox="230.4,240.5,224.5,5.6" data-line="1" data-segment="0">A B C A B C</span>
<span class="f6" data-bbox="249.0,251.1,204.3,4.4" data-line="2" data-segment="0">4 1 9 4 1</span>
<span class="f4" data-bbox="213.2,255.1,195.8,5.7" data-line="3" data-segment="0">A 0 A A 0</span>
<span class="f6" data-bbox="249.0,258.4,206.3,4.4" data-line="4" data-segment="0">5 5 27 15 15</span>
<span class="f6" data-bbox="232.1,265.5,197.9,4.2" data-line="5" data-segment="0">1 1 16 8 8</span>
<span class="f7" data-bbox="201.0,269.3,253.0,5.8" data-line="6" data-segment="0">i B 0 B i B 0</span>
<span class="f6" data-bbox="232.1,272.8,199.7,4.4" data-line="7" data-segment="0">2 2 27 27 27</span>
<span class="f6" data-bbox="232.1,279.8,221.3,4.4" data-line="8" data-segment="0">1 2 1 2 1 4 1</span>
<span class="f4" data-bbox="213.7,283.7,177.9,5.8" data-line="9" data-segment="0">C C C</span>
<span class="f6" data-bbox="232.1,287.2,225.0,4.2" data-line="10" data-segment="0">2 5 10 27 27 135 135</span>
</p>
<p>
<span class="f4" data-bbox="138.7,305.9,208.1,5.6" data-line="0" data-segment="0">A typical message from this source is the following:</span>
<span class="f4" data-bbox="138.7,320.0,380.7,5.6" data-line="1" data-segment="0">A B B A B A B A B A B A B A B B B A B B B B B A B A B A B A B A B B B A C A C A B</span>
<span class="f4" data-bbox="138.7,331.9,177.1,5.6" data-line="2" data-segment="0">B A B B B B A B B A B A C B B B A B A.</span>
<span class="f4" data-bbox="138.7,345.8,380.8,5.6" data-line="3" data-segment="0">The next increase in complexity would involve trigram frequencies but no more. The choice of</span>
<span class="f4" data-bbox="138.7,357.8,380.5,5.6" data-line="4" data-segment="0">a letter would depend on the preceding two letters but not on the message before that point. A</span>
<span class="f4" data-bbox="138.7,369.8,380.7,5.6" data-line="5" data-segment="0">set of trigram frequencies p(i; j;k) or equivalently a set of transition probabilities pi j (k) would</span>
<span class="f4" data-bbox="138.7,381.7,380.9,5.6" data-line="6" data-segment="0">be required. Continuing in this way one obtains successively more complicated stochastic pro-</span>
<span class="f4" data-bbox="138.7,393.7,380.7,5.6" data-line="7" data-segment="0">cesses. In the general n-gram case a set of n-gram probabilities p(i1;i2;:::;in) or of transition</span>
<span class="f4" data-bbox="138.7,405.7,290.8,5.6" data-line="8" data-segment="0">probabilities pi ;i ;:::;i (in) is required to specify the statistical structure.</span>
<span class="f11" data-bbox="198.8,408.5,34.0,3.3" data-line="9" data-segment="0">1 2 n 1</span>
<span class="f4" data-bbox="120.0,421.5,399.4,5.6" data-line="10" data-segment="0">(D) Stochastic processes can also be deﬁned which produce a text consisting of a sequence of</span>
<span class="f4" data-bbox="138.7,433.5,380.8,5.6" data-line="11" data-segment="0">“words.” Suppose there are ﬁve letters A, B, C, D, E and 16 “words” in the language with</span>
<span class="f4" data-bbox="138.7,445.5,96.2,5.6" data-line="12" data-segment="0">associated probabilities:</span>
</p>
<p>
<span class="f4" data-bbox="224.2,462.9,203.2,5.6" data-line="0" data-segment="0">.10 A .16 BEBE .11 CABED .04 DEB</span>
<span class="f4" data-bbox="224.2,474.9,210.0,5.6" data-line="1" data-segment="0">.04 ADEB .04 BED .05 CEED .15 DEED</span>
<span class="f4" data-bbox="224.2,486.9,203.3,5.6" data-line="2" data-segment="0">.05 ADEE .02 BEED .08 DAB .01 EAB</span>
<span class="f4" data-bbox="224.2,498.8,195.5,5.6" data-line="3" data-segment="0">.01 BADD .05 CA .04 DAD .05 EE</span>
</p>
<p>
<span class="f4" data-bbox="138.7,516.2,380.9,5.6" data-line="0" data-segment="0">Suppose successive “words” are chosen independently and are separated by a space. A typical</span>
<span class="f4" data-bbox="138.7,528.2,74.2,5.6" data-line="1" data-segment="0">message might be:</span>
<span class="f4" data-bbox="138.7,542.1,380.5,5.6" data-line="2" data-segment="0">DAB EE A BEBE DEED DEB ADEE ADEE EE DEB BEBE BEBE BEBE ADEE BED DEED</span>
<span class="f4" data-bbox="138.7,554.0,350.7,5.6" data-line="3" data-segment="0">DEED CEED ADEE A DEED DEED BEBE CABED BEBE BED DAB DEED ADEB.</span>
<span class="f4" data-bbox="138.7,568.0,380.9,5.6" data-line="4" data-segment="0">If all the words are of ﬁnite length this process is equivalent to one of the preceding type, but</span>
<span class="f4" data-bbox="138.7,579.9,380.5,5.6" data-line="5" data-segment="0">the description may be simpler in terms of the word structure and probabilities. We may also</span>
<span class="f4" data-bbox="138.7,591.9,291.6,5.6" data-line="6" data-segment="0">generalize here and introduce transition probabilities between words, etc.</span>
</p>
<p>
<span class="f4" data-bbox="106.9,611.8,412.5,5.6" data-line="0" data-segment="0">These artiﬁcial languages are useful in constructing simple problems and examples to illustrate vari-</span>
<span class="f4" data-bbox="91.9,623.8,427.4,5.6" data-line="1" data-segment="0">ous possibilities. We can also approximate to a natural language by means of a series of simple artiﬁcial</span>
<span class="f4" data-bbox="91.9,635.7,427.3,5.6" data-line="2" data-segment="0">languages. The zero-order approximation is obtained by choosing all letters with the same probability and</span>
<span class="f4" data-bbox="91.9,647.7,427.1,5.6" data-line="3" data-segment="0">independently. The ﬁrst-order approximation is obtained by choosing successive letters independently but</span>
<span class="f6" data-bbox="398.6,656.0,3.7,4.1" data-line="4" data-segment="0">5</span>
<span class="f4" data-bbox="91.9,659.6,427.5,5.6" data-line="5" data-segment="0">each letter having the same probability that it has in the natural language. Thus, in the ﬁrst-order ap-</span>
<span class="f4" data-bbox="91.9,671.6,427.4,5.6" data-line="6" data-segment="0">proximation to English, E is chosen with probability .12 (its frequency in normal English) and W with</span>
<span class="f4" data-bbox="91.9,683.6,427.4,5.6" data-line="7" data-segment="0">probability .02, but there is no inﬂuence between adjacent letters and no tendency to form the preferred</span>
</p>
<p>
<span class="f11" data-bbox="102.8,698.7,3.0,3.3" data-line="0" data-segment="0">5</span>
<span class="f1" data-bbox="106.3,701.7,412.8,4.5" data-line="1" data-segment="0">Letter, digram and trigram frequencies are given in Secret and Urgent by Fletcher Pratt, Blue Ribbon Books, 1939. Word frequen-</span>
<span class="f1" data-bbox="91.9,711.1,352.3,4.5" data-line="2" data-segment="0">cies are tabulated in Relative Frequency of English Speech Sounds, G. Dewey, Harvard University Press, 1923.</span>
</p>
<p>
<span class="f4" data-bbox="303.1,744.5,5.0,5.6" data-line="0" data-segment="0">6</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <p>
<span class="f4" data-bbox="91.9,99.9,427.5,5.6" data-line="0" data-segment="0">digrams such as TH, ED, etc. In the second-order approximation, digram structure is introduced. After a</span>
<span class="f4" data-bbox="91.9,111.9,427.6,5.6" data-line="1" data-segment="0">letter is chosen, the next one is chosen in accordance with the frequencies with which the various letters</span>
<span class="f4" data-bbox="91.9,123.9,427.9,5.6" data-line="2" data-segment="0">follow the ﬁrst one. This requires a table of digram frequencies pi( j). In the third-order approximation,</span>
<span class="f4" data-bbox="91.9,135.8,427.4,5.6" data-line="3" data-segment="0">trigram structure is introduced. Each letter is chosen with probabilities which depend on the preceding two</span>
<span class="f4" data-bbox="91.9,147.8,26.9,5.6" data-line="4" data-segment="0">letters.</span>
</p>
<p>
<span class="f4" data-bbox="199.2,168.7,212.9,5.6" data-line="0" data-segment="0">3. THE SERIES OF APPROXIMATIONS TO ENGLISH</span>
<span class="f4" data-bbox="91.9,187.4,427.4,5.6" data-line="1" data-segment="0">To give a visual idea of how this series of processes approaches a language, typical sequences in the approx-</span>
<span class="f4" data-bbox="91.9,199.4,427.8,5.6" data-line="2" data-segment="0">imations to English have been constructed and are given below. In all cases we have assumed a 27-symbol</span>
<span class="f4" data-bbox="91.9,211.3,149.4,5.6" data-line="3" data-segment="0">“alphabet,” the 26 letters and a space.</span>
</p>
<p>
<span class="f4" data-bbox="104.4,231.2,284.1,5.6" data-line="0" data-segment="0">1. Zero-order approximation (symbols independent and equiprobable).</span>
</p>
<p>
<span class="f4" data-bbox="138.7,251.1,380.9,5.6" data-line="0" data-segment="0">XFOML RXKHRJFFJUJ ZLPWCFWKCYJ FFJEYVKCQSGHYD QPAAMKBZAACIBZL-</span>
<span class="f4" data-bbox="138.7,263.1,28.0,5.6" data-line="1" data-segment="0">HJQD.</span>
</p>
<p>
<span class="f4" data-bbox="104.4,283.0,356.4,5.6" data-line="0" data-segment="0">2. First-order approximation (symbols independent but with frequencies of English text).</span>
</p>
<p>
<span class="f4" data-bbox="138.7,302.9,380.7,5.6" data-line="0" data-segment="0">OCRO HLI RGWR NMIELWIS EU LL NBNESEBYA TH EEI ALHENHTTPA OOBTTVA</span>
<span class="f4" data-bbox="138.7,314.9,45.6,5.6" data-line="1" data-segment="0">NAH BRL.</span>
</p>
<p>
<span class="f4" data-bbox="104.4,334.9,258.1,5.6" data-line="0" data-segment="0">3. Second-order approximation (digram structure as in English).</span>
</p>
<p>
<span class="f4" data-bbox="138.7,354.8,380.5,5.6" data-line="0" data-segment="0">ON IE ANTSOUTINYS ARE T INCTORE ST BE S DEAMY ACHIN D ILONASIVE TU-</span>
<span class="f4" data-bbox="138.7,366.7,300.6,5.6" data-line="1" data-segment="0">COOWE AT TEASONARE FUSO TIZIN ANDY TOBE SEACE CTISBE.</span>
</p>
<p>
<span class="f4" data-bbox="104.4,386.6,252.0,5.6" data-line="0" data-segment="0">4. Third-order approximation (trigram structure as in English).</span>
</p>
<p>
<span class="f4" data-bbox="138.7,406.5,380.7,5.6" data-line="0" data-segment="0">IN NO IST LAT WHEY CRATICT FROURE BIRS GROCID PONDENOME OF DEMONS-</span>
<span class="f4" data-bbox="138.7,418.5,243.4,5.6" data-line="1" data-segment="0">TURES OF THE REPTAGIN IS REGOACTIONA OF CRE.</span>
</p>
<p>
<span class="f4" data-bbox="104.4,438.4,414.9,5.6" data-line="0" data-segment="0">5. First-order word approximation. Rather than continue with tetragram, ::: , n-gram structure it is easier</span>
<span class="f4" data-bbox="116.9,450.4,402.5,5.6" data-line="1" data-segment="0">and better to jump at this point to word units. Here words are chosen independently but with their</span>
<span class="f4" data-bbox="116.9,462.3,96.5,5.6" data-line="2" data-segment="0">appropriate frequencies.</span>
</p>
<p>
<span class="f4" data-bbox="138.7,482.2,380.5,5.6" data-line="0" data-segment="0">REPRESENTING AND SPEEDILY IS AN GOOD APT OR COME CAN DIFFERENT NAT-</span>
<span class="f4" data-bbox="138.7,494.2,380.7,5.6" data-line="1" data-segment="0">URAL HERE HE THE A IN CAME THE TO OF TO EXPERT GRAY COME TO FURNISHES</span>
<span class="f4" data-bbox="138.7,506.2,168.6,5.6" data-line="2" data-segment="0">THE LINE MESSAGE HAD BE THESE.</span>
</p>
<p>
<span class="f4" data-bbox="104.4,526.1,415.2,5.6" data-line="0" data-segment="0">6. Second-order word approximation. The word transition probabilities are correct but no further struc-</span>
<span class="f4" data-bbox="116.9,538.0,64.2,5.6" data-line="1" data-segment="0">ture is included.</span>
</p>
<p>
<span class="f4" data-bbox="138.7,557.9,380.6,5.6" data-line="0" data-segment="0">THE HEAD AND IN FRONTAL ATTACK ON AN ENGLISH WRITER THAT THE CHAR-</span>
<span class="f4" data-bbox="138.7,569.9,380.6,5.6" data-line="1" data-segment="0">ACTER OF THIS POINT IS THEREFORE ANOTHER METHOD FOR THE LETTERS THAT</span>
<span class="f4" data-bbox="138.7,581.9,325.4,5.6" data-line="2" data-segment="0">THE TIME OF WHO EVER TOLD THE PROBLEM FOR AN UNEXPECTED.</span>
</p>
<p>
<span class="f4" data-bbox="106.9,601.9,412.0,5.6" data-line="0" data-segment="0">The resemblance to ordinary English text increases quite noticeably at each of the above steps. Note that</span>
<span class="f4" data-bbox="91.9,613.7,427.2,5.6" data-line="1" data-segment="0">these samples have reasonably good structure out to about twice the range that is taken into account in their</span>
<span class="f4" data-bbox="91.9,625.7,427.7,5.6" data-line="2" data-segment="0">construction. Thus in (3) the statistical process insures reasonable text for two-letter sequences, but four-</span>
<span class="f4" data-bbox="91.9,637.7,427.7,5.6" data-line="3" data-segment="0">letter sequences from the sample can usually be ﬁtted into good sentences. In (6) sequences of four or more</span>
<span class="f4" data-bbox="91.9,649.6,427.4,5.6" data-line="4" data-segment="0">words can easily be placed in sentences without unusual or strained constructions. The particular sequence</span>
<span class="f4" data-bbox="91.9,661.6,427.4,5.6" data-line="5" data-segment="0">of ten words “attack on an English writer that the character of this” is not at all unreasonable. It appears then</span>
<span class="f4" data-bbox="91.9,673.5,413.6,5.6" data-line="6" data-segment="0">that a sufﬁciently complex stochastic process will give a satisfactory representation of a discrete source.</span>
<span class="f4" data-bbox="106.9,685.5,412.4,5.6" data-line="7" data-segment="0">The ﬁrst two samples were constructed by the use of a book of random numbers in conjunction with</span>
<span class="f4" data-bbox="91.9,697.5,426.7,5.6" data-line="8" data-segment="0">(for example 2) a table of letter frequencies. This method might have been continued for (3), (4) and (5),</span>
<span class="f4" data-bbox="91.9,709.4,426.8,5.6" data-line="9" data-segment="0">since digram, trigram and word frequency tables are available, but a simpler equivalent method was used.</span>
</p>
<p>
<span class="f4" data-bbox="303.1,744.5,5.0,5.6" data-line="0" data-segment="0">7</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                                                                                                                                                                                                                                            <p>
<span class="f4" data-bbox="91.9,99.9,427.6,5.6" data-line="0" data-segment="0">To construct (3) for example, one opens a book at random and selects a letter at random on the page. This</span>
<span class="f4" data-bbox="91.9,111.9,427.9,5.6" data-line="1" data-segment="0">letter is recorded. The book is then opened to another page and one reads until this letter is encountered.</span>
<span class="f4" data-bbox="91.9,123.9,427.2,5.6" data-line="2" data-segment="0">The succeeding letter is then recorded. Turning to another page this second letter is searched for and the</span>
<span class="f4" data-bbox="91.9,135.8,427.5,5.6" data-line="3" data-segment="0">succeeding letter recorded, etc. A similar process was used for (4), (5) and (6). It would be interesting if</span>
<span class="f4" data-bbox="91.9,147.8,416.8,5.6" data-line="4" data-segment="0">further approximations could be constructed, but the labor involved becomes enormous at the next stage.</span>
</p>
<p>
<span class="f4" data-bbox="179.5,168.7,252.1,5.6" data-line="0" data-segment="0">4. GRAPHICAL REPRESENTATION OF A MARKOFF PROCESS</span>
</p>
<p>
<span class="f4" data-bbox="91.9,187.4,427.6,5.6" data-line="0" data-segment="0">Stochastic processes of the type described above are known mathematically as discrete Markoff processes</span>
<span class="f6" data-bbox="297.4,195.7,3.7,4.1" data-line="1" data-segment="0">6</span>
<span class="f4" data-bbox="91.9,199.4,427.7,5.6" data-line="2" data-segment="0">and have been extensively studied in the literature. The general case can be described as follows: There</span>
<span class="f4" data-bbox="91.9,211.3,427.5,5.6" data-line="3" data-segment="0">exist a ﬁnite number of possible “states” of a system; S1;S2;:::;Sn. In addition there is a set of transition</span>
<span class="f4" data-bbox="91.9,223.3,427.5,5.6" data-line="4" data-segment="0">probabilities; pi( j) the probability that if the system is in state Si it will next go to state S j . To make this</span>
<span class="f4" data-bbox="91.9,235.1,427.5,5.6" data-line="5" data-segment="0">Markoff process into an information source we need only assume that a letter is produced for each transition</span>
<span class="f4" data-bbox="91.9,247.1,417.1,5.6" data-line="6" data-segment="0">from one state to another. The states will correspond to the “residue of inﬂuence” from preceding letters.</span>
<span class="f4" data-bbox="106.9,259.1,412.6,5.6" data-line="7" data-segment="0">The situation can be represented graphically as shown in Figs. 3, 4 and 5. The “states” are the junction</span>
</p>
<p>
<span class="f6" data-bbox="295.1,285.7,5.2,4.0" data-line="0" data-segment="0">A</span>
<span class="f24" data-bbox="307.1,291.2,7.1,4.5" data-line="1" data-segment="0">.1</span>
<span class="f24" data-bbox="267.0,295.6,73.1,5.0" data-line="2" data-segment="0">.4 B</span>
</p>
<p>
<span class="f6" data-bbox="261.5,322.6,92.6,5.0" data-line="0" data-segment="0">E .2</span>
<span class="f24" data-bbox="257.0,335.6,95.0,5.1" data-line="1" data-segment="0">.1 C</span>
</p>
<p>
<span class="f6" data-bbox="294.0,364.6,23.2,6.1" data-line="0" data-segment="0">D .2</span>
<span class="f12" data-bbox="198.2,380.1,214.7,5.0" data-line="1" data-segment="0">Fig. 3 — A graph corresponding to the source in example B.</span>
</p>
<p>
<span class="f4" data-bbox="91.9,403.9,427.9,5.6" data-line="0" data-segment="0">points in the graph and the probabilities and letters produced for a transition are given beside the correspond-</span>
<span class="f4" data-bbox="91.9,415.9,426.6,5.6" data-line="1" data-segment="0">ing line. Figure 3 is for the example B in Section 2, while Fig. 4 corresponds to the example C. In Fig. 3</span>
</p>
<p>
<span class="f6" data-bbox="266.8,450.3,63.8,5.9" data-line="0" data-segment="0">C B</span>
<span class="f6" data-bbox="300.6,462.3,5.2,4.0" data-line="1" data-segment="0">A</span>
<span class="f24" data-bbox="343.6,467.8,7.1,4.5" data-line="2" data-segment="0">.8</span>
<span class="f6" data-bbox="285.6,472.3,5.2,4.0" data-line="3" data-segment="0">A</span>
<span class="f24" data-bbox="245.5,474.9,7.1,4.5" data-line="4" data-segment="0">.2</span>
<span class="f24" data-bbox="319.6,484.9,7.1,4.5" data-line="5" data-segment="0">.5</span>
<span class="f24" data-bbox="270.6,488.8,7.1,4.5" data-line="6" data-segment="0">.5</span>
<span class="f6" data-bbox="382.8,493.3,4.8,4.0" data-line="7" data-segment="0">B</span>
<span class="f6" data-bbox="219.7,506.8,105.8,4.5" data-line="8" data-segment="0">C .4</span>
<span class="f6" data-bbox="263.8,508.9,124.8,4.5" data-line="9" data-segment="0">B .5</span>
<span class="f24" data-bbox="218.5,519.8,7.1,4.5" data-line="10" data-segment="0">.1</span>
</p>
<p>
<span class="f12" data-bbox="198.2,536.8,214.7,5.0" data-line="0" data-segment="0">Fig. 4 — A graph corresponding to the source in example C.</span>
</p>
<p>
<span class="f4" data-bbox="91.9,560.6,427.2,5.6" data-line="0" data-segment="0">there is only one state since successive letters are independent. In Fig. 4 there are as many states as letters.</span>
<span class="f6" data-bbox="347.5,568.9,3.7,4.1" data-line="1" data-segment="0">2</span>
<span class="f4" data-bbox="91.9,572.5,427.5,5.6" data-line="2" data-segment="0">If a trigram example were constructed there would be at most n states corresponding to the possible pairs</span>
<span class="f4" data-bbox="91.9,584.5,428.6,5.6" data-line="3" data-segment="0">of letters preceding the one being chosen. Figure 5 is a graph for the case of word structure in example D.</span>
<span class="f4" data-bbox="91.9,596.5,171.3,5.6" data-line="4" data-segment="0">Here S corresponds to the “space” symbol.</span>
</p>
<p>
<span class="f4" data-bbox="230.6,617.3,149.9,5.6" data-line="0" data-segment="0">5. ERGODIC AND MIXED SOURCES</span>
</p>
<p>
<span class="f4" data-bbox="91.9,636.1,427.4,5.6" data-line="0" data-segment="0">As we have indicated above a discrete source for our purposes can be considered to be represented by a</span>
<span class="f4" data-bbox="91.9,647.9,427.7,5.6" data-line="1" data-segment="0">Markoff process. Among the possible discrete Markoff processes there is a group with special properties</span>
<span class="f4" data-bbox="91.9,659.9,427.3,5.6" data-line="2" data-segment="0">of signiﬁcance in communication theory. This special class consists of the “ergodic” processes and we</span>
<span class="f4" data-bbox="91.9,671.9,427.5,5.6" data-line="3" data-segment="0">shall call the corresponding sources ergodic sources. Although a rigorous deﬁnition of an ergodic process is</span>
<span class="f4" data-bbox="91.9,683.8,427.5,5.6" data-line="4" data-segment="0">somewhat involved, the general idea is simple. In an ergodic process every sequence produced by the process</span>
<span class="f11" data-bbox="102.8,699.1,3.0,3.3" data-line="5" data-segment="0">6</span>
<span class="f1" data-bbox="106.3,701.9,412.9,4.5" data-line="6" data-segment="0">For a detailed treatment see M. Fréchet, Méthode des fonctions arbitraires. Théorie des événements en chaı̂ne dans le cas d’un</span>
<span class="f2" data-bbox="91.9,711.4,188.9,4.5" data-line="7" data-segment="0">nombre ﬁni d’états possibles. Paris, Gauthier-Villars, 1938.</span>
</p>
<p>
<span class="f4" data-bbox="303.1,744.5,5.0,5.6" data-line="0" data-segment="0">8</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                                                                                                                                                                                                                                                              <p>
<span class="f4" data-bbox="91.9,99.9,427.4,5.6" data-line="0" data-segment="0">is the same in statistical properties. Thus the letter frequencies, digram frequencies, etc., obtained from</span>
<span class="f4" data-bbox="91.9,111.9,427.8,5.6" data-line="1" data-segment="0">particular sequences, will, as the lengths of the sequences increase, approach deﬁnite limits independent</span>
<span class="f4" data-bbox="91.9,123.9,427.4,5.6" data-line="2" data-segment="0">of the particular sequence. Actually this is not true of every sequence but the set for which it is false has</span>
<span class="f4" data-bbox="91.9,135.8,311.0,5.6" data-line="3" data-segment="0">probability zero. Roughly the ergodic property means statistical homogeneity.</span>
<span class="f4" data-bbox="106.9,147.8,412.4,5.6" data-line="4" data-segment="0">All the examples of artiﬁcial languages given above are ergodic. This property is related to the structure</span>
<span class="f6" data-bbox="389.0,156.1,3.7,4.1" data-line="5" data-segment="0">7</span>
<span class="f4" data-bbox="91.9,159.8,427.5,5.6" data-line="6" data-segment="0">of the corresponding graph. If the graph has the following two properties the corresponding process will</span>
<span class="f4" data-bbox="91.9,171.7,44.6,5.6" data-line="7" data-segment="0">be ergodic:</span>
</p>
<p>
<span class="f4" data-bbox="104.4,188.5,414.4,5.6" data-line="0" data-segment="0">1. The graph does not consist of two isolated parts A and B such that it is impossible to go from junction</span>
<span class="f4" data-bbox="116.9,200.5,402.3,5.6" data-line="1" data-segment="0">points in part A to junction points in part B along lines of the graph in the direction of arrows and also</span>
<span class="f4" data-bbox="116.9,212.5,253.7,5.6" data-line="2" data-segment="0">impossible to go from junctions in part B to junctions in part A.</span>
</p>
<p>
<span class="f4" data-bbox="104.4,230.8,414.9,5.6" data-line="0" data-segment="0">2. A closed series of lines in the graph with all arrows on the lines pointing in the same orientation will</span>
<span class="f4" data-bbox="116.9,242.7,402.3,5.6" data-line="1" data-segment="0">be called a “circuit.” The “length” of a circuit is the number of lines in it. Thus in Fig. 5 series BEBES</span>
<span class="f4" data-bbox="116.9,254.7,402.7,5.6" data-line="2" data-segment="0">is a circuit of length 5. The second property required is that the greatest common divisor of the lengths</span>
<span class="f4" data-bbox="116.9,266.7,133.9,5.6" data-line="3" data-segment="0">of all circuits in the graph be one.</span>
</p>
<p>
<span class="f6" data-bbox="355.9,292.5,4.4,4.0" data-line="0" data-segment="0">E</span>
<span class="f6" data-bbox="236.5,295.5,5.2,4.0" data-line="1" data-segment="0">D</span>
</p>
<p>
<span class="f6" data-bbox="401.8,311.5,4.8,4.0" data-line="0" data-segment="0">B</span>
<span class="f6" data-bbox="379.0,315.5,4.4,4.0" data-line="1" data-segment="0">E</span>
</p>
<p>
<span class="f6" data-bbox="187.2,334.5,150.4,7.0" data-line="0" data-segment="0">S A B</span>
<span class="f6" data-bbox="259.0,339.5,116.4,5.0" data-line="1" data-segment="0">E E</span>
<span class="f6" data-bbox="352.6,346.5,5.2,4.0" data-line="2" data-segment="0">D</span>
<span class="f6" data-bbox="280.6,354.5,5.2,4.0" data-line="3" data-segment="0">A</span>
<span class="f6" data-bbox="202.8,359.5,4.8,4.0" data-line="4" data-segment="0">B</span>
<span class="f6" data-bbox="308.5,361.5,36.8,4.0" data-line="5" data-segment="0">D E</span>
<span class="f6" data-bbox="221.2,370.5,147.6,6.1" data-line="6" data-segment="0">S B E D</span>
</p>
<p>
<span class="f6" data-bbox="215.8,387.5,110.6,5.9" data-line="0" data-segment="0">C A E E</span>
<span class="f6" data-bbox="341.8,396.5,28.8,4.0" data-line="1" data-segment="0">B B</span>
<span class="f6" data-bbox="213.6,405.5,5.2,4.0" data-line="2" data-segment="0">D</span>
<span class="f6" data-bbox="265.9,408.5,4.4,4.0" data-line="3" data-segment="0">E</span>
<span class="f6" data-bbox="290.5,414.5,70.2,5.9" data-line="4" data-segment="0">A D</span>
<span class="f6" data-bbox="380.8,424.5,4.8,4.0" data-line="5" data-segment="0">B</span>
<span class="f6" data-bbox="205.0,436.5,4.4,4.0" data-line="6" data-segment="0">E</span>
<span class="f6" data-bbox="301.0,440.5,4.4,4.0" data-line="7" data-segment="0">E</span>
</p>
<p>
<span class="f6" data-bbox="305.5,455.5,5.2,4.0" data-line="0" data-segment="0">A</span>
</p>
<p>
<span class="f6" data-bbox="295.2,488.5,4.0,4.0" data-line="0" data-segment="0">S</span>
</p>
<p>
<span class="f12" data-bbox="198.0,511.0,215.2,5.0" data-line="0" data-segment="0">Fig. 5 — A graph corresponding to the source in example D.</span>
</p>
<p>
<span class="f4" data-bbox="106.9,533.2,412.5,5.6" data-line="0" data-segment="0">If the ﬁrst condition is satisﬁed but the second one violated by having the greatest common divisor equal</span>
<span class="f4" data-bbox="91.9,545.1,427.7,5.6" data-line="1" data-segment="0">to d &gt; 1, the sequences have a certain type of periodic structure. The various sequences fall into d different</span>
<span class="f4" data-bbox="91.9,557.1,427.2,5.6" data-line="2" data-segment="0">classes which are statistically the same apart from a shift of the origin (i.e., which letter in the sequence is</span>
<span class="f4" data-bbox="91.9,569.0,427.5,8.5" data-line="3" data-segment="0">called letter 1). By a shift of from 0 up to d  1 any sequence can be made statistically equivalent to any</span>
<span class="f4" data-bbox="91.9,581.0,427.3,5.6" data-line="4" data-segment="0">other. A simple example with d = 2 is the following: There are three possible letters a;b;c. Letter a is</span>
<span class="f6" data-bbox="273.1,589.0,29.0,4.1" data-line="5" data-segment="0">1 2</span>
<span class="f4" data-bbox="91.9,593.0,427.4,5.6" data-line="6" data-segment="0">followed with either b or c with probabilities and respectively. Either b or c is always followed by letter</span>
<span class="f6" data-bbox="273.1,596.3,29.0,4.1" data-line="7" data-segment="0">3 3</span>
<span class="f7" data-bbox="91.9,604.9,115.2,5.6" data-line="8" data-segment="0">a. Thus a typical sequence is</span>
<span class="f7" data-bbox="232.4,616.9,146.4,5.5" data-line="9" data-segment="0">a b a c a c a c a b a c a b a b a c a c:</span>
</p>
<p>
<span class="f4" data-bbox="91.9,632.5,247.4,5.6" data-line="0" data-segment="0">This type of situation is not of much importance for our work.</span>
<span class="f4" data-bbox="106.9,644.5,412.2,5.6" data-line="1" data-segment="0">If the ﬁrst condition is violated the graph may be separated into a set of subgraphs each of which satisﬁes</span>
<span class="f4" data-bbox="91.9,656.3,427.3,5.6" data-line="2" data-segment="0">the ﬁrst condition. We will assume that the second condition is also satisﬁed for each subgraph. We have in</span>
<span class="f4" data-bbox="91.9,668.3,427.9,5.6" data-line="3" data-segment="0">this case what may be called a “mixed” source made up of a number of pure components. The components</span>
<span class="f4" data-bbox="91.9,680.2,378.0,5.6" data-line="4" data-segment="0">correspond to the various subgraphs. If L1, L2, L3;::: are the component sources we may write</span>
</p>
<p>
<span class="f7" data-bbox="245.8,698.3,118.7,8.5" data-line="0" data-segment="0">L = p1L1 + p2L2 + p3L3 +    </span>
<span class="f11" data-bbox="102.8,711.8,3.0,3.3" data-line="1" data-segment="0">7</span>
<span class="f1" data-bbox="106.3,714.7,240.0,4.5" data-line="2" data-segment="0">These are restatements in terms of the graph of conditions given in Fréchet.</span>
</p>
<p>
<span class="f4" data-bbox="303.1,744.5,5.0,5.6" data-line="0" data-segment="0">9</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   <p>
<span class="f4" data-bbox="91.9,99.9,218.9,5.6" data-line="0" data-segment="0">where pi is the probability of the component source Li.</span>
<span class="f4" data-bbox="106.9,111.9,412.4,5.6" data-line="1" data-segment="0">Physically the situation represented is this: There are several different sources L1, L2, L3;::: which are</span>
<span class="f4" data-bbox="91.9,123.9,427.4,5.6" data-line="2" data-segment="0">each of homogeneous statistical structure (i.e., they are ergodic). We do not know a priori which is to be</span>
<span class="f4" data-bbox="91.9,135.8,427.4,5.6" data-line="3" data-segment="0">used, but once the sequence starts in a given pure component Li, it continues indeﬁnitely according to the</span>
<span class="f4" data-bbox="91.9,147.8,152.8,5.6" data-line="4" data-segment="0">statistical structure of that component.</span>
<span class="f4" data-bbox="106.9,159.8,412.5,5.6" data-line="5" data-segment="0">As an example one may take two of the processes deﬁned above and assume p1 = :2 and p2 = :8. A</span>
<span class="f4" data-bbox="91.9,171.7,128.9,5.6" data-line="6" data-segment="0">sequence from the mixed source</span>
<span class="f7" data-bbox="274.1,183.7,62.9,5.6" data-line="7" data-segment="0">L = :2L1 + :8L2</span>
</p>
<p>
<span class="f4" data-bbox="91.9,199.5,427.4,5.6" data-line="0" data-segment="0">would be obtained by choosing ﬁrst L1 or L2 with probabilities .2 and .8 and after this choice generating a</span>
<span class="f4" data-bbox="91.9,211.5,152.7,5.6" data-line="1" data-segment="0">sequence from whichever was chosen.</span>
<span class="f4" data-bbox="106.9,223.4,413.3,5.6" data-line="2" data-segment="0">Except when the contrary is stated we shall assume a source to be ergodic. This assumption enables one</span>
<span class="f4" data-bbox="91.9,235.4,427.6,5.6" data-line="3" data-segment="0">to identify averages along a sequence with averages over the ensemble of possible sequences (the probability</span>
<span class="f4" data-bbox="91.9,247.4,427.2,5.6" data-line="4" data-segment="0">of a discrepancy being zero). For example the relative frequency of the letter A in a particular inﬁnite</span>
<span class="f4" data-bbox="91.9,259.3,396.5,5.6" data-line="5" data-segment="0">sequence will be, with probability one, equal to its relative frequency in the ensemble of sequences.</span>
<span class="f4" data-bbox="106.9,271.3,412.4,5.6" data-line="6" data-segment="0">If Pi is the probability of state i and pi( j) the transition probability to state j, then for the process to be</span>
<span class="f4" data-bbox="91.9,283.1,266.8,5.6" data-line="7" data-segment="0">stationary it is clear that the Pi must satisfy equilibrium conditions:</span>
</p>
<p>
<span class="f7" data-bbox="274.0,301.6,63.2,5.5" data-line="0" data-segment="0">Pj = Pi pi( j):</span>
<span class="f20" data-bbox="295.1,303.8,10.3,9.4" data-line="1" data-segment="0">∑</span>
<span class="f8" data-bbox="299.2,311.8,2.1,4.1" data-line="2" data-segment="0">i</span>
</p>
<p>
<span class="f4" data-bbox="91.9,327.8,427.4,5.6" data-line="0" data-segment="0">In the ergodic case it can be shown that with any starting conditions the probabilities Pj (N) of being in state</span>
<span class="f7" data-bbox="93.4,339.8,246.6,8.5" data-line="1" data-segment="0">j after N symbols, approach the equilibrium values as N !∞.</span>
</p>
<p>
<span class="f4" data-bbox="215.9,360.4,179.4,5.6" data-line="0" data-segment="0">6. CHOICE, UNCERTAINTY AND ENTROPY</span>
</p>
<p>
<span class="f4" data-bbox="91.9,379.0,427.4,5.6" data-line="0" data-segment="0">We have represented a discrete information source as a Markoff process. Can we deﬁne a quantity which</span>
<span class="f4" data-bbox="91.9,391.0,427.2,5.6" data-line="1" data-segment="0">will measure, in some sense, how much information is “produced” by such a process, or better, at what rate</span>
<span class="f4" data-bbox="91.9,403.0,100.5,5.6" data-line="2" data-segment="0">information is produced?</span>
<span class="f4" data-bbox="106.9,414.9,412.4,5.6" data-line="3" data-segment="0">Suppose we have a set of possible events whose probabilities of occurrence are p1; p2;:::; pn. These</span>
<span class="f4" data-bbox="91.9,426.9,427.6,5.6" data-line="4" data-segment="0">probabilities are known but that is all we know concerning which event will occur. Can we ﬁnd a measure</span>
<span class="f4" data-bbox="91.9,438.8,423.6,5.6" data-line="5" data-segment="0">of how much “choice” is involved in the selection of the event or of how uncertain we are of the outcome?</span>
<span class="f4" data-bbox="106.9,450.8,412.7,5.6" data-line="6" data-segment="0">If there is such a measure, say H(p1; p2;:::; pn), it is reasonable to require of it the following properties:</span>
</p>
<p>
<span class="f4" data-bbox="104.4,467.9,145.7,5.6" data-line="0" data-segment="0">1. H should be continuous in the pi.</span>
</p>
<p>
<span class="f6" data-bbox="229.4,482.6,3.7,4.1" data-line="0" data-segment="0">1</span>
<span class="f4" data-bbox="104.4,486.5,415.0,5.6" data-line="1" data-segment="0">2. If all the pi are equal, pi = , then H should be a monotonic increasing function of n. With equally</span>
<span class="f8" data-bbox="229.4,489.9,3.7,4.1" data-line="2" data-segment="0">n</span>
<span class="f4" data-bbox="116.9,498.4,345.1,5.6" data-line="3" data-segment="0">likely events there is more choice, or uncertainty, when there are more possible events.</span>
</p>
<p>
<span class="f4" data-bbox="104.4,517.0,414.9,5.6" data-line="0" data-segment="0">3. If a choice be broken down into two successive choices, the original H should be the weighted sum</span>
<span class="f4" data-bbox="116.9,528.9,402.5,5.6" data-line="1" data-segment="0">of the individual values of H. The meaning of this is illustrated in Fig. 6. At the left we have three</span>
</p>
<p>
<span class="f24" data-bbox="248.8,554.0,128.6,5.5" data-line="0" data-segment="0">1/2 1/2</span>
<span class="f24" data-bbox="318.7,563.0,13.7,4.5" data-line="1" data-segment="0">1/2</span>
<span class="f24" data-bbox="248.8,572.9,13.7,4.5" data-line="2" data-segment="0">1/3</span>
</p>
<p>
<span class="f24" data-bbox="341.8,588.9,13.7,4.5" data-line="0" data-segment="0">2/3</span>
<span class="f24" data-bbox="363.7,591.9,13.7,4.5" data-line="1" data-segment="0">1/3</span>
<span class="f24" data-bbox="318.7,597.9,13.7,4.5" data-line="2" data-segment="0">1/2</span>
<span class="f24" data-bbox="245.8,601.9,13.7,4.5" data-line="3" data-segment="0">1/6</span>
<span class="f24" data-bbox="337.7,611.0,39.7,5.5" data-line="4" data-segment="0">1/3 1/6</span>
</p>
<p>
<span class="f12" data-bbox="197.9,628.6,215.4,5.0" data-line="0" data-segment="0">Fig. 6 — Decomposition of a choice from three possibilities.</span>
</p>
<p>
<span class="f6" data-bbox="189.8,647.0,69.5,4.1" data-line="0" data-segment="0">1 1 1</span>
<span class="f4" data-bbox="116.9,650.9,402.3,5.6" data-line="1" data-segment="0">possibilities p1 = , p2 = , p3 = . On the right we ﬁrst choose between two possibilities each with</span>
<span class="f6" data-bbox="189.8,654.4,69.5,4.1" data-line="2" data-segment="0">2 3 6</span>
<span class="f6" data-bbox="164.4,660.4,283.4,4.1" data-line="3" data-segment="0">1 2 1</span>
<span class="f4" data-bbox="116.9,664.4,402.5,5.6" data-line="4" data-segment="0">probability , and if the second occurs make another choice with probabilities , . The ﬁnal results</span>
<span class="f6" data-bbox="164.4,667.7,283.4,4.1" data-line="5" data-segment="0">2 3 3</span>
<span class="f4" data-bbox="116.9,676.3,295.8,5.6" data-line="6" data-segment="0">have the same probabilities as before. We require, in this special case, that</span>
</p>
<p>
<span class="f6" data-bbox="262.8,690.8,115.7,4.1" data-line="0" data-segment="0">1 1 1 1 1 1 2 1</span>
<span class="f7" data-bbox="249.8,694.7,136.4,5.5" data-line="1" data-segment="0">H( ; ; ) = H( ; ) + H( ; ):</span>
<span class="f6" data-bbox="262.8,698.2,115.7,4.1" data-line="2" data-segment="0">2 3 6 2 2 2 3 3</span>
<span class="f6" data-bbox="180.4,710.7,3.7,4.1" data-line="3" data-segment="0">1</span>
<span class="f4" data-bbox="116.9,714.7,290.8,5.6" data-line="4" data-segment="0">The coefﬁcient is because this second choice only occurs half the time.</span>
<span class="f6" data-bbox="180.4,718.1,3.7,4.1" data-line="5" data-segment="0">2</span>
</p>
<p>
<span class="f4" data-bbox="300.6,744.5,10.0,5.6" data-line="0" data-segment="0">10</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                                                                                                                                                 <p>
<span class="f4" data-bbox="106.9,99.9,199.1,5.6" data-line="0" data-segment="0">In Appendix 2, the following result is established:</span>
<span class="f7" data-bbox="106.9,114.9,312.3,5.7" data-line="1" data-segment="0">Theorem 2: The only H satisfying the three above assumptions is of the form:</span>
<span class="f8" data-bbox="305.5,130.6,3.7,4.1" data-line="2" data-segment="0">n</span>
<span class="f7" data-bbox="265.6,141.2,79.7,8.5" data-line="3" data-segment="0">H =  K pi log pi</span>
<span class="f20" data-bbox="302.3,143.3,10.3,9.4" data-line="4" data-segment="0">∑</span>
<span class="f8" data-bbox="301.7,151.5,11.5,4.1" data-line="5" data-segment="0">i=1</span>
</p>
<p>
<span class="f4" data-bbox="91.9,168.7,122.0,5.7" data-line="0" data-segment="0">where K is a positive constant.</span>
<span class="f4" data-bbox="106.9,183.7,412.8,5.6" data-line="1" data-segment="0">This theorem, and the assumptions required for its proof, are in no way necessary for the present theory.</span>
<span class="f4" data-bbox="91.9,195.5,427.5,5.6" data-line="2" data-segment="0">It is given chieﬂy to lend a certain plausibility to some of our later deﬁnitions. The real justiﬁcation of these</span>
<span class="f4" data-bbox="91.9,207.5,212.6,5.6" data-line="3" data-segment="0">deﬁnitions, however, will reside in their implications.</span>
<span class="f4" data-bbox="106.9,219.4,412.5,8.5" data-line="4" data-segment="0">Quantities of the form H = ∑ pi log pi (the constant K merely amounts to a choice of a unit of measure)</span>
<span class="f4" data-bbox="91.9,231.4,426.7,5.6" data-line="5" data-segment="0">play a central role in information theory as measures of information, choice and uncertainty. The form of H</span>
<span class="f6" data-bbox="468.6,239.8,3.7,4.1" data-line="6" data-segment="0">8</span>
<span class="f4" data-bbox="91.9,243.4,427.4,5.6" data-line="7" data-segment="0">will be recognized as that of entropy as deﬁned in certain formulations of statistical mechanics where pi is</span>
<span class="f4" data-bbox="91.9,255.3,427.6,5.6" data-line="8" data-segment="0">the probability of a system being in cell i of its phase space. H is then, for example, the H in Boltzmann’s</span>
<span class="f4" data-bbox="91.9,267.3,427.4,8.5" data-line="9" data-segment="0">famous H theorem. We shall call H =  ∑ pi log pi the entropy of the set of probabilities p1;:::; pn. If x is a</span>
<span class="f4" data-bbox="91.9,279.3,427.3,5.6" data-line="10" data-segment="0">chance variable we will write H(x) for its entropy; thus x is not an argument of a function but a label for a</span>
<span class="f4" data-bbox="91.9,291.2,310.5,5.6" data-line="11" data-segment="0">number, to differentiate it from H(y) say, the entropy of the chance variable y.</span>
<span class="f4" data-bbox="106.9,303.2,344.0,8.5" data-line="12" data-segment="0">The entropy in the case of two possibilities with probabilities p and q = 1  p, namely</span>
</p>
<p>
<span class="f7" data-bbox="256.9,323.5,97.4,8.5" data-line="0" data-segment="0">H =  (p log p + q log q)</span>
</p>
<p>
<span class="f4" data-bbox="91.9,343.7,147.7,5.6" data-line="0" data-segment="0">is plotted in Fig. 7 as a function of p.</span>
</p>
<p>
<span class="f24" data-bbox="178.8,385.1,11.6,4.5" data-line="0" data-segment="0">1.0</span>
</p>
<p>
<span class="f24" data-bbox="183.5,405.1,7.1,4.5" data-line="0" data-segment="0">.9</span>
</p>
<p>
<span class="f24" data-bbox="183.5,425.1,7.1,4.5" data-line="0" data-segment="0">.8</span>
</p>
<p>
<span class="f24" data-bbox="183.5,445.1,7.1,4.5" data-line="0" data-segment="0">.7</span>
<span class="f2" data-bbox="162.5,459.8,5.8,4.4" data-line="1" data-segment="0">H</span>
<span class="f24" data-bbox="183.5,465.1,7.1,4.5" data-line="2" data-segment="0">.6</span>
<span class="f6" data-bbox="157.2,467.5,17.1,4.0" data-line="3" data-segment="0">BITS</span>
</p>
<p>
<span class="f24" data-bbox="183.5,485.1,7.1,4.5" data-line="0" data-segment="0">.5</span>
</p>
<p>
<span class="f24" data-bbox="183.5,505.1,7.1,4.5" data-line="0" data-segment="0">.4</span>
</p>
<p>
<span class="f24" data-bbox="183.5,525.1,7.1,4.5" data-line="0" data-segment="0">.3</span>
</p>
<p>
<span class="f24" data-bbox="183.5,545.1,7.1,4.5" data-line="0" data-segment="0">.2</span>
</p>
<p>
<span class="f24" data-bbox="183.5,565.1,7.1,4.5" data-line="0" data-segment="0">.1</span>
</p>
<p>
<span class="f24" data-bbox="186.0,585.1,4.6,4.5" data-line="0" data-segment="0">0</span>
<span class="f24" data-bbox="193.3,591.1,208.1,4.5" data-line="1" data-segment="0">0 .1 .2 .3 .4 .5 .6 .7 .8 .9 1.0</span>
<span class="f2" data-bbox="304.0,599.1,4.0,4.4" data-line="2" data-segment="0">p</span>
<span class="f12" data-bbox="160.8,614.1,289.5,5.0" data-line="3" data-segment="0">Fig. 7 — Entropy in the case of two possibilities with probabilities p and (1  p).</span>
</p>
<p>
<span class="f4" data-bbox="106.9,637.4,412.5,5.6" data-line="0" data-segment="0">The quantity H has a number of interesting properties which further substantiate it as a reasonable</span>
<span class="f4" data-bbox="91.9,649.4,135.5,5.6" data-line="1" data-segment="0">measure of choice or information.</span>
<span class="f4" data-bbox="106.9,661.4,412.4,5.6" data-line="2" data-segment="0">1. H = 0 if and only if all the pi but one are zero, this one having the value unity. Thus only when we</span>
<span class="f4" data-bbox="91.9,673.3,265.8,5.6" data-line="3" data-segment="0">are certain of the outcome does H vanish. Otherwise H is positive.</span>
<span class="f6" data-bbox="456.2,681.3,3.7,4.1" data-line="4" data-segment="0">1</span>
<span class="f4" data-bbox="106.9,685.3,412.4,5.6" data-line="5" data-segment="0">2. For a given n, H is a maximum and equal to log n when all the pi are equal (i.e., ). This is also</span>
<span class="f8" data-bbox="456.2,688.7,3.7,4.1" data-line="6" data-segment="0">n</span>
<span class="f4" data-bbox="91.9,697.3,156.0,5.6" data-line="7" data-segment="0">intuitively the most uncertain situation.</span>
<span class="f11" data-bbox="102.8,711.8,3.0,3.3" data-line="8" data-segment="0">8</span>
<span class="f1" data-bbox="106.3,714.7,301.7,4.5" data-line="9" data-segment="0">See, for example, R. C. Tolman, Principles of Statistical Mechanics, Oxford, Clarendon, 1938.</span>
</p>
<p>
<span class="f4" data-bbox="300.6,744.5,10.0,5.6" data-line="0" data-segment="0">11</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                                                                                                                                                                       <p>
<span class="f4" data-bbox="106.9,99.9,412.5,5.6" data-line="0" data-segment="0">3. Suppose there are two events, x and y, in question with m possibilities for the ﬁrst and n for the second.</span>
<span class="f4" data-bbox="91.9,111.9,427.4,5.6" data-line="1" data-segment="0">Let p(i; j) be the probability of the joint occurrence of i for the ﬁrst and j for the second. The entropy of the</span>
<span class="f4" data-bbox="91.9,123.9,51.0,5.6" data-line="2" data-segment="0">joint event is</span>
<span class="f7" data-bbox="243.2,135.8,124.7,8.5" data-line="3" data-segment="0">H(x;y) =   p(i; j)log p(i; j)</span>
<span class="f20" data-bbox="292.6,137.9,10.3,9.4" data-line="4" data-segment="0">∑</span>
<span class="f8" data-bbox="294.0,146.0,7.2,4.1" data-line="5" data-segment="0">i; j</span>
</p>
<p>
<span class="f4" data-bbox="91.9,161.3,22.2,5.6" data-line="0" data-segment="0">while</span>
</p>
<p>
<span class="f7" data-bbox="240.2,181.9,127.8,8.5" data-line="0" data-segment="0">H(x) =   p(i; j)log p(i; j)</span>
<span class="f20" data-bbox="281.4,184.0,61.1,9.4" data-line="1" data-segment="0">∑ ∑</span>
<span class="f8" data-bbox="282.7,192.1,56.2,4.1" data-line="2" data-segment="0">i; j j</span>
<span class="f7" data-bbox="240.4,207.2,130.4,8.5" data-line="3" data-segment="0">H(y) =   p(i; j)log p(i; j):</span>
<span class="f20" data-bbox="281.4,209.3,61.1,9.4" data-line="4" data-segment="0">∑ ∑</span>
<span class="f8" data-bbox="282.7,217.3,55.7,4.1" data-line="5" data-segment="0">i; j i</span>
</p>
<p>
<span class="f4" data-bbox="91.9,236.2,86.7,5.6" data-line="0" data-segment="0">It is easily shown that</span>
<span class="f7" data-bbox="260.0,248.1,91.3,8.5" data-line="1" data-segment="0">H(x;y)  H(x) + H(y)</span>
</p>
<p>
<span class="f4" data-bbox="91.9,265.3,427.4,5.6" data-line="0" data-segment="0">with equality only if the events are independent (i.e., p(i; j) = p(i)p( j)). The uncertainty of a joint event is</span>
<span class="f4" data-bbox="91.9,277.1,237.7,5.6" data-line="1" data-segment="0">less than or equal to the sum of the individual uncertainties.</span>
<span class="f4" data-bbox="106.9,289.1,412.5,5.6" data-line="2" data-segment="0">4. Any change toward equalization of the probabilities p1; p2;:::; pn increases H. Thus if p1 &lt; p2 and</span>
<span class="f4" data-bbox="91.9,301.1,427.5,5.6" data-line="3" data-segment="0">we increase p1, decreasing p2 an equal amount so that p1 and p2 are more nearly equal, then H increases.</span>
<span class="f4" data-bbox="91.9,313.0,312.7,5.6" data-line="4" data-segment="0">More generally, if we perform any “averaging” operation on the pi of the form</span>
</p>
<p>
<span class="f14" data-bbox="285.2,329.5,2.0,2.5" data-line="0" data-segment="0">0</span>
<span class="f7" data-bbox="280.2,333.5,50.9,5.5" data-line="1" data-segment="0">p = ai j p j</span>
<span class="f8" data-bbox="285.2,335.7,25.0,9.4" data-line="2" data-segment="0">i ∑</span>
<span class="f8" data-bbox="304.4,343.7,2.1,4.1" data-line="3" data-segment="0">j</span>
</p>
<p>
<span class="f4" data-bbox="91.9,362.6,427.8,8.5" data-line="0" data-segment="0">where ∑ ai j = ∑ ai j = 1, and all ai j   0, then H increases (except in the special case where this transfor-</span>
<span class="f8" data-bbox="126.2,364.7,37.2,4.1" data-line="1" data-segment="0">i j</span>
<span class="f4" data-bbox="91.9,374.5,380.7,5.6" data-line="2" data-segment="0">mation amounts to no more than a permutation of the p j with H of course remaining the same).</span>
<span class="f4" data-bbox="106.9,386.5,412.6,5.6" data-line="3" data-segment="0">5. Suppose there are two chance events x and y as in 3, not necessarily independent. For any particular</span>
<span class="f4" data-bbox="91.9,398.5,408.9,5.6" data-line="4" data-segment="0">value i that x can assume there is a conditional probability pi( j) that y has the value j. This is given by</span>
</p>
<p>
<span class="f7" data-bbox="309.0,417.9,23.8,5.5" data-line="0" data-segment="0">p(i; j)</span>
<span class="f7" data-bbox="269.0,424.7,73.8,5.5" data-line="1" data-segment="0">pi( j) = :</span>
<span class="f25" data-bbox="302.3,431.5,36.5,7.2" data-line="2" data-segment="0">∑ j p(i; j)</span>
</p>
<p>
<span class="f4" data-bbox="91.9,452.2,427.5,5.6" data-line="0" data-segment="0">We deﬁne the conditional entropy of y, Hx(y) as the average of the entropy of y for each value of x, weighted</span>
<span class="f4" data-bbox="91.9,464.2,249.4,5.6" data-line="1" data-segment="0">according to the probability of getting that particular x. That is</span>
</p>
<p>
<span class="f7" data-bbox="246.0,484.7,119.0,8.5" data-line="0" data-segment="0">Hx (y) =   p(i; j)log pi( j):</span>
<span class="f20" data-bbox="289.9,486.8,10.3,9.4" data-line="1" data-segment="0">∑</span>
<span class="f8" data-bbox="291.4,494.8,7.2,4.1" data-line="2" data-segment="0">i; j</span>
</p>
<p>
<span class="f4" data-bbox="91.9,513.7,427.5,5.6" data-line="0" data-segment="0">This quantity measures how uncertain we are of y on the average when we know x. Substituting the value of</span>
<span class="f7" data-bbox="92.6,525.7,61.4,5.6" data-line="1" data-segment="0">pi( j) we obtain</span>
</p>
<p>
<span class="f7" data-bbox="197.3,546.2,216.6,8.5" data-line="0" data-segment="0">Hx(y) =   p(i; j)log p(i; j) + p(i; j)log p(i; j)</span>
<span class="f20" data-bbox="241.2,548.2,147.1,9.4" data-line="1" data-segment="0">∑ ∑ ∑</span>
<span class="f8" data-bbox="242.5,556.3,142.1,4.1" data-line="2" data-segment="0">i; j i; j j</span>
<span class="f9" data-bbox="222.4,571.1,69.0,8.5" data-line="3" data-segment="0">= H(x;y)  H(x)</span>
</p>
<p>
<span class="f4" data-bbox="91.9,591.7,8.4,5.6" data-line="0" data-segment="0">or</span>
<span class="f7" data-bbox="257.3,603.7,96.7,5.5" data-line="1" data-segment="0">H(x;y) = H(x) + Hx(y):</span>
</p>
<p>
<span class="f4" data-bbox="91.9,620.7,427.3,5.6" data-line="0" data-segment="0">The uncertainty (or entropy) of the joint event x;y is the uncertainty of x plus the uncertainty of y when x is</span>
<span class="f4" data-bbox="91.9,632.7,29.6,5.6" data-line="1" data-segment="0">known.</span>
<span class="f4" data-bbox="106.9,644.6,99.1,5.6" data-line="2" data-segment="0">6. From 3 and 5 we have</span>
<span class="f7" data-bbox="225.8,656.6,159.5,8.5" data-line="3" data-segment="0">H(x) +H(y)  H(x;y) = H(x) + Hx(y):</span>
</p>
<p>
<span class="f4" data-bbox="91.9,673.6,25.6,5.6" data-line="0" data-segment="0">Hence</span>
<span class="f7" data-bbox="276.7,685.6,57.8,8.5" data-line="1" data-segment="0">H(y)  Hx (y):</span>
</p>
<p>
<span class="f4" data-bbox="91.9,702.7,427.9,5.6" data-line="0" data-segment="0">The uncertainty of y is never increased by knowledge of x. It will be decreased unless x and y are independent</span>
<span class="f4" data-bbox="91.9,714.7,155.1,5.6" data-line="1" data-segment="0">events, in which case it is not changed.</span>
</p>
<p>
<span class="f4" data-bbox="300.6,744.5,10.0,5.6" data-line="0" data-segment="0">12</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                                                                                                                                                                                            <p>
<span class="f4" data-bbox="203.2,99.9,204.8,5.6" data-line="0" data-segment="0">7. THE ENTROPY OF AN INFORMATION SOURCE</span>
</p>
<p>
<span class="f4" data-bbox="91.9,118.6,427.4,5.6" data-line="0" data-segment="0">Consider a discrete source of the ﬁnite state type considered above. For each possible state i there will be a</span>
<span class="f4" data-bbox="91.9,130.6,427.5,5.6" data-line="1" data-segment="0">set of probabilities pi( j) of producing the various possible symbols j. Thus there is an entropy Hi for each</span>
<span class="f4" data-bbox="91.9,142.5,427.3,5.6" data-line="2" data-segment="0">state. The entropy of the source will be deﬁned as the average of these Hi weighted in accordance with the</span>
<span class="f4" data-bbox="91.9,154.5,200.3,5.6" data-line="3" data-segment="0">probability of occurrence of the states in question:</span>
</p>
<p>
<span class="f7" data-bbox="251.8,176.2,48.0,5.5" data-line="0" data-segment="0">H = PiHi</span>
<span class="f20" data-bbox="271.8,178.4,10.3,9.4" data-line="1" data-segment="0">∑</span>
<span class="f8" data-bbox="275.9,186.3,2.1,4.1" data-line="2" data-segment="0">i</span>
</p>
<p>
<span class="f9" data-bbox="261.8,200.1,97.4,8.5" data-line="0" data-segment="0">=   Pi pi( j)log pi( j):</span>
<span class="f20" data-bbox="280.7,202.1,10.3,9.4" data-line="1" data-segment="0">∑</span>
<span class="f8" data-bbox="282.1,210.2,7.2,4.1" data-line="2" data-segment="0">i; j</span>
</p>
<p>
<span class="f4" data-bbox="91.9,230.3,427.2,5.6" data-line="0" data-segment="0">This is the entropy of the source per symbol of text. If the Markoff process is proceeding at a deﬁnite time</span>
<span class="f4" data-bbox="91.9,242.2,155.2,5.6" data-line="1" data-segment="0">rate there is also an entropy per second</span>
<span class="f14" data-bbox="288.4,250.1,2.0,2.5" data-line="2" data-segment="0">0</span>
<span class="f7" data-bbox="280.4,254.2,49.9,5.5" data-line="3" data-segment="0">H = fi Hi</span>
<span class="f20" data-bbox="303.0,256.4,10.3,9.4" data-line="4" data-segment="0">∑</span>
<span class="f8" data-bbox="307.1,264.3,2.1,4.1" data-line="5" data-segment="0">i</span>
</p>
<p>
<span class="f4" data-bbox="91.9,279.1,306.7,5.6" data-line="0" data-segment="0">where fi is the average frequency (occurrences per second) of state i. Clearly</span>
</p>
<p>
<span class="f14" data-bbox="294.7,296.7,2.0,2.5" data-line="0" data-segment="0">0</span>
<span class="f7" data-bbox="286.8,300.8,37.0,5.5" data-line="1" data-segment="0">H = mH</span>
</p>
<p>
<span class="f14" data-bbox="382.1,318.9,2.0,2.5" data-line="0" data-segment="0">0</span>
<span class="f4" data-bbox="91.9,322.5,427.6,5.6" data-line="1" data-segment="0">where m is the average number of symbols produced per second. H or H measures the amount of informa-</span>
<span class="f4" data-bbox="91.9,334.5,427.2,5.6" data-line="2" data-segment="0">tion generated by the source per symbol or per second. If the logarithmic base is 2, they will represent bits</span>
<span class="f4" data-bbox="91.9,346.5,103.5,5.6" data-line="3" data-segment="0">per symbol or per second.</span>
<span class="f4" data-bbox="106.9,358.4,412.5,8.5" data-line="4" data-segment="0">If successive symbols are independent then H is simply  ∑ pi log pi where pi is the probability of sym-</span>
<span class="f4" data-bbox="91.9,370.4,427.5,5.6" data-line="5" data-segment="0">bol i. Suppose in this case we consider a long message of N symbols. It will contain with high probability</span>
<span class="f4" data-bbox="91.9,382.3,427.3,5.6" data-line="6" data-segment="0">about p1N occurrences of the ﬁrst symbol, p2N occurrences of the second, etc. Hence the probability of this</span>
<span class="f4" data-bbox="91.9,394.3,138.0,5.6" data-line="7" data-segment="0">particular message will be roughly</span>
<span class="f8" data-bbox="285.6,401.2,63.4,5.2" data-line="8" data-segment="0">p1N p2N pnN</span>
<span class="f7" data-bbox="262.1,406.3,74.2,8.5" data-line="9" data-segment="0">p = p p    p</span>
<span class="f6" data-bbox="285.0,408.7,55.1,4.7" data-line="10" data-segment="0">1 2 n</span>
</p>
<p>
<span class="f4" data-bbox="91.9,424.0,8.4,5.6" data-line="0" data-segment="0">or</span>
</p>
<p>
<span class="f10" data-bbox="288.5,440.1,2.8,5.0" data-line="0" data-segment="0">:</span>
<span class="f4" data-bbox="264.1,445.7,82.5,5.6" data-line="1" data-segment="0">log p = N pi log pi</span>
<span class="f20" data-bbox="304.2,447.9,10.3,9.4" data-line="2" data-segment="0">∑</span>
<span class="f8" data-bbox="308.3,455.9,2.1,4.1" data-line="3" data-segment="0">i</span>
<span class="f10" data-bbox="288.5,463.7,2.8,5.0" data-line="4" data-segment="0">:</span>
<span class="f4" data-bbox="264.1,469.4,54.0,8.5" data-line="5" data-segment="0">log p =  NH</span>
</p>
<p>
<span class="f10" data-bbox="288.5,484.4,38.2,6.1" data-line="0" data-segment="0">: log 1=p</span>
<span class="f7" data-bbox="275.9,491.1,54.7,5.5" data-line="1" data-segment="0">H = :</span>
<span class="f7" data-bbox="308.3,497.9,6.7,5.5" data-line="2" data-segment="0">N</span>
</p>
<p>
<span class="f7" data-bbox="91.9,515.7,427.3,5.6" data-line="0" data-segment="0">H is thus approximately the logarithm of the reciprocal probability of a typical long sequence divided by the</span>
<span class="f4" data-bbox="91.9,527.6,427.3,5.6" data-line="1" data-segment="0">number of symbols in the sequence. The same result holds for any source. Stated more precisely we have</span>
<span class="f4" data-bbox="91.9,539.6,71.4,5.6" data-line="2" data-segment="0">(see Appendix 3):</span>
</p>
<p>
<span class="f7" data-bbox="106.9,554.5,411.9,8.5" data-line="0" data-segment="0">Theorem 3: Given any   &gt; 0 and   &gt; 0, we can ﬁnd an N0 such that the sequences of any length N  N0</span>
<span class="f4" data-bbox="91.9,566.5,81.3,5.7" data-line="1" data-segment="0">fall into two classes:</span>
</p>
<p>
<span class="f4" data-bbox="104.4,586.3,182.9,5.7" data-line="0" data-segment="0">1. A set whose total probability is less than  .</span>
</p>
<p>
<span class="f4" data-bbox="104.4,606.1,334.7,5.7" data-line="0" data-segment="0">2. The remainder, all of whose members have probabilities satisfying the inequality</span>
<span class="f19" data-bbox="267.4,617.9,57.0,14.9" data-line="1" data-segment="0">   </span>
<span class="f19" data-bbox="267.4,622.0,57.0,16.9" data-line="2" data-segment="0"> log p 1  </span>
<span class="f19" data-bbox="267.4,629.8,57.0,14.9" data-line="3" data-segment="0">   </span>
<span class="f16" data-bbox="303.8,632.3,40.1,8.5" data-line="4" data-segment="0"> H &lt;  :</span>
<span class="f19" data-bbox="267.4,635.8,57.0,14.9" data-line="5" data-segment="0">   </span>
<span class="f7" data-bbox="283.0,639.2,6.7,5.5" data-line="6" data-segment="0">N</span>
</p>
<p>
<span class="f14" data-bbox="306.7,653.6,9.5,4.1" data-line="0" data-segment="0"> 1</span>
<span class="f4" data-bbox="287.2,657.3,19.5,5.6" data-line="1" data-segment="0">log p</span>
<span class="f4" data-bbox="106.9,664.0,342.5,5.6" data-line="2" data-segment="0">In other words we are almost certain to have very close to H when N is large.</span>
<span class="f7" data-bbox="298.3,670.9,6.7,5.5" data-line="3" data-segment="0">N</span>
<span class="f4" data-bbox="106.9,678.8,412.5,5.6" data-line="4" data-segment="0">A closely related result deals with the number of sequences of various probabilities. Consider again the</span>
<span class="f4" data-bbox="91.9,690.8,427.4,5.6" data-line="5" data-segment="0">sequences of length N and let them be arranged in order of decreasing probability. We deﬁne n(q) to be</span>
<span class="f4" data-bbox="91.9,702.7,427.4,5.6" data-line="6" data-segment="0">the number we must take from this set starting with the most probable one in order to accumulate a total</span>
<span class="f4" data-bbox="91.9,714.7,115.4,5.6" data-line="7" data-segment="0">probability q for those taken.</span>
</p>
<p>
<span class="f4" data-bbox="300.6,744.5,10.0,5.6" data-line="0" data-segment="0">13</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     <p>
<span class="f7" data-bbox="106.9,99.9,45.8,5.5" data-line="0" data-segment="0">Theorem 4:</span>
<span class="f4" data-bbox="289.3,108.5,31.7,5.6" data-line="1" data-segment="0">log n(q)</span>
<span class="f4" data-bbox="269.8,115.3,71.8,5.6" data-line="2" data-segment="0">Lim = H</span>
<span class="f8" data-bbox="269.0,121.7,39.2,5.9" data-line="3" data-segment="0">N!∞ N</span>
</p>
<p>
<span class="f4" data-bbox="91.9,136.6,117.3,5.7" data-line="0" data-segment="0">when q does not equal 0 or 1.</span>
</p>
<p>
<span class="f4" data-bbox="106.9,151.5,412.4,5.6" data-line="0" data-segment="0">We may interpret log n(q) as the number of bits required to specify the sequence when we consider only</span>
<span class="f4" data-bbox="339.4,162.5,31.7,5.6" data-line="1" data-segment="0">log n(q)</span>
<span class="f4" data-bbox="91.9,169.3,427.5,5.6" data-line="2" data-segment="0">the most probable sequences with a total probability q. Then is the number of bits per symbol for</span>
<span class="f7" data-bbox="351.6,176.1,6.7,5.5" data-line="3" data-segment="0">N</span>
<span class="f4" data-bbox="91.9,184.0,427.4,5.6" data-line="4" data-segment="0">the speciﬁcation. The theorem says that for large N this will be independent of q and equal to H. The rate</span>
<span class="f4" data-bbox="91.9,196.0,427.5,5.6" data-line="5" data-segment="0">of growth of the logarithm of the number of reasonably probable sequences is given by H, regardless of our</span>
<span class="f4" data-bbox="91.9,207.9,428.3,5.6" data-line="6" data-segment="0">interpretation of “reasonably probable.” Due to these results, which are proved in Appendix 3, it is possible</span>
<span class="f8" data-bbox="379.0,216.3,10.7,4.1" data-line="7" data-segment="0">HN</span>
<span class="f4" data-bbox="91.9,219.9,427.5,5.6" data-line="8" data-segment="0">for most purposes to treat the long sequences as though there were just 2 of them, each with a probability</span>
<span class="f14" data-bbox="97.0,228.2,16.5,4.1" data-line="9" data-segment="0"> HN</span>
<span class="f4" data-bbox="91.9,231.9,24.9,5.6" data-line="10" data-segment="0">2 .</span>
<span class="f14" data-bbox="286.3,240.2,2.0,2.5" data-line="11" data-segment="0">0</span>
<span class="f4" data-bbox="106.9,243.8,412.5,5.6" data-line="12" data-segment="0">The next two theorems show that H and H can be determined by limiting operations directly from</span>
<span class="f4" data-bbox="91.9,255.8,427.6,5.6" data-line="13" data-segment="0">the statistics of the message sequences, without reference to the states and transition probabilities between</span>
<span class="f4" data-bbox="91.9,267.7,24.6,5.6" data-line="14" data-segment="0">states.</span>
</p>
<p>
<span class="f7" data-bbox="106.9,282.7,359.7,5.7" data-line="0" data-segment="0">Theorem 5: Let p(Bi) be the probability of a sequence Bi of symbols from the source. Let</span>
</p>
<p>
<span class="f4" data-bbox="283.8,302.8,5.0,5.6" data-line="0" data-segment="0">1</span>
<span class="f7" data-bbox="248.5,309.5,114.1,8.5" data-line="1" data-segment="0">GN =   p(Bi)log p(Bi)</span>
<span class="f20" data-bbox="292.2,311.7,10.3,9.4" data-line="2" data-segment="0">∑</span>
<span class="f7" data-bbox="282.7,316.4,6.7,5.5" data-line="3" data-segment="0">N</span>
<span class="f8" data-bbox="296.3,319.6,2.1,4.1" data-line="4" data-segment="0">i</span>
</p>
<p>
<span class="f4" data-bbox="91.9,338.5,427.6,5.7" data-line="0" data-segment="0">where the sum is over all sequences Bi containing N symbols. Then GN is a monotonic decreasing function</span>
<span class="f4" data-bbox="91.9,350.5,35.0,5.7" data-line="1" data-segment="0">of N and</span>
<span class="f4" data-bbox="278.9,362.3,54.2,5.6" data-line="2" data-segment="0">Lim GN = H:</span>
<span class="f8" data-bbox="278.2,368.8,17.9,4.8" data-line="3" data-segment="0">N!∞</span>
</p>
<p>
<span class="f7" data-bbox="106.9,384.4,412.4,5.7" data-line="0" data-segment="0">Theorem 6: Let p(Bi;S j ) be the probability of sequence Bi followed by symbol S j and pB (S j ) =</span>
<span class="f17" data-bbox="489.2,387.2,1.7,3.3" data-line="1" data-segment="0">i</span>
<span class="f7" data-bbox="92.6,396.3,254.2,5.7" data-line="2" data-segment="0">p(Bi;S j )=p(Bi) be the conditional probability of S j after Bi. Let</span>
</p>
<p>
<span class="f7" data-bbox="244.4,418.3,122.4,8.5" data-line="0" data-segment="0">FN =   p(Bi;S j )log pB (S j )</span>
<span class="f20" data-bbox="276.4,420.4,73.1,9.4" data-line="1" data-segment="0">∑ i</span>
<span class="f8" data-bbox="277.8,428.3,7.2,4.1" data-line="2" data-segment="0">i; j</span>
</p>
<p>
<span class="f4" data-bbox="91.9,448.7,427.8,8.5" data-line="0" data-segment="0">where the sum is over all blocks Bi of N  1 symbols and over all symbols S j . Then FN is a monotonic</span>
<span class="f4" data-bbox="91.9,460.6,101.3,5.7" data-line="1" data-segment="0">decreasing function of N,</span>
</p>
<p>
<span class="f7" data-bbox="251.3,482.6,109.7,8.5" data-line="0" data-segment="0">FN = NGN  (N  1)GN 1;</span>
</p>
<p>
<span class="f8" data-bbox="289.1,495.9,4.9,4.1" data-line="0" data-segment="0">N</span>
<span class="f4" data-bbox="276.7,499.9,5.0,5.6" data-line="1" data-segment="0">1</span>
<span class="f7" data-bbox="249.2,506.6,62.2,5.5" data-line="2" data-segment="0">GN = Fn;</span>
<span class="f20" data-bbox="286.6,508.6,10.3,9.4" data-line="3" data-segment="0">∑</span>
<span class="f7" data-bbox="275.6,513.4,6.7,5.5" data-line="4" data-segment="0">N</span>
<span class="f8" data-bbox="285.1,516.8,13.2,4.1" data-line="5" data-segment="0">n=1</span>
</p>
<p>
<span class="f7" data-bbox="251.3,530.2,39.0,8.5" data-line="0" data-segment="0">FN  GN ;</span>
</p>
<p>
<span class="f4" data-bbox="91.9,552.2,86.6,6.3" data-line="0" data-segment="0">and LimN!∞ FN = H.</span>
</p>
<p>
<span class="f4" data-bbox="106.9,567.1,412.6,5.6" data-line="0" data-segment="0">These results are derived in Appendix 3. They show that a series of approximations to H can be obtained</span>
<span class="f4" data-bbox="91.9,579.1,427.4,5.6" data-line="1" data-segment="0">by considering only the statistical structure of the sequences extending over 1;2;:::;N symbols. FN is the</span>
<span class="f6" data-bbox="322.9,587.5,5.7,4.1" data-line="2" data-segment="0">th</span>
<span class="f4" data-bbox="91.9,591.1,427.5,5.6" data-line="3" data-segment="0">better approximation. In fact FN is the entropy of the N order approximation to the source of the type</span>
<span class="f4" data-bbox="91.9,602.9,427.4,5.6" data-line="4" data-segment="0">discussed above. If there are no statistical inﬂuences extending over more than N symbols, that is if the</span>
<span class="f4" data-bbox="91.9,614.9,427.5,8.5" data-line="5" data-segment="0">conditional probability of the next symbol knowing the preceding (N  1) is not changed by a knowledge of</span>
<span class="f4" data-bbox="91.9,626.9,427.4,8.5" data-line="6" data-segment="0">any before that, then FN = H. FN of course is the conditional entropy of the next symbol when the (N  1)</span>
<span class="f4" data-bbox="91.9,638.8,348.6,5.6" data-line="7" data-segment="0">preceding ones are known, while GN is the entropy per symbol of blocks of N symbols.</span>
<span class="f4" data-bbox="106.9,650.8,412.4,5.6" data-line="8" data-segment="0">The ratio of the entropy of a source to the maximum value it could have while still restricted to the same</span>
<span class="f4" data-bbox="91.9,662.7,427.5,5.6" data-line="9" data-segment="0">symbols will be called its relative entropy. This is the maximum compression possible when we encode into</span>
<span class="f4" data-bbox="91.9,674.7,427.5,5.6" data-line="10" data-segment="0">the same alphabet. One minus the relative entropy is the redundancy. The redundancy of ordinary English,</span>
<span class="f4" data-bbox="91.9,686.7,428.2,5.6" data-line="11" data-segment="0">not considering statistical structure over greater distances than about eight letters, is roughly 50%. This</span>
<span class="f4" data-bbox="91.9,698.6,428.2,5.6" data-line="12" data-segment="0">means that when we write English half of what we write is determined by the structure of the language and</span>
<span class="f4" data-bbox="91.9,710.6,427.5,5.6" data-line="13" data-segment="0">half is chosen freely. The ﬁgure 50% was found by several independent methods which all gave results in</span>
</p>
<p>
<span class="f4" data-bbox="300.6,744.5,10.0,5.6" data-line="0" data-segment="0">14</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          <p>
<span class="f4" data-bbox="91.9,99.9,427.7,5.6" data-line="0" data-segment="0">this neighborhood. One is by calculation of the entropy of the approximations to English. A second method</span>
<span class="f4" data-bbox="91.9,111.9,427.6,5.6" data-line="1" data-segment="0">is to delete a certain fraction of the letters from a sample of English text and then let someone attempt to</span>
<span class="f4" data-bbox="91.9,123.9,427.4,5.6" data-line="2" data-segment="0">restore them. If they can be restored when 50% are deleted the redundancy must be greater than 50%. A</span>
<span class="f4" data-bbox="91.9,135.8,254.0,5.6" data-line="3" data-segment="0">third method depends on certain known results in cryptography.</span>
<span class="f4" data-bbox="106.9,147.8,411.8,5.6" data-line="4" data-segment="0">Two extremes of redundancy in English prose are represented by Basic English and by James Joyce’s</span>
<span class="f4" data-bbox="91.9,159.8,426.8,5.6" data-line="5" data-segment="0">book “Finnegans Wake”. The Basic English vocabulary is limited to 850 words and the redundancy is very</span>
<span class="f4" data-bbox="91.9,171.7,427.6,5.6" data-line="6" data-segment="0">high. This is reﬂected in the expansion that occurs when a passage is translated into Basic English. Joyce</span>
<span class="f4" data-bbox="91.9,183.7,408.7,5.6" data-line="7" data-segment="0">on the other hand enlarges the vocabulary and is alleged to achieve a compression of semantic content.</span>
<span class="f4" data-bbox="106.9,195.5,411.6,5.6" data-line="8" data-segment="0">The redundancy of a language is related to the existence of crossword puzzles. If the redundancy is</span>
<span class="f4" data-bbox="91.9,207.5,427.3,5.6" data-line="9" data-segment="0">zero any sequence of letters is a reasonable text in the language and any two-dimensional array of letters</span>
<span class="f4" data-bbox="91.9,219.5,426.7,5.6" data-line="10" data-segment="0">forms a crossword puzzle. If the redundancy is too high the language imposes too many constraints for large</span>
<span class="f4" data-bbox="91.9,231.4,427.4,5.6" data-line="11" data-segment="0">crossword puzzles to be possible. A more detailed analysis shows that if we assume the constraints imposed</span>
<span class="f4" data-bbox="91.9,243.4,427.3,5.6" data-line="12" data-segment="0">by the language are of a rather chaotic and random nature, large crossword puzzles are just possible when</span>
<span class="f4" data-bbox="91.9,255.4,426.7,5.6" data-line="13" data-segment="0">the redundancy is 50%. If the redundancy is 33%, three-dimensional crossword puzzles should be possible,</span>
<span class="f4" data-bbox="91.9,267.3,14.1,5.6" data-line="14" data-segment="0">etc.</span>
</p>
<p>
<span class="f4" data-bbox="155.8,288.3,299.8,5.6" data-line="0" data-segment="0">8. REPRESENTATION OF THE ENCODING AND DECODING OPERATIONS</span>
</p>
<p>
<span class="f4" data-bbox="91.9,306.9,427.4,5.6" data-line="0" data-segment="0">We have yet to represent mathematically the operations performed by the transmitter and receiver in en-</span>
<span class="f4" data-bbox="91.9,318.9,427.2,5.6" data-line="1" data-segment="0">coding and decoding the information. Either of these will be called a discrete transducer. The input to the</span>
<span class="f4" data-bbox="91.9,330.8,427.6,5.6" data-line="2" data-segment="0">transducer is a sequence of input symbols and its output a sequence of output symbols. The transducer may</span>
<span class="f4" data-bbox="91.9,342.8,427.5,5.6" data-line="3" data-segment="0">have an internal memory so that its output depends not only on the present input symbol but also on the past</span>
<span class="f4" data-bbox="91.9,354.8,427.5,5.6" data-line="4" data-segment="0">history. We assume that the internal memory is ﬁnite, i.e., there exist a ﬁnite number m of possible states of</span>
<span class="f4" data-bbox="91.9,366.7,427.6,5.6" data-line="5" data-segment="0">the transducer and that its output is a function of the present state and the present input symbol. The next</span>
<span class="f4" data-bbox="91.9,378.7,427.7,5.6" data-line="6" data-segment="0">state will be a second function of these two quantities. Thus a transducer can be described by two functions:</span>
</p>
<p>
<span class="f7" data-bbox="282.7,400.6,57.2,5.5" data-line="0" data-segment="0">yn = f (xn; n )</span>
<span class="f10" data-bbox="271.3,415.5,67.8,5.6" data-line="1" data-segment="0"> n+1 = g(xn; n)</span>
</p>
<p>
<span class="f4" data-bbox="91.9,437.5,24.5,5.6" data-line="0" data-segment="0">where</span>
</p>
<p>
<span class="f6" data-bbox="134.4,453.8,5.7,4.1" data-line="0" data-segment="0">th</span>
<span class="f7" data-bbox="91.9,457.4,106.1,5.6" data-line="1" data-segment="0">xn is the n input symbol,</span>
</p>
<p>
<span class="f6" data-bbox="265.2,473.7,5.7,4.1" data-line="0" data-segment="0">th</span>
<span class="f10" data-bbox="91.9,477.3,291.3,5.6" data-line="1" data-segment="0"> n is the state of the transducer when the n input symbol is introduced,</span>
</p>
<p>
<span class="f7" data-bbox="91.9,497.2,423.2,5.6" data-line="0" data-segment="0">yn is the output symbol (or sequence of output symbols) produced when xn is introduced if the state is  n.</span>
</p>
<p>
<span class="f4" data-bbox="106.9,517.1,412.7,5.6" data-line="0" data-segment="0">If the output symbols of one transducer can be identiﬁed with the input symbols of a second, they can be</span>
<span class="f4" data-bbox="91.9,529.1,427.5,5.6" data-line="1" data-segment="0">connected in tandem and the result is also a transducer. If there exists a second transducer which operates</span>
<span class="f4" data-bbox="91.9,541.0,427.7,5.6" data-line="2" data-segment="0">on the output of the ﬁrst and recovers the original input, the ﬁrst transducer will be called non-singular and</span>
<span class="f4" data-bbox="91.9,553.0,143.9,5.6" data-line="3" data-segment="0">the second will be called its inverse.</span>
<span class="f7" data-bbox="106.9,567.9,412.2,5.7" data-line="4" data-segment="0">Theorem 7: The output of a ﬁnite state transducer driven by a ﬁnite state statistical source is a ﬁnite</span>
<span class="f4" data-bbox="91.9,579.9,427.6,5.7" data-line="5" data-segment="0">state statistical source, with entropy (per unit time) less than or equal to that of the input. If the transducer</span>
<span class="f4" data-bbox="91.9,591.9,120.3,5.7" data-line="6" data-segment="0">is non-singular they are equal.</span>
<span class="f4" data-bbox="106.9,606.8,412.5,5.6" data-line="7" data-segment="0">Let   represent the state of the source, which produces a sequence of symbols xi; and let   be the state of</span>
<span class="f4" data-bbox="91.9,618.8,427.7,5.6" data-line="8" data-segment="0">the transducer, which produces, in its output, blocks of symbols y j . The combined system can be represented</span>
<span class="f4" data-bbox="91.9,630.7,427.5,5.6" data-line="9" data-segment="0">by the “product state space” of pairs ( ; ). Two points in the space ( 1 ; 1) and ( 2; 2), are connected by</span>
<span class="f4" data-bbox="91.9,642.7,427.5,5.6" data-line="10" data-segment="0">a line if  1 can produce an x which changes  1 to  2, and this line is given the probability of that x in this</span>
<span class="f4" data-bbox="91.9,654.7,427.6,5.6" data-line="11" data-segment="0">case. The line is labeled with the block of y j symbols produced by the transducer. The entropy of the output</span>
<span class="f4" data-bbox="91.9,666.5,427.5,5.6" data-line="12" data-segment="0">can be calculated as the weighted sum over the states. If we sum ﬁrst on   each resulting term is less than or</span>
<span class="f4" data-bbox="91.9,678.5,427.6,5.6" data-line="13" data-segment="0">equal to the corresponding term for  , hence the entropy is not increased. If the transducer is non-singular</span>
<span class="f14" data-bbox="317.5,686.8,48.5,2.5" data-line="14" data-segment="0">0 0 0</span>
<span class="f4" data-bbox="91.9,690.5,427.6,5.6" data-line="15" data-segment="0">let its output be connected to the inverse transducer. If H , H and H are the output entropies of the source,</span>
<span class="f6" data-bbox="316.6,693.4,50.1,4.1" data-line="16" data-segment="0">1 2 3</span>
<span class="f14" data-bbox="299.2,698.8,164.2,2.5" data-line="17" data-segment="0">0 0 0 0 0 0</span>
<span class="f4" data-bbox="91.9,702.4,375.0,8.5" data-line="18" data-segment="0">the ﬁrst and second transducers respectively, then H  H  H = H and therefore H = H .</span>
<span class="f6" data-bbox="298.2,705.3,165.8,4.1" data-line="19" data-segment="0">1 2 3 1 1 2</span>
</p>
<p>
<span class="f4" data-bbox="300.6,744.5,10.0,5.6" data-line="0" data-segment="0">15</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <p>
<span class="f4" data-bbox="106.9,99.9,412.6,5.6" data-line="0" data-segment="0">Suppose we have a system of constraints on possible sequences of the type which can be represented by</span>
<span class="f18" data-bbox="265.2,108.7,8.6,4.1" data-line="1" data-segment="0">(s)</span>
<span class="f4" data-bbox="91.9,113.9,427.1,5.6" data-line="2" data-segment="0">a linear graph as in Fig. 2. If probabilities p were assigned to the various lines connecting state i to state j</span>
<span class="f8" data-bbox="265.2,116.8,5.2,4.1" data-line="3" data-segment="0">i j</span>
<span class="f4" data-bbox="91.9,125.9,427.6,5.6" data-line="4" data-segment="0">this would become a source. There is one particular assignment which maximizes the resulting entropy (see</span>
</p>
<p>
<span class="f4" data-bbox="91.9,137.9,52.6,5.6" data-line="0" data-segment="0">Appendix 4).</span>
</p>
<p>
<span class="f7" data-bbox="106.9,152.8,412.4,5.7" data-line="0" data-segment="0">Theorem 8: Let the system of constraints considered as a channel have a capacity C = logW . If we</span>
</p>
<p>
<span class="f4" data-bbox="91.9,164.8,24.9,5.7" data-line="0" data-segment="0">assign</span>
<span class="f21" data-bbox="330.2,172.4,6.8,3.3" data-line="1" data-segment="0">(s)</span>
<span class="f18" data-bbox="278.9,174.4,51.4,5.7" data-line="2" data-segment="0">(s) B j  `</span>
<span class="f17" data-bbox="330.2,179.0,4.2,3.3" data-line="3" data-segment="0">i j</span>
<span class="f7" data-bbox="273.8,181.4,46.4,5.5" data-line="4" data-segment="0">p = W</span>
<span class="f8" data-bbox="278.9,184.3,5.2,4.1" data-line="5" data-segment="0">i j</span>
<span class="f7" data-bbox="301.9,188.1,8.2,5.5" data-line="6" data-segment="0">Bi</span>
</p>
<p>
<span class="f18" data-bbox="123.0,199.7,106.1,6.0" data-line="0" data-segment="0">(s) th</span>
<span class="f4" data-bbox="91.9,205.1,358.3,5.7" data-line="1" data-segment="0">where ` is the duration of the s symbol leading from state i to state j and the Bi satisfy</span>
<span class="f8" data-bbox="123.0,208.0,5.2,4.1" data-line="2" data-segment="0">i j</span>
</p>
<p>
<span class="f21" data-bbox="331.7,220.9,6.8,3.3" data-line="0" data-segment="0">(s)</span>
<span class="f14" data-bbox="322.8,224.9,8.9,3.7" data-line="1" data-segment="0"> `</span>
<span class="f17" data-bbox="331.7,227.5,4.2,3.3" data-line="2" data-segment="0">i j</span>
<span class="f7" data-bbox="271.7,229.9,50.0,5.5" data-line="3" data-segment="0">Bi = B jW</span>
<span class="f20" data-bbox="292.6,231.9,10.3,9.4" data-line="4" data-segment="0">∑</span>
<span class="f8" data-bbox="293.5,239.9,8.1,4.1" data-line="5" data-segment="0">s; j</span>
</p>
<p>
<span class="f4" data-bbox="91.9,256.1,146.0,5.7" data-line="0" data-segment="0">then H is maximized and equal to C.</span>
</p>
<p>
<span class="f4" data-bbox="106.9,271.0,412.5,5.6" data-line="0" data-segment="0">By proper assignment of the transition probabilities the entropy of symbols on a channel can be maxi-</span>
</p>
<p>
<span class="f4" data-bbox="91.9,283.0,120.0,5.6" data-line="0" data-segment="0">mized at the channel capacity.</span>
</p>
<p>
<span class="f4" data-bbox="170.9,303.5,269.4,5.6" data-line="0" data-segment="0">9. THE FUNDAMENTAL THEOREM FOR A NOISELESS CHANNEL</span>
</p>
<p>
<span class="f4" data-bbox="91.9,322.3,427.5,5.6" data-line="0" data-segment="0">We will now justify our interpretation of H as the rate of generating information by proving that H deter-</span>
</p>
<p>
<span class="f4" data-bbox="91.9,334.3,252.4,5.6" data-line="0" data-segment="0">mines the channel capacity required with most efﬁcient coding.</span>
</p>
<p>
<span class="f7" data-bbox="106.9,349.1,412.5,5.7" data-line="0" data-segment="0">Theorem 9: Let a source have entropy H (bits per symbol) and a channel have a capacity C (bits per</span>
</p>
<p>
<span class="f4" data-bbox="91.9,361.1,427.3,5.7" data-line="0" data-segment="0">second). Then it is possible to encode the output of the source in such a way as to transmit at the average</span>
<span class="f7" data-bbox="110.9,371.3,6.7,5.5" data-line="1" data-segment="0">C</span>
<span class="f4" data-bbox="91.9,378.1,427.3,8.5" data-line="2" data-segment="0">rate    symbols per second over the channel where   is arbitrarily small. It is not possible to transmit at</span>
<span class="f7" data-bbox="110.6,384.9,7.2,5.5" data-line="3" data-segment="0">H</span>
<span class="f7" data-bbox="205.7,392.6,6.7,5.5" data-line="4" data-segment="0">C</span>
<span class="f4" data-bbox="91.9,399.3,125.0,5.7" data-line="5" data-segment="0">an average rate greater than .</span>
<span class="f7" data-bbox="205.3,406.1,7.2,5.5" data-line="6" data-segment="0">H</span>
<span class="f7" data-bbox="261.7,416.8,6.7,5.5" data-line="7" data-segment="0">C</span>
<span class="f4" data-bbox="106.9,423.5,412.6,5.6" data-line="8" data-segment="0">The converse part of the theorem, that cannot be exceeded, may be proved by noting that the entropy</span>
<span class="f7" data-bbox="261.5,430.4,7.2,5.5" data-line="9" data-segment="0">H</span>
<span class="f4" data-bbox="91.9,438.2,427.4,5.6" data-line="10" data-segment="0">of the channel input per second is equal to that of the source, since the transmitter must be non-singular, and</span>
<span class="f14" data-bbox="341.6,446.6,2.0,2.5" data-line="11" data-segment="0">0</span>
<span class="f4" data-bbox="91.9,450.2,427.6,8.5" data-line="12" data-segment="0">also this entropy cannot exceed the channel capacity. Hence H  C and the number of symbols per second</span>
<span class="f14" data-bbox="109.8,458.5,2.0,2.5" data-line="13" data-segment="0">0</span>
<span class="f9" data-bbox="91.9,462.1,67.2,8.5" data-line="14" data-segment="0">= H =H  C=H.</span>
</p>
<p>
<span class="f4" data-bbox="106.9,474.1,412.3,5.6" data-line="0" data-segment="0">The ﬁrst part of the theorem will be proved in two different ways. The ﬁrst method is to consider the</span>
</p>
<p>
<span class="f4" data-bbox="91.9,486.1,427.6,5.6" data-line="0" data-segment="0">set of all sequences of N symbols produced by the source. For N large we can divide these into two groups,</span>
<span class="f18" data-bbox="197.8,494.3,231.5,4.1" data-line="1" data-segment="0">(H+ )N RN</span>
<span class="f4" data-bbox="91.9,497.9,427.4,5.6" data-line="2" data-segment="0">one containing less than 2 members and the second containing less than 2 members (where R is</span>
</p>
<p>
<span class="f4" data-bbox="91.9,509.9,427.5,5.6" data-line="0" data-segment="0">the logarithm of the number of different symbols) and having a total probability less than  . As N increases</span>
<span class="f18" data-bbox="465.2,518.2,24.4,4.1" data-line="1" data-segment="0">(C  )T</span>
<span class="f10" data-bbox="91.9,521.9,427.1,5.6" data-line="2" data-segment="0">  and   approach zero. The number of signals of duration T in the channel is greater than 2 with  </span>
</p>
<p>
<span class="f4" data-bbox="91.9,533.8,142.4,5.6" data-line="0" data-segment="0">small when T is large. if we choose</span>
<span class="f19" data-bbox="290.9,537.4,41.3,14.9" data-line="1" data-segment="0">   </span>
<span class="f7" data-bbox="299.4,544.7,7.2,5.5" data-line="2" data-segment="0">H</span>
<span class="f7" data-bbox="271.9,551.5,66.9,5.5" data-line="3" data-segment="0">T = +   N</span>
<span class="f7" data-bbox="299.6,558.3,6.7,5.5" data-line="4" data-segment="0">C</span>
</p>
<p>
<span class="f4" data-bbox="91.9,572.2,427.5,5.6" data-line="0" data-segment="0">then there will be a sufﬁcient number of sequences of channel symbols for the high probability group when</span>
</p>
<p>
<span class="f7" data-bbox="91.9,584.2,427.5,5.6" data-line="0" data-segment="0">N and T are sufﬁciently large (however small  ) and also some additional ones. The high probability group</span>
</p>
<p>
<span class="f4" data-bbox="91.9,596.1,427.5,5.6" data-line="0" data-segment="0">is coded in an arbitrary one-to-one way into this set. The remaining sequences are represented by larger</span>
</p>
<p>
<span class="f4" data-bbox="91.9,608.1,427.6,5.6" data-line="0" data-segment="0">sequences, starting and ending with one of the sequences not used for the high probability group. This</span>
</p>
<p>
<span class="f4" data-bbox="91.9,620.1,427.5,5.6" data-line="0" data-segment="0">special sequence acts as a start and stop signal for a different code. In between a sufﬁcient time is allowed</span>
</p>
<p>
<span class="f4" data-bbox="91.9,632.0,354.9,5.6" data-line="0" data-segment="0">to give enough different sequences for all the low probability messages. This will require</span>
<span class="f19" data-bbox="292.3,641.5,40.6,14.9" data-line="1" data-segment="0">   </span>
<span class="f7" data-bbox="301.0,648.8,6.1,5.5" data-line="2" data-segment="0">R</span>
<span class="f7" data-bbox="271.2,655.5,68.2,5.6" data-line="3" data-segment="0">T1 = + ' N</span>
<span class="f7" data-bbox="300.4,662.3,6.7,5.5" data-line="4" data-segment="0">C</span>
</p>
<p>
<span class="f4" data-bbox="91.9,678.7,419.8,5.6" data-line="0" data-segment="0">where ' is small. The mean rate of transmission in message symbols per second will then be greater than</span>
</p>
<p>
<span class="f19" data-bbox="255.1,690.2,5.8,14.9" data-line="0" data-segment="0">#</span>
<span class="f19" data-bbox="185.5,692.6,227.5,15.5" data-line="1" data-segment="0">   1    </span>
<span class="f19" data-bbox="316.4,695.6,106.1,15.4" data-line="2" data-segment="0">         1</span>
<span class="f7" data-bbox="220.4,700.4,163.1,5.6" data-line="3" data-segment="0">T T1 H R</span>
<span class="f9" data-bbox="190.8,707.2,235.0,8.5" data-line="4" data-segment="0">(1   ) +   = (1   ) +   +   + ' :</span>
<span class="f7" data-bbox="220.2,714.1,163.3,5.5" data-line="5" data-segment="0">N N C C</span>
</p>
<p>
<span class="f4" data-bbox="300.6,744.5,10.0,5.6" data-line="0" data-segment="0">16</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 <p>
<span class="f7" data-bbox="354.6,96.7,6.7,5.5" data-line="0" data-segment="0">C</span>
<span class="f4" data-bbox="91.9,103.4,273.9,5.6" data-line="1" data-segment="0">As N increases  ,   and ' approach zero and the rate approaches .</span>
<span class="f7" data-bbox="354.2,110.2,7.2,5.5" data-line="2" data-segment="0">H</span>
<span class="f4" data-bbox="106.9,118.0,412.7,5.6" data-line="3" data-segment="0">Another method of performing this coding and thereby proving the theorem can be described as follows:</span>
</p>
<p>
<span class="f4" data-bbox="91.9,130.0,427.3,5.6" data-line="0" data-segment="0">Arrange the messages of length N in order of decreasing probability and suppose their probabilities are</span>
<span class="f8" data-bbox="228.6,137.8,12.3,4.1" data-line="1" data-segment="0">s 1</span>
<span class="f7" data-bbox="92.6,142.0,426.7,8.5" data-line="2" data-segment="0">p1   p2   p3       pn. Let Ps = ∑ pi; that is Ps is the cumulative probability up to, but not including, ps.</span>
<span class="f6" data-bbox="228.6,145.0,3.7,4.1" data-line="3" data-segment="0">1</span>
<span class="f4" data-bbox="91.9,153.9,427.6,5.6" data-line="4" data-segment="0">We ﬁrst encode into a binary system. The binary code for message s is obtained by expanding Ps as a binary</span>
</p>
<p>
<span class="f4" data-bbox="91.9,165.9,339.3,5.6" data-line="0" data-segment="0">number. The expansion is carried out to ms places, where ms is the integer satisfying:</span>
</p>
<p>
<span class="f4" data-bbox="270.8,185.1,84.9,5.6" data-line="0" data-segment="0">1 1</span>
<span class="f4" data-bbox="249.6,191.8,112.1,8.5" data-line="1" data-segment="0">log  ms &lt; 1 + log :</span>
<span class="f6" data-bbox="262.3,194.3,83.5,4.1" data-line="2" data-segment="0">2 2</span>
<span class="f7" data-bbox="269.5,198.7,87.7,5.5" data-line="3" data-segment="0">ps ps</span>
</p>
<p>
<span class="f4" data-bbox="91.9,218.0,427.6,5.6" data-line="0" data-segment="0">Thus the messages of high probability are represented by short codes and those of low probability by long</span>
</p>
<p>
<span class="f4" data-bbox="91.9,229.9,156.4,5.6" data-line="0" data-segment="0">codes. From these inequalities we have</span>
</p>
<p>
<span class="f4" data-bbox="272.3,249.1,59.2,5.6" data-line="0" data-segment="0">1 1</span>
<span class="f16" data-bbox="285.0,255.9,59.4,8.5" data-line="1" data-segment="0">  ps &lt; :</span>
<span class="f8" data-bbox="273.0,259.9,67.1,4.1" data-line="2" data-segment="0">m m  1</span>
<span class="f4" data-bbox="268.0,260.9,62.1,7.4" data-line="3" data-segment="0">2 s 2 s</span>
</p>
<p>
<span class="f4" data-bbox="91.9,279.9,427.7,5.6" data-line="0" data-segment="0">The code for Ps will differ from all succeeding ones in one or more of its ms places, since all the remaining</span>
<span class="f6" data-bbox="152.5,287.9,3.7,4.1" data-line="1" data-segment="0">1</span>
<span class="f7" data-bbox="91.9,291.9,427.4,5.6" data-line="2" data-segment="0">Pi are at least m larger and their binary expansions therefore differ in the ﬁrst ms places. Consequently all</span>
<span class="f6" data-bbox="148.7,294.1,10.4,5.3" data-line="3" data-segment="0">2 s</span>
<span class="f4" data-bbox="91.9,303.8,427.3,5.6" data-line="4" data-segment="0">the codes are different and it is possible to recover the message from its code. If the channel sequences are</span>
</p>
<p>
<span class="f4" data-bbox="91.9,315.8,427.3,5.6" data-line="0" data-segment="0">not already sequences of binary digits, they can be ascribed binary numbers in an arbitrary fashion and the</span>
</p>
<p>
<span class="f4" data-bbox="91.9,327.8,253.2,5.6" data-line="0" data-segment="0">binary code thus translated into signals suitable for the channel.</span>
<span class="f14" data-bbox="200.9,336.1,2.0,2.5" data-line="1" data-segment="0">0</span>
<span class="f4" data-bbox="106.9,339.7,412.2,5.6" data-line="2" data-segment="0">The average number H of binary digits used per symbol of original message is easily estimated. We</span>
</p>
<p>
<span class="f4" data-bbox="91.9,351.7,18.6,5.6" data-line="0" data-segment="0">have</span>
<span class="f4" data-bbox="297.0,359.5,5.0,5.6" data-line="1" data-segment="0">1</span>
<span class="f14" data-bbox="280.0,362.1,2.0,2.5" data-line="2" data-segment="0">0</span>
<span class="f7" data-bbox="272.0,366.2,67.1,5.5" data-line="3" data-segment="0">H = ms ps:</span>
<span class="f20" data-bbox="305.4,368.3,10.3,9.4" data-line="4" data-segment="0">∑</span>
<span class="f7" data-bbox="295.9,373.0,6.7,5.5" data-line="5" data-segment="0">N</span>
</p>
<p>
<span class="f4" data-bbox="91.9,386.5,16.9,5.6" data-line="0" data-segment="0">But,</span>
<span class="f19" data-bbox="214.3,391.3,194.5,14.9" data-line="1" data-segment="0">       </span>
<span class="f4" data-bbox="195.7,395.6,203.8,5.6" data-line="2" data-segment="0">1 1 1 1 1</span>
<span class="f4" data-bbox="220.3,402.3,194.1,8.5" data-line="3" data-segment="0">log p   m p &lt; 1 + log p</span>
<span class="f20" data-bbox="204.1,403.7,213.2,10.1" data-line="4" data-segment="0">∑ 2 s ∑ s s ∑ 2 s</span>
<span class="f7" data-bbox="194.6,409.1,206.5,5.5" data-line="5" data-segment="0">N ps N N ps</span>
</p>
<p>
<span class="f4" data-bbox="91.9,424.6,55.6,5.6" data-line="0" data-segment="0">and therefore,</span>
<span class="f4" data-bbox="338.8,433.6,5.0,5.6" data-line="1" data-segment="0">1</span>
<span class="f14" data-bbox="298.3,436.3,2.0,2.5" data-line="2" data-segment="0">0</span>
<span class="f7" data-bbox="265.1,440.3,70.1,8.5" data-line="3" data-segment="0">GN  H &lt; GN +</span>
<span class="f7" data-bbox="337.7,447.2,6.7,5.5" data-line="4" data-segment="0">N</span>
<span class="f14" data-bbox="356.2,458.1,2.0,2.5" data-line="5" data-segment="0">0</span>
<span class="f4" data-bbox="91.9,461.7,327.0,5.6" data-line="6" data-segment="0">As N increases GN approaches H, the entropy of the source and H approaches H.</span>
</p>
<p>
<span class="f4" data-bbox="106.9,473.7,412.5,5.6" data-line="0" data-segment="0">We see from this that the inefﬁciency in coding, when only a ﬁnite delay of N symbols is used, need</span>
<span class="f6" data-bbox="174.8,481.6,3.7,4.1" data-line="1" data-segment="0">1</span>
<span class="f4" data-bbox="91.9,485.6,427.6,5.6" data-line="2" data-segment="0">not be greater than plus the difference between the true entropy H and the entropy GN calculated for</span>
<span class="f8" data-bbox="174.0,489.1,4.9,4.1" data-line="3" data-segment="0">N</span>
<span class="f4" data-bbox="91.9,497.6,364.2,5.6" data-line="4" data-segment="0">sequences of length N. The per cent excess time needed over the ideal is therefore less than</span>
</p>
<p>
<span class="f7" data-bbox="276.0,516.8,36.0,5.6" data-line="0" data-segment="0">GN 1</span>
<span class="f9" data-bbox="291.6,523.6,45.1,8.5" data-line="1" data-segment="0">+  1:</span>
<span class="f7" data-bbox="278.5,530.5,38.0,5.5" data-line="2" data-segment="0">H HN</span>
</p>
<p>
<span class="f6" data-bbox="495.6,546.1,3.7,4.1" data-line="0" data-segment="0">9</span>
<span class="f4" data-bbox="106.9,549.7,412.4,5.6" data-line="1" data-segment="0">This method of encoding is substantially the same as one found independently by R. M. Fano. His</span>
</p>
<p>
<span class="f4" data-bbox="91.9,561.5,427.3,5.6" data-line="0" data-segment="0">method is to arrange the messages of length N in order of decreasing probability. Divide this series into two</span>
</p>
<p>
<span class="f4" data-bbox="91.9,573.5,427.2,5.6" data-line="0" data-segment="0">groups of as nearly equal probability as possible. If the message is in the ﬁrst group its ﬁrst binary digit</span>
</p>
<p>
<span class="f4" data-bbox="91.9,585.5,427.3,5.6" data-line="0" data-segment="0">will be 0, otherwise 1. The groups are similarly divided into subsets of nearly equal probability and the</span>
</p>
<p>
<span class="f4" data-bbox="91.9,597.4,427.7,5.6" data-line="0" data-segment="0">particular subset determines the second binary digit. This process is continued until each subset contains</span>
</p>
<p>
<span class="f4" data-bbox="91.9,609.4,427.5,5.6" data-line="0" data-segment="0">only one message. It is easily seen that apart from minor differences (generally in the last digit) this amounts</span>
</p>
<p>
<span class="f4" data-bbox="91.9,621.3,238.6,5.6" data-line="0" data-segment="0">to the same thing as the arithmetic process described above.</span>
</p>
<p>
<span class="f4" data-bbox="234.2,642.2,142.8,5.6" data-line="0" data-segment="0">10. DISCUSSION AND EXAMPLES</span>
</p>
<p>
<span class="f4" data-bbox="91.9,660.9,427.3,5.6" data-line="0" data-segment="0">In order to obtain the maximum power transfer from a generator to a load, a transformer must in general be</span>
</p>
<p>
<span class="f4" data-bbox="91.9,672.9,427.6,5.6" data-line="0" data-segment="0">introduced so that the generator as seen from the load has the load resistance. The situation here is roughly</span>
</p>
<p>
<span class="f4" data-bbox="91.9,684.8,427.1,5.6" data-line="0" data-segment="0">analogous. The transducer which does the encoding should match the source to the channel in a statistical</span>
</p>
<p>
<span class="f4" data-bbox="91.9,696.8,427.3,5.6" data-line="0" data-segment="0">sense. The source as seen from the channel through the transducer should have the same statistical structure</span>
</p>
<p>
<span class="f11" data-bbox="102.8,711.8,3.0,3.3" data-line="0" data-segment="0">9</span>
<span class="f1" data-bbox="106.3,714.7,290.4,4.5" data-line="1" data-segment="0">Technical Report No. 65, The Research Laboratory of Electronics, M.I.T., March 17, 1949.</span>
</p>
<p>
<span class="f4" data-bbox="300.6,744.5,10.0,5.6" data-line="0" data-segment="0">17</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          <p>
<span class="f4" data-bbox="91.9,99.9,427.2,5.6" data-line="0" data-segment="0">as the source which maximizes the entropy in the channel. The content of Theorem 9 is that, although an</span>
</p>
<p>
<span class="f4" data-bbox="91.9,111.9,427.4,5.6" data-line="0" data-segment="0">exact match is not in general possible, we can approximate it as closely as desired. The ratio of the actual</span>
</p>
<p>
<span class="f4" data-bbox="91.9,123.9,427.6,5.6" data-line="0" data-segment="0">rate of transmission to the capacity C may be called the efﬁciency of the coding system. This is of course</span>
</p>
<p>
<span class="f4" data-bbox="91.9,135.8,384.4,5.6" data-line="0" data-segment="0">equal to the ratio of the actual entropy of the channel symbols to the maximum possible entropy.</span>
</p>
<p>
<span class="f4" data-bbox="106.9,147.8,412.3,5.6" data-line="0" data-segment="0">In general, ideal or nearly ideal encoding requires a long delay in the transmitter and receiver. In the</span>
</p>
<p>
<span class="f4" data-bbox="91.9,159.8,427.5,5.6" data-line="0" data-segment="0">noiseless case which we have been considering, the main function of this delay is to allow reasonably good</span>
</p>
<p>
<span class="f4" data-bbox="91.9,171.7,427.6,5.6" data-line="0" data-segment="0">matching of probabilities to corresponding lengths of sequences. With a good code the logarithm of the</span>
</p>
<p>
<span class="f4" data-bbox="91.9,183.7,427.3,5.6" data-line="0" data-segment="0">reciprocal probability of a long message must be proportional to the duration of the corresponding signal, in</span>
</p>
<p>
<span class="f4" data-bbox="91.9,195.5,14.9,5.6" data-line="0" data-segment="0">fact</span>
<span class="f19" data-bbox="277.9,200.6,55.6,14.9" data-line="1" data-segment="0">   1  </span>
<span class="f19" data-bbox="277.9,205.3,55.6,16.3" data-line="2" data-segment="0"> log p  </span>
<span class="f19" data-bbox="277.9,212.0,55.6,15.4" data-line="3" data-segment="0">   C </span>
<span class="f7" data-bbox="293.8,218.8,5.6,5.5" data-line="4" data-segment="0">T</span>
</p>
<p>
<span class="f4" data-bbox="91.9,230.2,249.6,5.6" data-line="0" data-segment="0">must be small for all but a small fraction of the long messages.</span>
</p>
<p>
<span class="f4" data-bbox="106.9,242.1,411.9,5.6" data-line="0" data-segment="0">If a source can produce only one particular message its entropy is zero, and no channel is required. For</span>
</p>
<p>
<span class="f4" data-bbox="91.9,254.1,427.5,5.6" data-line="0" data-segment="0">example, a computing machine set up to calculate the successive digits of   produces a deﬁnite sequence</span>
</p>
<p>
<span class="f4" data-bbox="91.9,266.1,427.2,5.6" data-line="0" data-segment="0">with no chance element. No channel is required to “transmit” this to another point. One could construct a</span>
</p>
<p>
<span class="f4" data-bbox="91.9,278.0,427.3,5.6" data-line="0" data-segment="0">second machine to compute the same sequence at the point. However, this may be impractical. In such a case</span>
</p>
<p>
<span class="f4" data-bbox="91.9,290.0,426.7,5.6" data-line="0" data-segment="0">we can choose to ignore some or all of the statistical knowledge we have of the source. We might consider</span>
</p>
<p>
<span class="f4" data-bbox="91.9,302.0,427.7,5.6" data-line="0" data-segment="0">the digits of   to be a random sequence in that we construct a system capable of sending any sequence of</span>
</p>
<p>
<span class="f4" data-bbox="91.9,313.9,427.6,5.6" data-line="0" data-segment="0">digits. In a similar way we may choose to use some of our statistical knowledge of English in constructing</span>
</p>
<p>
<span class="f4" data-bbox="91.9,325.9,427.4,5.6" data-line="0" data-segment="0">a code, but not all of it. In such a case we consider the source with the maximum entropy subject to the</span>
</p>
<p>
<span class="f4" data-bbox="91.9,337.7,427.6,5.6" data-line="0" data-segment="0">statistical conditions we wish to retain. The entropy of this source determines the channel capacity which</span>
</p>
<p>
<span class="f4" data-bbox="91.9,349.7,427.4,5.6" data-line="0" data-segment="0">is necessary and sufﬁcient. In the   example the only information retained is that all the digits are chosen</span>
</p>
<p>
<span class="f4" data-bbox="91.9,361.7,427.4,5.6" data-line="0" data-segment="0">from the set 0;1;:::;9. In the case of English one might wish to use the statistical saving possible due to</span>
</p>
<p>
<span class="f4" data-bbox="91.9,373.6,427.5,5.6" data-line="0" data-segment="0">letter frequencies, but nothing else. The maximum entropy source is then the ﬁrst approximation to English</span>
</p>
<p>
<span class="f4" data-bbox="91.9,385.6,227.4,5.6" data-line="0" data-segment="0">and its entropy determines the required channel capacity.</span>
</p>
<p>
<span class="f4" data-bbox="106.9,397.6,413.2,5.6" data-line="0" data-segment="0">As a simple example of some of these results consider a source which produces a sequence of letters</span>
<span class="f6" data-bbox="286.6,405.5,35.9,4.1" data-line="1" data-segment="0">1 1 1 1</span>
<span class="f4" data-bbox="91.9,409.5,427.8,5.6" data-line="2" data-segment="0">chosen from among A, B, C, D with probabilities , , , , successive symbols being chosen independently.</span>
<span class="f6" data-bbox="286.6,413.0,35.9,4.1" data-line="3" data-segment="0">2 4 8 8</span>
<span class="f4" data-bbox="91.9,421.5,34.1,5.6" data-line="4" data-segment="0">We have</span>
<span class="f19" data-bbox="263.9,431.3,111.4,14.9" data-line="5" data-segment="0">   </span>
<span class="f6" data-bbox="269.5,435.4,99.9,4.1" data-line="6" data-segment="0">1 1 1 1 2 1</span>
<span class="f7" data-bbox="236.0,439.4,127.5,8.5" data-line="7" data-segment="0">H =   log + log + log</span>
<span class="f6" data-bbox="269.5,442.9,99.9,4.1" data-line="8" data-segment="0">2 2 4 4 8 8</span>
<span class="f6" data-bbox="257.3,451.9,3.7,4.1" data-line="9" data-segment="0">7</span>
<span class="f9" data-bbox="246.1,455.8,82.4,5.6" data-line="10" data-segment="0">= bits per symbol:</span>
<span class="f6" data-bbox="257.3,459.3,3.7,4.1" data-line="11" data-segment="0">4</span>
</p>
<p>
<span class="f4" data-bbox="91.9,473.7,427.3,5.6" data-line="0" data-segment="0">Thus we can approximate a coding system to encode messages from this source into binary digits with an</span>
<span class="f6" data-bbox="136.3,481.7,3.7,4.1" data-line="1" data-segment="0">7</span>
<span class="f4" data-bbox="91.9,485.7,427.6,5.6" data-line="2" data-segment="0">average of binary digit per symbol. In this case we can actually achieve the limiting value by the following</span>
<span class="f6" data-bbox="136.3,489.1,3.7,4.1" data-line="3" data-segment="0">4</span>
<span class="f4" data-bbox="91.9,497.6,261.1,5.6" data-line="4" data-segment="0">code (obtained by the method of the second proof of Theorem 9):</span>
</p>
<p>
<span class="f7" data-bbox="285.1,515.1,41.6,5.6" data-line="0" data-segment="0">A 0</span>
</p>
<p>
<span class="f7" data-bbox="285.1,527.0,41.6,5.6" data-line="0" data-segment="0">B 10</span>
</p>
<p>
<span class="f7" data-bbox="284.5,539.0,42.3,5.6" data-line="0" data-segment="0">C 110</span>
</p>
<p>
<span class="f7" data-bbox="284.6,551.0,42.2,5.6" data-line="0" data-segment="0">D 111</span>
</p>
<p>
<span class="f4" data-bbox="91.9,568.3,347.0,5.6" data-line="0" data-segment="0">The average number of binary digits used in encoding a sequence of N symbols will be</span>
</p>
<p>
<span class="f19" data-bbox="247.0,582.7,96.4,14.9" data-line="0" data-segment="0">  2  </span>
<span class="f6" data-bbox="252.8,586.9,107.4,4.1" data-line="1" data-segment="0">1 1 7</span>
<span class="f7" data-bbox="239.8,590.8,131.6,8.5" data-line="2" data-segment="0">N  1 +  2 +  3 = N:</span>
<span class="f6" data-bbox="252.8,594.2,107.4,4.1" data-line="3" data-segment="0">2 4 4</span>
<span class="f4" data-bbox="316.9,597.7,5.0,5.6" data-line="4" data-segment="0">8</span>
</p>
<p>
<span class="f6" data-bbox="338.0,609.7,15.3,4.1" data-line="0" data-segment="0">1 1</span>
<span class="f4" data-bbox="91.9,613.5,427.5,5.6" data-line="1" data-segment="0">It is easily seen that the binary digits 0, 1 have probabilities , so the H for the coded sequences is one</span>
<span class="f6" data-bbox="338.0,617.0,15.3,4.1" data-line="2" data-segment="0">2 2</span>
<span class="f6" data-bbox="282.6,623.0,3.7,4.1" data-line="3" data-segment="0">7</span>
<span class="f4" data-bbox="91.9,626.8,427.4,5.6" data-line="4" data-segment="0">bit per symbol. Since, on the average, we have binary symbols per original letter, the entropies on a time</span>
<span class="f6" data-bbox="282.6,630.3,3.7,4.1" data-line="5" data-segment="0">4</span>
<span class="f4" data-bbox="91.9,638.8,427.4,5.6" data-line="6" data-segment="0">basis are the same. The maximum possible entropy for the original set is log 4 = 2, occurring when A, B, C,</span>
<span class="f6" data-bbox="176.4,646.9,166.2,4.1" data-line="7" data-segment="0">1 1 1 1 7</span>
<span class="f7" data-bbox="91.9,650.7,427.5,5.6" data-line="8" data-segment="0">D have probabilities , , , . Hence the relative entropy is . We can translate the binary sequences into</span>
<span class="f6" data-bbox="176.4,654.2,166.2,4.1" data-line="9" data-segment="0">4 4 4 4 8</span>
<span class="f4" data-bbox="91.9,662.7,285.8,5.6" data-line="10" data-segment="0">the original set of symbols on a two-to-one basis by the following table:</span>
</p>
<p>
<span class="f14" data-bbox="322.4,676.6,2.0,2.5" data-line="0" data-segment="0">0</span>
<span class="f4" data-bbox="285.8,680.2,36.6,5.6" data-line="1" data-segment="0">00 A</span>
<span class="f14" data-bbox="322.4,688.5,2.0,2.5" data-line="2" data-segment="0">0</span>
<span class="f4" data-bbox="285.8,692.1,36.6,5.6" data-line="3" data-segment="0">01 B</span>
<span class="f14" data-bbox="322.4,700.5,2.0,2.5" data-line="4" data-segment="0">0</span>
<span class="f4" data-bbox="285.8,704.1,36.4,5.6" data-line="5" data-segment="0">10 C</span>
<span class="f14" data-bbox="322.9,712.4,2.0,2.5" data-line="6" data-segment="0">0</span>
<span class="f4" data-bbox="285.8,716.1,37.1,5.6" data-line="7" data-segment="0">11 D</span>
</p>
<p>
<span class="f4" data-bbox="300.6,744.5,10.0,5.6" data-line="0" data-segment="0">18</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   <p>
<span class="f4" data-bbox="91.9,99.9,427.8,5.6" data-line="0" data-segment="0">This double process then encodes the original message into the same symbols but with an average compres-</span>
<span class="f6" data-bbox="133.0,107.9,3.7,4.1" data-line="1" data-segment="0">7</span>
<span class="f4" data-bbox="91.9,111.9,48.5,5.6" data-line="2" data-segment="0">sion ratio .</span>
<span class="f6" data-bbox="133.0,115.4,3.7,4.1" data-line="3" data-segment="0">8</span>
<span class="f4" data-bbox="106.9,123.9,412.5,5.6" data-line="4" data-segment="0">As a second example consider a source which produces a sequence of A’s and B’s with probability p for</span>
<span class="f7" data-bbox="91.9,135.8,128.2,8.5" data-line="5" data-segment="0">A and q for B. If p   q we have</span>
<span class="f8" data-bbox="299.3,150.9,47.0,4.1" data-line="6" data-segment="0">p 1 p</span>
<span class="f7" data-bbox="250.2,155.0,82.3,8.5" data-line="7" data-segment="0">H =  log p (1  p)</span>
<span class="f18" data-bbox="333.6,168.1,27.6,4.1" data-line="8" data-segment="0">(1 p)=p</span>
<span class="f9" data-bbox="260.3,172.3,73.3,8.5" data-line="9" data-segment="0">=  p log p(1  p)</span>
<span class="f10" data-bbox="262.8,184.1,34.3,6.1" data-line="10" data-segment="0">: e</span>
<span class="f9" data-bbox="260.3,190.9,41.5,5.6" data-line="11" data-segment="0">= p log :</span>
<span class="f7" data-bbox="292.8,197.7,5.0,5.5" data-line="12" data-segment="0">p</span>
</p>
<p>
<span class="f4" data-bbox="91.9,214.7,427.3,5.6" data-line="0" data-segment="0">In such a case one can construct a fairly good coding of the message on a 0, 1 channel by sending a special</span>
<span class="f4" data-bbox="91.9,226.7,427.6,5.6" data-line="1" data-segment="0">sequence, say 0000, for the infrequent symbol A and then a sequence indicating the number of B’s following</span>
<span class="f4" data-bbox="91.9,238.6,427.4,5.6" data-line="2" data-segment="0">it. This could be indicated by the binary representation with all numbers containing the special sequence</span>
<span class="f4" data-bbox="91.9,250.6,427.7,5.6" data-line="3" data-segment="0">deleted. All numbers up to 16 are represented as usual; 16 is represented by the next binary number after 16</span>
<span class="f4" data-bbox="91.9,262.5,237.6,5.6" data-line="4" data-segment="0">which does not contain four zeros, namely 17 = 10001, etc.</span>
<span class="f4" data-bbox="106.9,274.5,412.3,8.5" data-line="5" data-segment="0">It can be shown that as p !0 the coding approaches ideal provided the length of the special sequence is</span>
<span class="f4" data-bbox="91.9,286.5,71.9,5.6" data-line="6" data-segment="0">properly adjusted.</span>
</p>
<p>
<span class="f13" data-bbox="174.7,312.4,261.9,6.7" data-line="0" data-segment="0">PART II: THE DISCRETE CHANNEL WITH NOISE</span>
</p>
<p>
<span class="f4" data-bbox="186.1,336.3,239.0,5.6" data-line="0" data-segment="0">11. REPRESENTATION OF A NOISY DISCRETE CHANNEL</span>
</p>
<p>
<span class="f4" data-bbox="91.9,354.9,427.6,5.6" data-line="0" data-segment="0">We now consider the case where the signal is perturbed by noise during transmission or at one or the other</span>
<span class="f4" data-bbox="91.9,366.9,427.3,5.6" data-line="1" data-segment="0">of the terminals. This means that the received signal is not necessarily the same as that sent out by the</span>
<span class="f4" data-bbox="91.9,378.9,427.5,5.6" data-line="2" data-segment="0">transmitter. Two cases may be distinguished. If a particular transmitted signal always produces the same</span>
<span class="f4" data-bbox="91.9,390.8,427.5,5.6" data-line="3" data-segment="0">received signal, i.e., the received signal is a deﬁnite function of the transmitted signal, then the effect may be</span>
<span class="f4" data-bbox="91.9,402.8,427.6,5.6" data-line="4" data-segment="0">called distortion. If this function has an inverse — no two transmitted signals producing the same received</span>
<span class="f4" data-bbox="91.9,414.8,427.5,5.6" data-line="5" data-segment="0">signal — distortion may be corrected, at least in principle, by merely performing the inverse functional</span>
<span class="f4" data-bbox="91.9,426.7,129.4,5.6" data-line="6" data-segment="0">operation on the received signal.</span>
<span class="f4" data-bbox="106.9,438.7,412.4,5.6" data-line="7" data-segment="0">The case of interest here is that in which the signal does not always undergo the same change in trans-</span>
<span class="f4" data-bbox="91.9,450.5,427.4,5.6" data-line="8" data-segment="0">mission. In this case we may assume the received signal E to be a function of the transmitted signal S and a</span>
<span class="f4" data-bbox="91.9,462.5,114.8,5.6" data-line="9" data-segment="0">second variable, the noise N.</span>
<span class="f7" data-bbox="281.3,474.5,48.6,5.5" data-line="10" data-segment="0">E = f (S;N)</span>
</p>
<p>
<span class="f4" data-bbox="91.9,490.7,427.5,5.6" data-line="0" data-segment="0">The noise is considered to be a chance variable just as the message was above. In general it may be repre-</span>
<span class="f4" data-bbox="91.9,502.7,427.3,5.6" data-line="1" data-segment="0">sented by a suitable stochastic process. The most general type of noisy discrete channel we shall consider</span>
<span class="f4" data-bbox="91.9,514.7,427.4,5.6" data-line="2" data-segment="0">is a generalization of the ﬁnite state noise-free channel described previously. We assume a ﬁnite number of</span>
<span class="f4" data-bbox="91.9,526.6,122.9,5.6" data-line="3" data-segment="0">states and a set of probabilities</span>
<span class="f7" data-bbox="286.1,538.6,39.6,5.5" data-line="4" data-segment="0">p ;i( ; j):</span>
</p>
<p>
<span class="f4" data-bbox="91.9,554.9,427.6,5.6" data-line="0" data-segment="0">This is the probability, if the channel is in state   and symbol i is transmitted, that symbol j will be received</span>
<span class="f4" data-bbox="91.9,566.8,427.5,5.6" data-line="1" data-segment="0">and the channel left in state  . Thus   and   range over the possible states, i over the possible transmitted</span>
<span class="f4" data-bbox="91.9,578.8,427.7,5.6" data-line="2" data-segment="0">signals and j over the possible received signals. In the case where successive symbols are independently per-</span>
<span class="f4" data-bbox="91.9,590.8,427.7,5.6" data-line="3" data-segment="0">turbed by the noise there is only one state, and the channel is described by the set of transition probabilities</span>
<span class="f7" data-bbox="92.6,602.7,259.0,5.6" data-line="4" data-segment="0">pi( j), the probability of transmitted symbol i being received as j.</span>
<span class="f4" data-bbox="106.9,614.7,412.3,5.6" data-line="5" data-segment="0">If a noisy channel is fed by a source there are two statistical processes at work: the source and the noise.</span>
<span class="f4" data-bbox="91.9,626.6,427.5,5.6" data-line="6" data-segment="0">Thus there are a number of entropies that can be calculated. First there is the entropy H(x) of the source</span>
<span class="f4" data-bbox="91.9,638.6,428.4,5.6" data-line="7" data-segment="0">or of the input to the channel (these will be equal if the transmitter is non-singular). The entropy of the</span>
<span class="f4" data-bbox="91.9,650.6,427.4,5.6" data-line="8" data-segment="0">output of the channel, i.e., the received signal, will be denoted by H(y). In the noiseless case H(y) = H(x).</span>
<span class="f4" data-bbox="91.9,662.5,427.5,5.6" data-line="9" data-segment="0">The joint entropy of input and output will be H(xy). Finally there are two conditional entropies Hx(y) and</span>
<span class="f7" data-bbox="91.9,674.5,427.7,5.6" data-line="10" data-segment="0">Hy(x), the entropy of the output when the input is known and conversely. Among these quantities we have</span>
<span class="f4" data-bbox="91.9,686.5,49.1,5.6" data-line="11" data-segment="0">the relations</span>
<span class="f7" data-bbox="224.4,698.3,162.4,5.5" data-line="12" data-segment="0">H(x;y) = H(x) + Hx(y) = H(y) + Hy(x):</span>
</p>
<p>
<span class="f4" data-bbox="91.9,714.7,310.0,5.6" data-line="0" data-segment="0">All of these entropies can be measured on a per-second or a per-symbol basis.</span>
</p>
<p>
<span class="f4" data-bbox="300.6,744.5,10.0,5.6" data-line="0" data-segment="0">19</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  <p>
<span class="f4" data-bbox="206.6,99.9,197.9,5.6" data-line="0" data-segment="0">12. EQUIVOCATION AND CHANNEL CAPACITY</span>
</p>
<p>
<span class="f4" data-bbox="91.9,118.6,427.5,5.6" data-line="0" data-segment="0">If the channel is noisy it is not in general possible to reconstruct the original message or the transmitted</span>
<span class="f4" data-bbox="91.9,130.6,427.5,5.6" data-line="1" data-segment="0">signal with certainty by any operation on the received signal E. There are, however, ways of transmitting</span>
<span class="f4" data-bbox="91.9,142.5,396.1,5.6" data-line="2" data-segment="0">the information which are optimal in combating noise. This is the problem which we now consider.</span>
<span class="f4" data-bbox="106.9,154.5,413.4,5.6" data-line="3" data-segment="0">Suppose there are two possible symbols 0 and 1, and we are transmitting at a rate of 1000 symbols per</span>
<span class="f6" data-bbox="243.0,162.5,3.7,4.1" data-line="4" data-segment="0">1</span>
<span class="f4" data-bbox="91.9,166.5,427.4,5.6" data-line="5" data-segment="0">second with probabilities p0 = p1 = . Thus our source is producing information at the rate of 1000 bits</span>
<span class="f6" data-bbox="243.0,169.9,3.7,4.1" data-line="6" data-segment="0">2</span>
<span class="f4" data-bbox="91.9,178.4,428.7,5.6" data-line="7" data-segment="0">per second. During transmission the noise introduces errors so that, on the average, 1 in 100 is received</span>
<span class="f4" data-bbox="91.9,190.4,427.8,5.6" data-line="8" data-segment="0">incorrectly (a 0 as 1, or 1 as 0). What is the rate of transmission of information? Certainly less than 1000</span>
<span class="f4" data-bbox="91.9,202.4,428.3,5.6" data-line="9" data-segment="0">bits per second since about 1% of the received symbols are incorrect. Our ﬁrst impulse might be to say</span>
<span class="f4" data-bbox="91.9,214.3,428.2,5.6" data-line="10" data-segment="0">the rate is 990 bits per second, merely subtracting the expected number of errors. This is not satisfactory</span>
<span class="f4" data-bbox="91.9,226.3,428.3,5.6" data-line="11" data-segment="0">since it fails to take into account the recipient’s lack of knowledge of where the errors occur. We may carry</span>
<span class="f4" data-bbox="91.9,238.1,428.6,5.6" data-line="12" data-segment="0">it to an extreme case and suppose the noise so great that the received symbols are entirely independent of</span>
<span class="f6" data-bbox="329.0,246.2,3.7,4.1" data-line="13" data-segment="0">1</span>
<span class="f4" data-bbox="91.9,250.1,427.3,5.6" data-line="14" data-segment="0">the transmitted symbols. The probability of receiving 1 is whatever was transmitted and similarly for 0.</span>
<span class="f6" data-bbox="329.0,253.6,3.7,4.1" data-line="15" data-segment="0">2</span>
<span class="f4" data-bbox="91.9,262.1,427.5,5.6" data-line="16" data-segment="0">Then about half of the received symbols are correct due to chance alone, and we would be giving the system</span>
<span class="f4" data-bbox="91.9,274.0,427.3,5.6" data-line="17" data-segment="0">credit for transmitting 500 bits per second while actually no information is being transmitted at all. Equally</span>
<span class="f4" data-bbox="91.9,286.0,427.2,5.6" data-line="18" data-segment="0">“good” transmission would be obtained by dispensing with the channel entirely and ﬂipping a coin at the</span>
<span class="f4" data-bbox="91.9,298.0,62.6,5.6" data-line="19" data-segment="0">receiving point.</span>
<span class="f4" data-bbox="106.9,309.9,412.7,5.6" data-line="20" data-segment="0">Evidently the proper correction to apply to the amount of information transmitted is the amount of this</span>
<span class="f4" data-bbox="91.9,321.9,427.9,5.6" data-line="21" data-segment="0">information which is missing in the received signal, or alternatively the uncertainty when we have received</span>
<span class="f4" data-bbox="91.9,333.8,427.6,5.6" data-line="22" data-segment="0">a signal of what was actually sent. From our previous discussion of entropy as a measure of uncertainty it</span>
<span class="f4" data-bbox="91.9,345.8,427.7,5.6" data-line="23" data-segment="0">seems reasonable to use the conditional entropy of the message, knowing the received signal, as a measure</span>
<span class="f4" data-bbox="91.9,357.8,427.5,5.6" data-line="24" data-segment="0">of this missing information. This is indeed the proper deﬁnition, as we shall see later. Following this idea</span>
<span class="f4" data-bbox="91.9,369.7,427.5,5.6" data-line="25" data-segment="0">the rate of actual transmission, R, would be obtained by subtracting from the rate of production (i.e., the</span>
<span class="f4" data-bbox="91.9,381.7,245.9,5.6" data-line="26" data-segment="0">entropy of the source) the average rate of conditional entropy.</span>
</p>
<p>
<span class="f7" data-bbox="269.8,403.6,71.6,8.5" data-line="0" data-segment="0">R = H(x) Hy(x)</span>
</p>
<p>
<span class="f4" data-bbox="106.9,425.5,412.6,5.6" data-line="0" data-segment="0">The conditional entropy Hy(x) will, for convenience, be called the equivocation. It measures the average</span>
<span class="f4" data-bbox="91.9,437.5,130.5,5.6" data-line="1" data-segment="0">ambiguity of the received signal.</span>
<span class="f4" data-bbox="106.9,449.3,412.5,5.6" data-line="2" data-segment="0">In the example considered above, if a 0 is received the a posteriori probability that a 0 was transmitted</span>
<span class="f4" data-bbox="91.9,461.3,370.6,5.6" data-line="3" data-segment="0">is .99, and that a 1 was transmitted is .01. These ﬁgures are reversed if a 1 is received. Hence</span>
</p>
<p>
<span class="f7" data-bbox="230.9,483.3,149.8,8.5" data-line="0" data-segment="0">Hy(x) =  [:99 log :99 + 0:01 log0:01]</span>
<span class="f9" data-bbox="256.0,498.2,76.6,5.6" data-line="1" data-segment="0">= :081 bits/symbol</span>
</p>
<p>
<span class="f4" data-bbox="91.9,520.1,427.5,8.5" data-line="0" data-segment="0">or 81 bits per second. We may say that the system is transmitting at a rate 1000  81 = 919 bits per second.</span>
<span class="f4" data-bbox="91.9,532.1,427.4,5.6" data-line="1" data-segment="0">In the extreme case where a 0 is equally likely to be received as a 0 or 1 and similarly for 1, the a posteriori</span>
<span class="f6" data-bbox="160.1,540.1,14.7,4.1" data-line="2" data-segment="0">1 1</span>
<span class="f4" data-bbox="91.9,544.0,101.1,5.6" data-line="3" data-segment="0">probabilities are , and</span>
<span class="f6" data-bbox="160.1,547.5,14.7,4.1" data-line="4" data-segment="0">2 2</span>
<span class="f19" data-bbox="290.6,559.3,73.0,14.9" data-line="5" data-segment="0">   </span>
<span class="f6" data-bbox="295.9,563.3,62.3,4.1" data-line="6" data-segment="0">1 1 1 1</span>
<span class="f7" data-bbox="247.7,567.3,104.6,8.5" data-line="7" data-segment="0">Hy (x) =   log + log</span>
<span class="f6" data-bbox="295.9,570.8,62.3,4.1" data-line="8" data-segment="0">2 2 2 2</span>
<span class="f9" data-bbox="272.9,582.2,74.9,5.6" data-line="9" data-segment="0">= 1 bit per symbol</span>
</p>
<p>
<span class="f4" data-bbox="91.9,604.1,293.8,5.6" data-line="0" data-segment="0">or 1000 bits per second. The rate of transmission is then 0 as it should be.</span>
<span class="f4" data-bbox="106.9,616.1,412.3,5.6" data-line="1" data-segment="0">The following theorem gives a direct intuitive interpretation of the equivocation and also serves to justify</span>
<span class="f4" data-bbox="91.9,628.0,427.4,5.6" data-line="2" data-segment="0">it as the unique appropriate measure. We consider a communication system and an observer (or auxiliary</span>
<span class="f4" data-bbox="91.9,640.0,427.5,5.6" data-line="3" data-segment="0">device) who can see both what is sent and what is recovered (with errors due to noise). This observer notes</span>
<span class="f4" data-bbox="91.9,652.0,427.4,5.6" data-line="4" data-segment="0">the errors in the recovered message and transmits data to the receiving point over a “correction channel” to</span>
<span class="f4" data-bbox="91.9,663.9,357.2,5.6" data-line="5" data-segment="0">enable the receiver to correct the errors. The situation is indicated schematically in Fig. 8.</span>
</p>
<p>
<span class="f7" data-bbox="106.9,678.9,412.5,5.7" data-line="0" data-segment="0">Theorem 10: If the correction channel has a capacity equal to Hy(x) it is possible to so encode the</span>
<span class="f4" data-bbox="91.9,690.8,427.5,5.7" data-line="1" data-segment="0">correction data as to send it over this channel and correct all but an arbitrarily small fraction   of the errors.</span>
<span class="f4" data-bbox="91.9,702.8,243.7,5.7" data-line="2" data-segment="0">This is not possible if the channel capacity is less than Hy (x).</span>
</p>
<p>
<span class="f4" data-bbox="300.6,744.5,10.0,5.6" data-line="0" data-segment="0">20</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                                                                                                   <p>
<span class="f6" data-bbox="323.6,94.4,72.2,4.0" data-line="0" data-segment="0">CORRECTION DATA</span>
</p>
<p>
<span class="f6" data-bbox="275.6,165.3,40.3,4.0" data-line="0" data-segment="0">OBSERVER</span>
</p>
<p>
<span class="f30" data-bbox="387.8,223.3,1.9,1.7" data-line="0" data-segment="0">0</span>
<span class="f6" data-bbox="202.4,225.4,260.4,5.1" data-line="1" data-segment="0">M M M</span>
</p>
<p>
<span class="f6" data-bbox="153.4,244.4,294.6,4.0" data-line="0" data-segment="0">SOURCE TRANSMITTER RECEIVER CORRECTING</span>
<span class="f6" data-bbox="408.4,252.3,28.8,4.0" data-line="1" data-segment="0">DEVICE</span>
</p>
<p>
<span class="f12" data-bbox="213.2,272.0,184.7,5.0" data-line="0" data-segment="0">Fig. 8 — Schematic diagram of a correction system.</span>
</p>
<p>
<span class="f4" data-bbox="106.9,303.5,412.6,5.6" data-line="0" data-segment="0">Roughly then, Hy(x) is the amount of additional information that must be supplied per second at the</span>
<span class="f4" data-bbox="91.9,315.5,189.5,5.6" data-line="1" data-segment="0">receiving point to correct the received message.</span>
<span class="f14" data-bbox="403.0,323.8,2.0,2.5" data-line="2" data-segment="0">0</span>
<span class="f4" data-bbox="106.9,327.4,412.6,5.6" data-line="3" data-segment="0">To prove the ﬁrst part, consider long sequences of received message M and corresponding original</span>
<span class="f4" data-bbox="91.9,339.4,427.5,5.6" data-line="4" data-segment="0">message M. There will be logarithmically T Hy(x) of the M’s which could reasonably have produced each</span>
<span class="f14" data-bbox="100.6,347.7,2.0,2.5" data-line="5" data-segment="0">0</span>
<span class="f7" data-bbox="91.9,351.4,427.6,5.6" data-line="6" data-segment="0">M . Thus we have T Hy (x) binary digits to send each T seconds. This can be done with   frequency of errors</span>
<span class="f4" data-bbox="91.9,363.3,124.7,5.6" data-line="7" data-segment="0">on a channel of capacity Hy(x).</span>
<span class="f4" data-bbox="106.9,375.3,360.3,5.6" data-line="8" data-segment="0">The second part can be proved by noting, ﬁrst, that for any discrete chance variables x, y, z</span>
</p>
<p>
<span class="f7" data-bbox="271.4,396.9,68.4,8.5" data-line="0" data-segment="0">Hy(x;z)  Hy (x):</span>
</p>
<p>
<span class="f4" data-bbox="91.9,418.6,170.8,5.6" data-line="0" data-segment="0">The left-hand side can be expanded to give</span>
</p>
<p>
<span class="f7" data-bbox="258.8,440.2,93.6,8.5" data-line="0" data-segment="0">Hy(z) + Hyz(x)  Hy (x)</span>
<span class="f7" data-bbox="224.9,455.2,161.5,8.5" data-line="1" data-segment="0">Hyz(x)  Hy(x)  Hy(z)  Hy(x)  H(z):</span>
</p>
<p>
<span class="f4" data-bbox="91.9,476.8,427.5,5.6" data-line="0" data-segment="0">If we identify x as the output of the source, y as the received signal and z as the signal sent over the correction</span>
<span class="f4" data-bbox="91.9,488.8,427.5,5.6" data-line="1" data-segment="0">channel, then the right-hand side is the equivocation less the rate of transmission over the correction channel.</span>
<span class="f4" data-bbox="91.9,500.8,427.3,5.6" data-line="2" data-segment="0">If the capacity of this channel is less than the equivocation the right-hand side will be greater than zero and</span>
<span class="f7" data-bbox="91.9,512.7,427.9,5.6" data-line="3" data-segment="0">Hyz(x) &gt; 0. But this is the uncertainty of what was sent, knowing both the received signal and the correction</span>
<span class="f4" data-bbox="91.9,524.7,334.5,5.6" data-line="4" data-segment="0">signal. If this is greater than zero the frequency of errors cannot be arbitrarily small.</span>
<span class="f7" data-bbox="91.9,542.6,38.4,5.5" data-line="5" data-segment="0">Example:</span>
</p>
<p>
<span class="f4" data-bbox="116.9,562.3,402.7,5.6" data-line="0" data-segment="0">Suppose the errors occur at random in a sequence of binary digits: probability p that a digit is wrong</span>
<span class="f4" data-bbox="116.9,574.1,402.3,8.5" data-line="1" data-segment="0">and q = 1   p that it is right. These errors can be corrected if their position is known. Thus the</span>
<span class="f4" data-bbox="116.9,586.1,402.8,5.6" data-line="2" data-segment="0">correction channel need only send information as to these positions. This amounts to transmitting</span>
<span class="f4" data-bbox="116.9,598.1,402.7,5.6" data-line="3" data-segment="0">from a source which produces binary digits with probability p for 1 (incorrect) and q for 0 (correct).</span>
<span class="f4" data-bbox="116.9,610.0,139.2,5.6" data-line="4" data-segment="0">This requires a channel of capacity</span>
<span class="f16" data-bbox="280.6,622.0,75.1,8.5" data-line="5" data-segment="0"> [p log p + q log q]</span>
<span class="f4" data-bbox="116.9,639.8,193.2,5.6" data-line="6" data-segment="0">which is the equivocation of the original system.</span>
</p>
<p>
<span class="f4" data-bbox="106.9,659.5,411.3,5.6" data-line="0" data-segment="0">The rate of transmission R can be written in two other forms due to the identities noted above. We have</span>
</p>
<p>
<span class="f7" data-bbox="250.3,681.2,71.8,8.5" data-line="0" data-segment="0">R = H(x)  Hy(x)</span>
<span class="f9" data-bbox="258.7,696.1,63.4,8.5" data-line="1" data-segment="0">= H(y)  Hx(y)</span>
<span class="f9" data-bbox="258.7,711.1,102.5,8.5" data-line="2" data-segment="0">= H(x) + H(y)  H(x;y):</span>
</p>
<p>
<span class="f4" data-bbox="300.6,744.5,10.0,5.6" data-line="0" data-segment="0">21</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           <p>
<span class="f4" data-bbox="91.9,99.9,427.3,5.6" data-line="0" data-segment="0">The ﬁrst deﬁning expression has already been interpreted as the amount of information sent less the uncer-</span>
<span class="f4" data-bbox="91.9,111.9,427.3,5.6" data-line="1" data-segment="0">tainty of what was sent. The second measures the amount received less the part of this which is due to noise.</span>
<span class="f4" data-bbox="91.9,123.9,427.5,5.6" data-line="2" data-segment="0">The third is the sum of the two amounts less the joint entropy and therefore in a sense is the number of bits</span>
<span class="f4" data-bbox="91.9,135.8,377.8,5.6" data-line="3" data-segment="0">per second common to the two. Thus all three expressions have a certain intuitive signiﬁcance.</span>
<span class="f4" data-bbox="106.9,147.8,412.5,5.6" data-line="4" data-segment="0">The capacity C of a noisy channel should be the maximum possible rate of transmission, i.e., the rate</span>
<span class="f4" data-bbox="91.9,159.8,385.7,5.6" data-line="5" data-segment="0">when the source is properly matched to the channel. We therefore deﬁne the channel capacity by</span>
<span class="f19" data-bbox="292.7,173.6,62.5,14.9" data-line="6" data-segment="0">   </span>
<span class="f7" data-bbox="255.5,181.6,95.2,8.5" data-line="7" data-segment="0">C = Max H(x)  Hy(x)</span>
</p>
<p>
<span class="f4" data-bbox="91.9,203.6,427.2,5.6" data-line="0" data-segment="0">where the maximum is with respect to all possible information sources used as input to the channel. If the</span>
<span class="f4" data-bbox="91.9,215.6,427.5,5.6" data-line="1" data-segment="0">channel is noiseless, Hy(x) = 0. The deﬁnition is then equivalent to that already given for a noiseless channel</span>
<span class="f4" data-bbox="91.9,227.5,232.1,5.6" data-line="2" data-segment="0">since the maximum entropy for the channel is its capacity.</span>
</p>
<p>
<span class="f4" data-bbox="142.8,248.3,325.5,5.6" data-line="0" data-segment="0">13. THE FUNDAMENTAL THEOREM FOR A DISCRETE CHANNEL WITH NOISE</span>
</p>
<p>
<span class="f4" data-bbox="91.9,267.1,427.6,5.6" data-line="0" data-segment="0">It may seem surprising that we should deﬁne a deﬁnite capacity C for a noisy channel since we can never</span>
<span class="f4" data-bbox="91.9,279.1,427.7,5.6" data-line="1" data-segment="0">send certain information in such a case. It is clear, however, that by sending the information in a redundant</span>
<span class="f4" data-bbox="91.9,290.9,427.3,5.6" data-line="2" data-segment="0">form the probability of errors can be reduced. For example, by repeating the message many times and by a</span>
<span class="f4" data-bbox="91.9,302.9,427.7,5.6" data-line="3" data-segment="0">statistical study of the different received versions of the message the probability of errors could be made very</span>
<span class="f4" data-bbox="91.9,314.9,428.1,5.6" data-line="4" data-segment="0">small. One would expect, however, that to make this probability of errors approach zero, the redundancy</span>
<span class="f4" data-bbox="91.9,326.8,427.3,5.6" data-line="5" data-segment="0">of the encoding must increase indeﬁnitely, and the rate of transmission therefore approach zero. This is by</span>
<span class="f4" data-bbox="91.9,338.8,427.4,5.6" data-line="6" data-segment="0">no means true. If it were, there would not be a very well deﬁned capacity, but only a capacity for a given</span>
<span class="f4" data-bbox="91.9,350.8,427.7,5.6" data-line="7" data-segment="0">frequency of errors, or a given equivocation; the capacity going down as the error requirements are made</span>
<span class="f4" data-bbox="91.9,362.7,427.4,5.6" data-line="8" data-segment="0">more stringent. Actually the capacity C deﬁned above has a very deﬁnite signiﬁcance. It is possible to send</span>
<span class="f4" data-bbox="91.9,374.7,427.8,5.6" data-line="9" data-segment="0">information at the rate C through the channel with as small a frequency of errors or equivocation as desired</span>
<span class="f4" data-bbox="91.9,386.6,427.5,5.6" data-line="10" data-segment="0">by proper encoding. This statement is not true for any rate greater than C. If an attempt is made to transmit</span>
<span class="f4" data-bbox="91.9,398.6,427.6,5.6" data-line="11" data-segment="0">at a higher rate than C, say C +R1, then there will necessarily be an equivocation equal to or greater than the</span>
<span class="f4" data-bbox="91.9,410.6,427.4,5.6" data-line="12" data-segment="0">excess R1. Nature takes payment by requiring just that much uncertainty, so that we are not actually getting</span>
<span class="f4" data-bbox="91.9,422.5,139.2,5.6" data-line="13" data-segment="0">any more than C through correctly.</span>
<span class="f4" data-bbox="106.9,434.5,413.2,5.6" data-line="14" data-segment="0">The situation is indicated in Fig. 9. The rate of information into the channel is plotted horizontally and</span>
<span class="f4" data-bbox="91.9,446.5,427.4,5.6" data-line="15" data-segment="0">the equivocation vertically. Any point above the heavy line in the shaded region can be attained and those</span>
<span class="f4" data-bbox="91.9,458.3,427.4,5.6" data-line="16" data-segment="0">below cannot. The points on the line cannot in general be attained, but there will usually be two points on</span>
<span class="f4" data-bbox="91.9,470.3,66.0,5.6" data-line="17" data-segment="0">the line that can.</span>
<span class="f4" data-bbox="106.9,482.2,341.5,5.6" data-line="18" data-segment="0">These results are the main justiﬁcation for the deﬁnition of C and will now be proved.</span>
<span class="f7" data-bbox="106.9,497.2,412.4,5.7" data-line="19" data-segment="0">Theorem 11: Let a discrete channel have the capacity C and a discrete source the entropy per second H.</span>
<span class="f4" data-bbox="91.9,509.2,427.6,8.5" data-line="20" data-segment="0">If H  C there exists a coding system such that the output of the source can be transmitted over the channel</span>
<span class="f4" data-bbox="91.9,521.1,427.5,5.7" data-line="21" data-segment="0">with an arbitrarily small frequency of errors (or an arbitrarily small equivocation). If H &gt; C it is possible</span>
<span class="f4" data-bbox="91.9,533.1,427.5,8.5" data-line="22" data-segment="0">to encode the source so that the equivocation is less than H  C +   where   is arbitrarily small. There is no</span>
<span class="f4" data-bbox="91.9,545.0,262.0,8.5" data-line="23" data-segment="0">method of encoding which gives an equivocation less than H  C.</span>
<span class="f4" data-bbox="106.9,560.0,412.3,5.6" data-line="24" data-segment="0">The method of proving the ﬁrst part of this theorem is not by exhibiting a coding method having the</span>
<span class="f4" data-bbox="91.9,572.0,427.4,5.6" data-line="25" data-segment="0">desired properties, but by showing that such a code must exist in a certain group of codes. In fact we will</span>
</p>
<p>
<span class="f6" data-bbox="271.8,613.4,48.1,4.0" data-line="0" data-segment="0">ATTAINABLE</span>
<span class="f26" data-bbox="210.2,618.1,20.8,5.2" data-line="1" data-segment="0">Hy(x)</span>
<span class="f6" data-bbox="281.0,622.4,29.6,4.0" data-line="2" data-segment="0">REGION</span>
<span class="f12" data-bbox="359.0,629.9,5.1,7.0" data-line="3" data-segment="0">.0</span>
<span class="f12" data-bbox="355.6,635.3,3.2,5.0" data-line="4" data-segment="0">1</span>
<span class="f12" data-bbox="350.0,640.9,3.6,5.0" data-line="5" data-segment="0">=</span>
<span class="f6" data-bbox="345.1,645.8,3.1,4.0" data-line="6" data-segment="0">E</span>
<span class="f6" data-bbox="342.0,648.9,2.8,4.0" data-line="7" data-segment="0">P</span>
<span class="f6" data-bbox="338.0,652.9,3.7,4.0" data-line="8" data-segment="0">O</span>
<span class="f6" data-bbox="334.5,656.4,3.1,4.0" data-line="9" data-segment="0">L</span>
<span class="f6" data-bbox="331.4,659.5,2.8,4.0" data-line="10" data-segment="0">S</span>
</p>
<p>
<span class="f26" data-bbox="297.2,686.3,82.3,5.7" data-line="0" data-segment="0">C H(x)</span>
</p>
<p>
<span class="f12" data-bbox="173.4,703.0,264.1,5.0" data-line="0" data-segment="0">Fig. 9 — The equivocation possible for a given input entropy to a channel.</span>
</p>
<p>
<span class="f4" data-bbox="300.6,744.5,10.0,5.6" data-line="0" data-segment="0">22</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                                                                                                                                      <p>
<span class="f4" data-bbox="91.9,99.9,427.4,5.6" data-line="0" data-segment="0">average the frequency of errors over this group and show that this average can be made less than  . If the</span>
<span class="f4" data-bbox="91.9,111.9,427.5,5.6" data-line="1" data-segment="0">average of a set of numbers is less than   there must exist at least one in the set which is less than  . This</span>
<span class="f4" data-bbox="91.9,123.9,126.0,5.6" data-line="2" data-segment="0">will establish the desired result.</span>
<span class="f4" data-bbox="106.9,135.8,214.9,5.6" data-line="3" data-segment="0">The capacity C of a noisy channel has been deﬁned as</span>
<span class="f19" data-bbox="292.7,149.7,62.5,14.9" data-line="4" data-segment="0">   </span>
<span class="f7" data-bbox="255.5,157.7,95.2,8.5" data-line="5" data-segment="0">C = Max H(x)  Hy(x)</span>
</p>
<p>
<span class="f4" data-bbox="91.9,179.7,427.4,5.6" data-line="0" data-segment="0">where x is the input and y the output. The maximization is over all sources which might be used as input to</span>
<span class="f4" data-bbox="91.9,191.6,48.4,5.6" data-line="1" data-segment="0">the channel.</span>
<span class="f4" data-bbox="106.9,203.6,412.6,5.6" data-line="2" data-segment="0">Let S0 be a source which achieves the maximum capacity C. If this maximum is not actually achieved</span>
<span class="f4" data-bbox="91.9,215.6,427.5,5.6" data-line="3" data-segment="0">by any source let S0 be a source which approximates to giving the maximum rate. Suppose S0 is used as</span>
<span class="f4" data-bbox="91.9,227.5,427.5,5.6" data-line="4" data-segment="0">input to the channel. We consider the possible transmitted and received sequences of a long duration T . The</span>
<span class="f4" data-bbox="91.9,239.5,89.3,5.6" data-line="5" data-segment="0">following will be true:</span>
<span class="f8" data-bbox="459.8,247.7,19.8,4.1" data-line="6" data-segment="0">T H(x)</span>
<span class="f4" data-bbox="106.9,251.3,412.6,5.6" data-line="7" data-segment="0">1. The transmitted sequences fall into two classes, a high probability group with about 2 members</span>
<span class="f4" data-bbox="91.9,263.3,218.3,5.6" data-line="8" data-segment="0">and the remaining sequences of small total probability.</span>
<span class="f8" data-bbox="414.5,271.7,19.8,4.1" data-line="9" data-segment="0">T H(y)</span>
<span class="f4" data-bbox="106.9,275.3,412.4,5.6" data-line="10" data-segment="0">2. Similarly the received sequences have a high probability set of about 2 members and a low</span>
<span class="f4" data-bbox="91.9,287.2,156.4,5.6" data-line="11" data-segment="0">probability set of remaining sequences.</span>
<span class="f8" data-bbox="354.7,295.6,22.2,4.4" data-line="12" data-segment="0">T Hy (x)</span>
<span class="f4" data-bbox="106.9,299.2,412.5,5.6" data-line="13" data-segment="0">3. Each high probability output could be produced by about 2 inputs. The probability of all other</span>
<span class="f4" data-bbox="91.9,311.2,136.4,5.6" data-line="14" data-segment="0">cases has a small total probability.</span>
<span class="f4" data-bbox="106.9,323.1,412.4,5.6" data-line="15" data-segment="0">All the  ’s and  ’s implied by the words “small” and “about” in these statements approach zero as we</span>
<span class="f4" data-bbox="91.9,335.1,250.2,5.6" data-line="16" data-segment="0">allow T to increase and S0 to approach the maximizing source.</span>
<span class="f4" data-bbox="106.9,347.0,412.7,5.6" data-line="17" data-segment="0">The situation is summarized in Fig. 10 where the input sequences are points on the left and output</span>
<span class="f4" data-bbox="91.9,359.0,427.7,5.6" data-line="18" data-segment="0">sequences points on the right. The fan of cross lines represents the range of possible causes for a typical</span>
<span class="f4" data-bbox="91.9,371.0,28.2,5.6" data-line="19" data-segment="0">output.</span>
</p>
<p>
<span class="f26" data-bbox="355.6,392.9,5.5,5.0" data-line="0" data-segment="0">E</span>
</p>
<p>
<span class="f26" data-bbox="248.8,422.9,7.5,5.0" data-line="0" data-segment="0">M</span>
</p>
<p>
<span class="f8" data-bbox="198.1,483.8,18.2,3.9" data-line="0" data-segment="0">H (x)T</span>
<span class="f12" data-bbox="193.7,487.0,4.5,5.0" data-line="1" data-segment="0">2</span>
<span class="f8" data-bbox="398.2,491.8,18.2,3.9" data-line="2" data-segment="0">H (y)T</span>
<span class="f6" data-bbox="169.0,494.9,229.3,5.1" data-line="3" data-segment="0">HIGH PROBABILITY 2</span>
<span class="f6" data-bbox="185.5,503.0,257.3,4.0" data-line="4" data-segment="0">MESSAGES HIGH PROBABILITY</span>
<span class="f6" data-bbox="368.6,510.9,74.2,4.0" data-line="5" data-segment="0">RECEIVED SIGNALS</span>
<span class="f8" data-bbox="297.1,515.8,20.2,3.8" data-line="6" data-segment="0">Hy (x)T</span>
<span class="f12" data-bbox="292.7,519.1,4.5,5.0" data-line="7" data-segment="0">2</span>
<span class="f6" data-bbox="264.1,527.0,83.3,4.0" data-line="8" data-segment="0">REASONABLE CAUSES</span>
<span class="f6" data-bbox="282.4,534.9,46.1,5.0" data-line="9" data-segment="0">FOR EACH E</span>
</p>
<p>
<span class="f8" data-bbox="297.1,613.7,20.2,3.8" data-line="0" data-segment="0">Hx (y)T</span>
<span class="f12" data-bbox="292.7,617.1,4.5,5.0" data-line="1" data-segment="0">2</span>
<span class="f6" data-bbox="262.7,624.9,86.3,4.0" data-line="2" data-segment="0">REASONABLE EFFECTS</span>
<span class="f6" data-bbox="281.5,632.9,48.1,5.0" data-line="3" data-segment="0">FOR EACH M</span>
</p>
<p>
<span class="f12" data-bbox="140.3,662.0,330.4,5.0" data-line="0" data-segment="0">Fig. 10 — Schematic representation of the relations between inputs and outputs in a channel.</span>
</p>
<p>
<span class="f4" data-bbox="106.9,685.6,412.5,5.6" data-line="0" data-segment="0">Now suppose we have another source producing information at rate R with R &lt; C. In the period T this</span>
<span class="f8" data-bbox="164.6,694.0,9.4,4.1" data-line="1" data-segment="0">T R</span>
<span class="f4" data-bbox="91.9,697.6,427.6,5.6" data-line="2" data-segment="0">source will have 2 high probability messages. We wish to associate these with a selection of the possible</span>
<span class="f4" data-bbox="91.9,709.6,427.3,5.6" data-line="3" data-segment="0">channel inputs in such a way as to get a small frequency of errors. We will set up this association in all</span>
</p>
<p>
<span class="f4" data-bbox="300.6,744.5,10.0,5.6" data-line="0" data-segment="0">23</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <p>
<span class="f4" data-bbox="91.9,99.9,427.4,5.6" data-line="0" data-segment="0">possible ways (using, however, only the high probability group of inputs as determined by the source S0)</span>
<span class="f4" data-bbox="91.9,111.9,427.6,5.6" data-line="1" data-segment="0">and average the frequency of errors for this large class of possible coding systems. This is the same as</span>
<span class="f4" data-bbox="91.9,123.9,427.8,5.6" data-line="2" data-segment="0">calculating the frequency of errors for a random association of the messages and channel inputs of duration</span>
<span class="f7" data-bbox="91.9,135.8,427.2,5.6" data-line="3" data-segment="0">T . Suppose a particular output y1 is observed. What is the probability of more than one message in the set</span>
<span class="f8" data-bbox="238.7,144.2,175.1,4.1" data-line="4" data-segment="0">T R T H(x)</span>
<span class="f4" data-bbox="91.9,147.8,427.5,5.6" data-line="5" data-segment="0">of possible causes of y1? There are 2 messages distributed at random in 2 points. The probability of</span>
<span class="f4" data-bbox="91.9,159.8,164.2,5.6" data-line="6" data-segment="0">a particular point being a message is thus</span>
<span class="f8" data-bbox="288.6,167.6,35.9,4.1" data-line="7" data-segment="0">T (R H(x))</span>
<span class="f4" data-bbox="283.6,171.7,44.2,5.6" data-line="8" data-segment="0">2 :</span>
</p>
<p>
<span class="f4" data-bbox="91.9,189.7,427.5,5.6" data-line="0" data-segment="0">The probability that none of the points in the fan is a message (apart from the actual originating message) is</span>
</p>
<p>
<span class="f19" data-bbox="269.2,207.2,87.4,15.4" data-line="0" data-segment="0">    T Hy (x)</span>
<span class="f8" data-bbox="294.0,210.2,43.9,5.5" data-line="1" data-segment="0">T (R H(x)) 2</span>
<span class="f7" data-bbox="250.9,215.8,109.4,8.5" data-line="2" data-segment="0">P = 1  2 :</span>
</p>
<p>
<span class="f4" data-bbox="91.9,237.7,324.8,8.5" data-line="0" data-segment="0">Now R &lt; H(x)  Hy(x) so R  H(x) =  Hy(x)    with   positive. Consequently</span>
</p>
<p>
<span class="f19" data-bbox="266.9,256.1,94.7,15.4" data-line="0" data-segment="0">    T Hy (x)</span>
<span class="f14" data-bbox="291.7,259.0,51.2,5.9" data-line="1" data-segment="0"> T Hy (x) T   2</span>
<span class="f7" data-bbox="248.5,264.7,43.2,8.5" data-line="2" data-segment="0">P = 1  2</span>
</p>
<p>
<span class="f4" data-bbox="91.9,286.6,93.0,8.5" data-line="0" data-segment="0">approaches (as T !∞)</span>
<span class="f14" data-bbox="307.0,294.4,14.5,4.1" data-line="1" data-segment="0"> T  </span>
<span class="f4" data-bbox="286.2,298.5,38.9,8.5" data-line="2" data-segment="0">1  2 :</span>
</p>
<p>
<span class="f4" data-bbox="91.9,316.5,367.7,5.6" data-line="0" data-segment="0">Hence the probability of an error approaches zero and the ﬁrst part of the theorem is proved.</span>
<span class="f4" data-bbox="106.9,328.4,412.5,5.6" data-line="1" data-segment="0">The second part of the theorem is easily shown by noting that we could merely send C bits per second</span>
<span class="f4" data-bbox="91.9,340.4,428.6,5.6" data-line="2" data-segment="0">from the source, completely neglecting the remainder of the information generated. At the receiver the</span>
<span class="f4" data-bbox="91.9,352.4,427.3,8.5" data-line="3" data-segment="0">neglected part gives an equivocation H(x)  C and the part transmitted need only add  . This limit can also</span>
<span class="f4" data-bbox="91.9,364.3,352.8,5.6" data-line="4" data-segment="0">be attained in many other ways, as will be shown when we consider the continuous case.</span>
<span class="f4" data-bbox="106.9,376.3,412.7,5.6" data-line="5" data-segment="0">The last statement of the theorem is a simple consequence of our deﬁnition of C. Suppose we can encode</span>
<span class="f4" data-bbox="91.9,388.3,427.5,8.5" data-line="6" data-segment="0">a source with H(x) = C +a in such a way as to obtain an equivocation Hy (x) = a    with   positive. Then</span>
<span class="f7" data-bbox="91.9,400.1,89.4,5.6" data-line="7" data-segment="0">R = H(x) = C + a and</span>
<span class="f7" data-bbox="262.3,412.1,86.8,8.5" data-line="8" data-segment="0">H(x)  Hy(x) = C +  </span>
</p>
<p>
<span class="f4" data-bbox="91.9,430.0,339.9,8.5" data-line="0" data-segment="0">with   positive. This contradicts the deﬁnition of C as the maximum of H(x)  Hy(x).</span>
<span class="f4" data-bbox="106.9,442.0,412.3,5.6" data-line="1" data-segment="0">Actually more has been proved than was stated in the theorem. If the average of a set of numbers is</span>
<span class="f16" data-bbox="295.3,446.8,93.1,8.5" data-line="2" data-segment="0">p p</span>
<span class="f4" data-bbox="91.9,453.9,427.4,5.6" data-line="3" data-segment="0">within   of of their maximum, a fraction of at most   can be more than   below the maximum. Since   is</span>
<span class="f4" data-bbox="91.9,465.9,347.1,5.6" data-line="4" data-segment="0">arbitrarily small we can say that almost all the systems are arbitrarily close to the ideal.</span>
</p>
<p>
<span class="f4" data-bbox="269.8,486.8,71.7,5.6" data-line="0" data-segment="0">14. DISCUSSION</span>
</p>
<p>
<span class="f4" data-bbox="91.9,505.5,427.3,5.6" data-line="0" data-segment="0">The demonstration of Theorem 11, while not a pure existence proof, has some of the deﬁciencies of such</span>
<span class="f4" data-bbox="91.9,517.5,426.5,5.6" data-line="1" data-segment="0">proofs. An attempt to obtain a good approximation to ideal coding by following the method of the proof is</span>
<span class="f4" data-bbox="91.9,529.4,427.8,5.6" data-line="2" data-segment="0">generally impractical. In fact, apart from some rather trivial cases and certain limiting situations, no explicit</span>
<span class="f4" data-bbox="91.9,541.4,427.6,5.6" data-line="3" data-segment="0">description of a series of approximation to the ideal has been found. Probably this is no accident but is</span>
<span class="f4" data-bbox="91.9,553.4,420.1,5.6" data-line="4" data-segment="0">related to the difﬁculty of giving an explicit construction for a good approximation to a random sequence.</span>
<span class="f4" data-bbox="106.9,565.3,411.7,5.6" data-line="5" data-segment="0">An approximation to the ideal would have the property that if the signal is altered in a reasonable way</span>
<span class="f4" data-bbox="91.9,577.3,427.3,5.6" data-line="6" data-segment="0">by the noise, the original can still be recovered. In other words the alteration will not in general bring it</span>
<span class="f4" data-bbox="91.9,589.3,427.2,5.6" data-line="7" data-segment="0">closer to another reasonable signal than the original. This is accomplished at the cost of a certain amount of</span>
<span class="f4" data-bbox="91.9,601.1,427.8,5.6" data-line="8" data-segment="0">redundancy in the coding. The redundancy must be introduced in the proper way to combat the particular</span>
<span class="f4" data-bbox="91.9,613.1,427.4,5.6" data-line="9" data-segment="0">noise structure involved. However, any redundancy in the source will usually help if it is utilized at the</span>
<span class="f4" data-bbox="91.9,625.0,427.2,5.6" data-line="10" data-segment="0">receiving point. In particular, if the source already has a certain redundancy and no attempt is made to</span>
<span class="f4" data-bbox="91.9,637.0,427.4,5.6" data-line="11" data-segment="0">eliminate it in matching to the channel, this redundancy will help combat noise. For example, in a noiseless</span>
<span class="f4" data-bbox="91.9,649.0,427.5,5.6" data-line="12" data-segment="0">telegraph channel one could save about 50% in time by proper encoding of the messages. This is not done</span>
<span class="f4" data-bbox="91.9,660.9,427.9,5.6" data-line="13" data-segment="0">and most of the redundancy of English remains in the channel symbols. This has the advantage, however,</span>
<span class="f4" data-bbox="91.9,672.9,427.8,5.6" data-line="14" data-segment="0">of allowing considerable noise in the channel. A sizable fraction of the letters can be received incorrectly</span>
<span class="f4" data-bbox="91.9,684.9,427.3,5.6" data-line="15" data-segment="0">and still reconstructed by the context. In fact this is probably not a bad approximation to the ideal in many</span>
<span class="f4" data-bbox="91.9,696.8,427.3,5.6" data-line="16" data-segment="0">cases, since the statistical structure of English is rather involved and the reasonable English sequences are</span>
<span class="f4" data-bbox="91.9,708.8,298.8,5.6" data-line="17" data-segment="0">not too far (in the sense required for the theorem) from a random selection.</span>
</p>
<p>
<span class="f4" data-bbox="300.6,744.5,10.0,5.6" data-line="0" data-segment="0">24</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                                                                                                                                    <p>
<span class="f4" data-bbox="106.9,99.9,412.3,5.6" data-line="0" data-segment="0">As in the noiseless case a delay is generally required to approach the ideal encoding. It now has the</span>
<span class="f4" data-bbox="91.9,111.9,427.3,5.6" data-line="1" data-segment="0">additional function of allowing a large sample of noise to affect the signal before any judgment is made</span>
<span class="f4" data-bbox="91.9,123.9,427.8,5.6" data-line="2" data-segment="0">at the receiving point as to the original message. Increasing the sample size always sharpens the possible</span>
<span class="f4" data-bbox="91.9,135.8,81.9,5.6" data-line="3" data-segment="0">statistical assertions.</span>
<span class="f4" data-bbox="106.9,147.8,412.8,5.6" data-line="4" data-segment="0">The content of Theorem 11 and its proof can be formulated in a somewhat different way which exhibits</span>
<span class="f4" data-bbox="91.9,159.8,427.5,5.6" data-line="5" data-segment="0">the connection with the noiseless case more clearly. Consider the possible signals of duration T and suppose</span>
<span class="f4" data-bbox="91.9,171.7,427.4,5.6" data-line="6" data-segment="0">a subset of them is selected to be used. Let those in the subset all be used with equal probability, and suppose</span>
<span class="f4" data-bbox="91.9,183.7,427.5,5.6" data-line="7" data-segment="0">the receiver is constructed to select, as the original signal, the most probable cause from the subset, when a</span>
<span class="f4" data-bbox="91.9,195.5,427.5,5.6" data-line="8" data-segment="0">perturbed signal is received. We deﬁne N(T;q) to be the maximum number of signals we can choose for the</span>
<span class="f4" data-bbox="91.9,207.5,342.1,5.6" data-line="9" data-segment="0">subset such that the probability of an incorrect interpretation is less than or equal to q.</span>
<span class="f4" data-bbox="182.6,221.2,43.7,5.6" data-line="10" data-segment="0">log N(T;q)</span>
<span class="f7" data-bbox="106.9,227.9,412.5,5.7" data-line="11" data-segment="0">Theorem 12: Lim = C, where C is the channel capacity, provided that q does not equal 0 or</span>
<span class="f8" data-bbox="162.7,234.3,44.0,6.0" data-line="12" data-segment="0">T !∞ T</span>
<span class="f4" data-bbox="91.9,243.2,7.5,5.7" data-line="13" data-segment="0">1.</span>
<span class="f4" data-bbox="106.9,258.1,411.1,5.6" data-line="14" data-segment="0">In other words, no matter how we set out limits of reliability, we can distinguish reliably in time T</span>
<span class="f4" data-bbox="91.9,270.1,427.7,5.6" data-line="15" data-segment="0">enough messages to correspond to about CT bits, when T is sufﬁciently large. Theorem 12 can be compared</span>
<span class="f4" data-bbox="91.9,281.9,300.5,5.6" data-line="16" data-segment="0">with the deﬁnition of the capacity of a noiseless channel given in Section 1.</span>
</p>
<p>
<span class="f4" data-bbox="176.5,302.9,258.0,5.6" data-line="0" data-segment="0">15. EXAMPLE OF A DISCRETE CHANNEL AND ITS CAPACITY</span>
</p>
<p>
<span class="f4" data-bbox="91.9,321.7,427.5,5.6" data-line="0" data-segment="0">A simple example of a discrete channel is indicated in Fig. 11. There are three possible symbols. The ﬁrst is</span>
<span class="f4" data-bbox="91.9,333.5,427.5,5.6" data-line="1" data-segment="0">never affected by noise. The second and third each have probability p of coming through undisturbed, and</span>
<span class="f7" data-bbox="91.9,345.5,427.4,8.5" data-line="2" data-segment="0">q of being changed into the other of the pair. We have (letting   =  [p log p + q log q] and P and Q be the</span>
</p>
<p>
<span class="f26" data-bbox="303.7,394.0,4.5,5.0" data-line="0" data-segment="0">p</span>
</p>
<p>
<span class="f26" data-bbox="283.4,418.0,4.5,5.0" data-line="0" data-segment="0">q</span>
<span class="f6" data-bbox="218.2,422.3,167.2,4.0" data-line="1" data-segment="0">TRANSMITTED RECEIVED</span>
<span class="f6" data-bbox="227.4,430.3,156.8,4.0" data-line="2" data-segment="0">SYMBOLS SYMBOLS</span>
<span class="f26" data-bbox="283.4,436.0,4.5,5.0" data-line="3" data-segment="0">q</span>
</p>
<p>
<span class="f26" data-bbox="303.7,458.9,4.5,5.0" data-line="0" data-segment="0">p</span>
<span class="f12" data-bbox="232.3,476.9,146.4,5.0" data-line="1" data-segment="0">Fig. 11 — Example of a discrete channel.</span>
</p>
<p>
<span class="f4" data-bbox="91.9,500.2,203.6,5.6" data-line="0" data-segment="0">probabilities of using the ﬁrst and second symbols)</span>
</p>
<p>
<span class="f7" data-bbox="251.0,522.2,112.0,8.5" data-line="0" data-segment="0">H(x) =  P log P  2Q log Q</span>
<span class="f7" data-bbox="248.3,537.1,56.4,5.6" data-line="1" data-segment="0">Hy(x) = 2Q :</span>
</p>
<p>
<span class="f4" data-bbox="91.9,559.0,427.4,8.5" data-line="0" data-segment="0">We wish to choose P and Q in such a way as to maximize H(x) Hy(x), subject to the constraint P +2Q = 1.</span>
<span class="f4" data-bbox="91.9,571.0,75.9,5.6" data-line="1" data-segment="0">Hence we consider</span>
<span class="f7" data-bbox="214.4,582.9,182.0,8.5" data-line="2" data-segment="0">U =  P log P  2Q log Q  2Q +  (P + 2Q)</span>
</p>
<p>
<span class="f25" data-bbox="236.5,609.8,11.7,6.5" data-line="0" data-segment="0">∂U</span>
<span class="f9" data-bbox="252.5,616.5,86.6,8.5" data-line="1" data-segment="0">=  1  log P +   = 0</span>
<span class="f25" data-bbox="237.2,623.3,11.0,6.5" data-line="2" data-segment="0">∂P</span>
<span class="f25" data-bbox="236.5,635.0,11.7,6.5" data-line="3" data-segment="0">∂U</span>
<span class="f9" data-bbox="252.5,641.7,123.5,8.5" data-line="4" data-segment="0">=  2  2 log Q  2 + 2  = 0:</span>
<span class="f25" data-bbox="236.8,648.5,12.1,6.5" data-line="5" data-segment="0">∂Q</span>
</p>
<p>
<span class="f4" data-bbox="91.9,668.2,55.3,5.6" data-line="0" data-segment="0">Eliminating  </span>
</p>
<p>
<span class="f4" data-bbox="268.2,690.2,70.2,5.6" data-line="0" data-segment="0">log P = log Q +  </span>
<span class="f29" data-bbox="311.9,701.0,5.2,3.2" data-line="1" data-segment="0"> </span>
<span class="f7" data-bbox="282.0,705.1,60.6,5.5" data-line="2" data-segment="0">P = Qe = Q </span>
</p>
<p>
<span class="f4" data-bbox="300.6,744.5,10.0,5.6" data-line="0" data-segment="0">25</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           <p>
<span class="f10" data-bbox="268.7,96.9,88.9,5.6" data-line="0" data-segment="0">  1</span>
<span class="f7" data-bbox="241.4,103.6,128.3,5.5" data-line="1" data-segment="0">P = Q = :</span>
<span class="f10" data-bbox="260.9,110.5,105.1,5.6" data-line="2" data-segment="0">  + 2   + 2</span>
</p>
<p>
<span class="f4" data-bbox="91.9,126.2,113.4,5.6" data-line="0" data-segment="0">The channel capacity is then</span>
<span class="f10" data-bbox="309.6,136.3,21.8,5.6" data-line="1" data-segment="0">  + 2</span>
<span class="f7" data-bbox="275.5,143.0,59.6,5.6" data-line="2" data-segment="0">C = log :</span>
<span class="f10" data-bbox="317.4,149.8,5.6,5.0" data-line="3" data-segment="0"> </span>
</p>
<p>
<span class="f6" data-bbox="380.5,163.7,3.7,4.1" data-line="0" data-segment="0">1</span>
<span class="f4" data-bbox="106.9,167.6,412.4,5.6" data-line="1" data-segment="0">Note how this checks the obvious values in the cases p = 1 and p = . In the ﬁrst,   = 1 and C = log 3,</span>
<span class="f6" data-bbox="380.5,171.1,3.7,4.1" data-line="2" data-segment="0">2</span>
<span class="f6" data-bbox="464.6,176.9,3.7,4.1" data-line="3" data-segment="0">1</span>
<span class="f4" data-bbox="91.9,180.9,427.5,5.6" data-line="4" data-segment="0">which is correct since the channel is then noiseless with three possible symbols. If p = ,   = 2 and</span>
<span class="f6" data-bbox="464.6,184.4,3.7,4.1" data-line="5" data-segment="0">2</span>
<span class="f7" data-bbox="91.4,192.9,428.2,5.6" data-line="6" data-segment="0">C = log 2. Here the second and third symbols cannot be distinguished at all and act together like one</span>
<span class="f6" data-bbox="311.9,200.9,3.7,4.1" data-line="7" data-segment="0">1</span>
<span class="f4" data-bbox="91.9,204.8,427.7,5.6" data-line="8" data-segment="0">symbol. The ﬁrst symbol is used with probability P = and the second and third together with probability</span>
<span class="f6" data-bbox="311.9,208.3,3.7,4.1" data-line="9" data-segment="0">2</span>
<span class="f6" data-bbox="93.1,214.1,3.7,4.1" data-line="10" data-segment="0">1</span>
<span class="f4" data-bbox="98.0,218.1,397.1,5.6" data-line="11" data-segment="0">. This may be distributed between them in any desired way and still achieve the maximum capacity.</span>
<span class="f6" data-bbox="93.1,221.6,3.7,4.1" data-line="12" data-segment="0">2</span>
<span class="f4" data-bbox="106.9,230.1,412.6,5.6" data-line="13" data-segment="0">For intermediate values of p the channel capacity will lie between log 2 and log 3. The distinction</span>
<span class="f4" data-bbox="91.9,242.0,427.3,5.6" data-line="14" data-segment="0">between the second and third symbols conveys some information but not as much as in the noiseless case.</span>
<span class="f4" data-bbox="91.9,254.0,419.2,5.6" data-line="15" data-segment="0">The ﬁrst symbol is used somewhat more frequently than the other two because of its freedom from noise.</span>
</p>
<p>
<span class="f4" data-bbox="179.9,274.9,251.4,5.6" data-line="0" data-segment="0">16. THE CHANNEL CAPACITY IN CERTAIN SPECIAL CASES</span>
</p>
<p>
<span class="f4" data-bbox="91.9,293.6,427.5,5.6" data-line="0" data-segment="0">If the noise affects successive channel symbols independently it can be described by a set of transition</span>
<span class="f4" data-bbox="91.9,305.6,427.5,5.6" data-line="1" data-segment="0">probabilities pi j . This is the probability, if symbol i is sent, that j will be received. The maximum channel</span>
<span class="f4" data-bbox="91.9,317.5,148.7,5.6" data-line="2" data-segment="0">rate is then given by the maximum of</span>
</p>
<p>
<span class="f16" data-bbox="229.3,339.4,151.9,8.5" data-line="0" data-segment="0">  Pi pi j log Pi pi j + Pi pi j log pi j</span>
<span class="f20" data-bbox="238.2,341.6,97.0,9.4" data-line="1" data-segment="0">∑ ∑ ∑</span>
<span class="f8" data-bbox="239.5,349.5,93.9,4.1" data-line="2" data-segment="0">i; j i i; j</span>
</p>
<p>
<span class="f4" data-bbox="91.9,369.8,384.6,7.2" data-line="0" data-segment="0">where we vary the Pi subject to ∑ Pi = 1. This leads by the method of Lagrange to the equations,</span>
</p>
<p>
<span class="f7" data-bbox="276.4,387.3,11.1,5.5" data-line="0" data-segment="0">ps j</span>
<span class="f7" data-bbox="239.0,394.1,145.2,5.6" data-line="1" data-segment="0">p log =   s = 1;2;::::</span>
<span class="f20" data-bbox="226.9,395.6,23.2,10.1" data-line="2" data-segment="0">∑ s j</span>
<span class="f25" data-bbox="266.9,401.0,24.1,7.2" data-line="3" data-segment="0">∑ P p</span>
<span class="f8" data-bbox="231.5,402.4,64.7,6.0" data-line="4" data-segment="0">j i i i j</span>
</p>
<p>
<span class="f4" data-bbox="91.9,424.6,427.5,5.6" data-line="0" data-segment="0">Multiplying by Ps and summing on s shows that   = C. Let the inverse of ps j (if it exists) be hst so that</span>
</p>
<p>
<span class="f25" data-bbox="91.9,436.6,86.5,7.2" data-line="0" data-segment="0">∑s hst ps j =  t j . Then:</span>
<span class="f7" data-bbox="236.9,448.6,148.9,8.5" data-line="1" data-segment="0">h p log p  log P p = C h :</span>
<span class="f20" data-bbox="225.5,450.1,156.5,10.0" data-line="2" data-segment="0">∑ st s j s j ∑ i it ∑ st</span>
<span class="f8" data-bbox="226.4,458.2,141.1,4.6" data-line="3" data-segment="0">s; j i s</span>
</p>
<p>
<span class="f4" data-bbox="91.9,474.9,301.2,15.8" data-line="0" data-segment="0">Hence: h i</span>
<span class="f7" data-bbox="229.4,486.8,152.2,8.5" data-line="1" data-segment="0">P p = exp  C h + h p log p</span>
<span class="f20" data-bbox="218.2,488.2,169.6,10.1" data-line="2" data-segment="0">∑ i it ∑ st ∑ st s j s j</span>
<span class="f8" data-bbox="222.2,496.5,113.4,4.6" data-line="3" data-segment="0">i s s; j</span>
</p>
<p>
<span class="f4" data-bbox="91.9,511.0,10.5,5.6" data-line="0" data-segment="0">or,</span>
<span class="f19" data-bbox="272.9,513.8,119.0,14.9" data-line="1" data-segment="0">h i</span>
<span class="f7" data-bbox="216.6,524.8,178.1,8.5" data-line="2" data-segment="0">Pi = hit exp  C hst + hst ps j log ps j :</span>
<span class="f20" data-bbox="236.4,527.0,99.4,9.4" data-line="3" data-segment="0">∑ ∑ ∑</span>
<span class="f8" data-bbox="240.0,534.4,94.5,4.5" data-line="4" data-segment="0">t s s; j</span>
</p>
<p>
<span class="f4" data-bbox="106.9,551.2,412.4,5.6" data-line="0" data-segment="0">This is the system of equations for determining the maximizing values of Pi, with C to be determined so</span>
<span class="f4" data-bbox="91.9,563.2,427.4,7.2" data-line="1" data-segment="0">that ∑ Pi = 1. When this is done C will be the channel capacity, and the Pi the proper probabilities for the</span>
<span class="f4" data-bbox="91.9,575.1,164.1,5.6" data-line="2" data-segment="0">channel symbols to achieve this capacity.</span>
<span class="f4" data-bbox="106.9,587.1,412.8,5.6" data-line="3" data-segment="0">If each input symbol has the same set of probabilities on the lines emerging from it, and the same is true</span>
<span class="f4" data-bbox="91.9,599.1,427.6,5.6" data-line="4" data-segment="0">of each output symbol, the capacity can be easily calculated. Examples are shown in Fig. 12. In such a case</span>
<span class="f7" data-bbox="91.9,611.0,427.0,8.5" data-line="5" data-segment="0">Hx (y) is independent of the distribution of probabilities on the input symbols, and is given by  ∑ pi log pi</span>
<span class="f4" data-bbox="91.9,623.0,417.2,5.6" data-line="6" data-segment="0">where the pi are the values of the transition probabilities from any input symbol. The channel capacity is</span>
</p>
<p>
<span class="f19" data-bbox="230.0,636.8,61.8,14.9" data-line="0" data-segment="0">   </span>
<span class="f4" data-bbox="211.8,644.9,187.7,8.5" data-line="1" data-segment="0">Max H(y)  Hx(y) = Max H(y) + pi log pi:</span>
<span class="f20" data-bbox="354.0,647.0,10.3,9.4" data-line="2" data-segment="0">∑</span>
</p>
<p>
<span class="f4" data-bbox="91.9,666.8,427.5,5.6" data-line="0" data-segment="0">The maximum of H(y) is clearly log m where m is the number of output symbols, since it is possible to make</span>
<span class="f4" data-bbox="91.9,678.8,427.6,5.6" data-line="1" data-segment="0">them all equally probable by making the input symbols equally probable. The channel capacity is therefore</span>
</p>
<p>
<span class="f7" data-bbox="257.3,700.7,96.1,5.6" data-line="0" data-segment="0">C = log m + pi log pi:</span>
<span class="f20" data-bbox="308.0,702.8,10.3,9.4" data-line="1" data-segment="0">∑</span>
</p>
<p>
<span class="f4" data-bbox="300.6,744.5,10.0,5.6" data-line="0" data-segment="0">26</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                                                       <p>
<span class="f24" data-bbox="178.7,100.3,253.7,5.5" data-line="0" data-segment="0">1/2 1/2</span>
<span class="f24" data-bbox="298.7,109.3,13.7,4.5" data-line="1" data-segment="0">1/3</span>
<span class="f24" data-bbox="187.7,117.2,237.7,5.6" data-line="2" data-segment="0">1/2 1/3</span>
<span class="f24" data-bbox="383.8,124.3,13.7,4.5" data-line="3" data-segment="0">1/6</span>
<span class="f24" data-bbox="162.7,130.3,179.6,5.5" data-line="4" data-segment="0">1/2 1/3</span>
<span class="f24" data-bbox="264.7,141.2,132.7,5.6" data-line="5" data-segment="0">1/6 1/6</span>
<span class="f24" data-bbox="332.8,147.2,83.6,4.5" data-line="6" data-segment="0">1/6 1/2</span>
<span class="f24" data-bbox="160.7,152.2,13.7,4.5" data-line="7" data-segment="0">1/2</span>
<span class="f24" data-bbox="332.8,157.3,13.7,4.5" data-line="8" data-segment="0">1/6</span>
<span class="f24" data-bbox="155.8,162.2,122.6,4.5" data-line="9" data-segment="0">1/2 1/6</span>
<span class="f24" data-bbox="383.8,164.2,13.7,4.5" data-line="10" data-segment="0">1/3</span>
<span class="f24" data-bbox="328.7,174.2,13.7,4.5" data-line="11" data-segment="0">1/3</span>
<span class="f24" data-bbox="191.8,179.2,205.7,4.5" data-line="12" data-segment="0">1/2 1/3</span>
<span class="f24" data-bbox="138.7,185.2,288.7,5.5" data-line="13" data-segment="0">1/2 1/6</span>
<span class="f24" data-bbox="298.7,193.3,13.7,4.5" data-line="14" data-segment="0">1/3</span>
</p>
<p>
<span class="f24" data-bbox="178.7,204.2,253.7,5.6" data-line="0" data-segment="0">1/2 1/2</span>
</p>
<p>
<span class="f4" data-bbox="183.4,225.2,244.4,6.7" data-line="0" data-segment="0">a b c</span>
</p>
<p>
<span class="f12" data-bbox="102.4,242.0,406.4,5.0" data-line="0" data-segment="0">Fig. 12 — Examples of discrete channels with the same transition probabilities for each input and for each output.</span>
</p>
<p>
<span class="f4" data-bbox="91.9,272.3,90.7,5.6" data-line="0" data-segment="0">In Fig. 12a it would be</span>
</p>
<p>
<span class="f7" data-bbox="254.9,284.2,101.0,8.5" data-line="0" data-segment="0">C = log 4  log 2 = log 2:</span>
</p>
<p>
<span class="f4" data-bbox="91.9,301.1,292.0,5.6" data-line="0" data-segment="0">This could be achieved by using only the 1st and 3d symbols. In Fig. 12b</span>
</p>
<p>
<span class="f6" data-bbox="299.5,317.3,40.2,4.1" data-line="0" data-segment="0">2 1</span>
<span class="f7" data-bbox="250.0,321.3,110.8,8.5" data-line="1" data-segment="0">C = log 4   log 3   log 6</span>
<span class="f6" data-bbox="299.5,324.7,40.2,4.1" data-line="2" data-segment="0">3 3</span>
<span class="f6" data-bbox="328.8,333.8,3.7,4.1" data-line="3" data-segment="0">1</span>
<span class="f9" data-bbox="259.0,337.6,94.6,8.5" data-line="4" data-segment="0">= log 4  log 3   log 2</span>
<span class="f6" data-bbox="328.8,341.1,3.7,4.1" data-line="5" data-segment="0">3</span>
<span class="f11" data-bbox="295.1,349.4,3.0,3.3" data-line="6" data-segment="0">5</span>
<span class="f6" data-bbox="284.0,352.4,3.7,4.1" data-line="7" data-segment="0">1</span>
<span class="f9" data-bbox="259.0,354.8,43.6,7.1" data-line="8" data-segment="0">= log 2 3 :</span>
<span class="f6" data-bbox="284.0,359.8,3.7,4.1" data-line="9" data-segment="0">3</span>
</p>
<p>
<span class="f4" data-bbox="91.9,376.5,78.6,5.6" data-line="0" data-segment="0">In Fig. 12c we have</span>
</p>
<p>
<span class="f6" data-bbox="281.3,392.7,76.7,4.1" data-line="0" data-segment="0">1 1 1</span>
<span class="f7" data-bbox="231.7,396.5,147.3,8.5" data-line="1" data-segment="0">C = log 3   log 2   log 3   log 6</span>
<span class="f6" data-bbox="281.3,400.0,76.7,4.1" data-line="2" data-segment="0">2 3 6</span>
<span class="f4" data-bbox="279.6,410.8,5.0,5.6" data-line="3" data-segment="0">3</span>
<span class="f9" data-bbox="240.7,417.5,61.6,5.6" data-line="4" data-segment="0">= log :</span>
<span class="f11" data-bbox="272.0,420.4,24.7,3.6" data-line="5" data-segment="0">1 1 1</span>
<span class="f4" data-bbox="265.8,426.1,30.8,6.8" data-line="6" data-segment="0">2 2 3 3 6 6</span>
</p>
<p>
<span class="f4" data-bbox="106.9,443.3,412.4,5.6" data-line="0" data-segment="0">Suppose the symbols fall into several groups such that the noise never causes a symbol in one group to</span>
</p>
<p>
<span class="f4" data-bbox="91.9,455.3,427.6,5.6" data-line="0" data-segment="0">be mistaken for a symbol in another group. Let the capacity for the nth group be Cn (in bits per second)</span>
</p>
<p>
<span class="f4" data-bbox="91.9,467.2,427.5,5.6" data-line="0" data-segment="0">when we use only the symbols in this group. Then it is easily shown that, for best use of the entire set, the</span>
</p>
<p>
<span class="f4" data-bbox="91.9,479.2,241.6,5.6" data-line="0" data-segment="0">total probability Pn of all symbols in the nth group should be</span>
</p>
<p>
<span class="f8" data-bbox="313.0,495.4,7.7,4.4" data-line="0" data-segment="0">Cn</span>
<span class="f4" data-bbox="308.3,499.1,5.0,5.6" data-line="1" data-segment="0">2</span>
<span class="f7" data-bbox="281.5,505.9,48.1,5.5" data-line="2" data-segment="0">Pn = :</span>
<span class="f8" data-bbox="317.0,509.8,7.7,4.4" data-line="3" data-segment="0">Cn</span>
<span class="f25" data-bbox="304.2,512.7,13.2,7.2" data-line="4" data-segment="0">∑ 2</span>
</p>
<p>
<span class="f4" data-bbox="91.9,530.3,427.5,5.6" data-line="0" data-segment="0">Within a group the probability is distributed just as it would be if these were the only symbols being used.</span>
</p>
<p>
<span class="f4" data-bbox="91.9,542.3,93.8,5.6" data-line="0" data-segment="0">The channel capacity is</span>
<span class="f8" data-bbox="324.1,550.1,7.7,4.4" data-line="1" data-segment="0">Cn</span>
<span class="f7" data-bbox="275.3,554.3,60.2,5.6" data-line="2" data-segment="0">C = log 2 :</span>
<span class="f20" data-bbox="308.2,556.4,10.3,9.4" data-line="3" data-segment="0">∑</span>
</p>
<p>
<span class="f4" data-bbox="217.8,575.1,175.6,5.6" data-line="0" data-segment="0">17. AN EXAMPLE OF EFFICIENT CODING</span>
</p>
<p>
<span class="f4" data-bbox="91.9,593.7,427.4,5.6" data-line="0" data-segment="0">The following example, although somewhat unrealistic, is a case in which exact matching to a noisy channel</span>
</p>
<p>
<span class="f4" data-bbox="91.9,605.7,426.8,5.6" data-line="0" data-segment="0">is possible. There are two channel symbols, 0 and 1, and the noise affects them in blocks of seven symbols.</span>
</p>
<p>
<span class="f4" data-bbox="91.9,617.7,427.3,5.6" data-line="0" data-segment="0">A block of seven is either transmitted without error, or exactly one symbol of the seven is incorrect. These</span>
</p>
<p>
<span class="f4" data-bbox="91.9,629.6,180.7,5.6" data-line="0" data-segment="0">eight possibilities are equally likely. We have</span>
</p>
<p>
<span class="f19" data-bbox="293.2,641.7,61.8,14.9" data-line="0" data-segment="0">   </span>
<span class="f7" data-bbox="255.8,649.7,94.9,8.5" data-line="1" data-segment="0">C = Max H(y)  Hx(y)</span>
<span class="f19" data-bbox="280.9,658.0,51.0,14.9" data-line="2" data-segment="0">   </span>
<span class="f6" data-bbox="276.1,662.2,50.4,4.1" data-line="3" data-segment="0">1 8 1</span>
<span class="f9" data-bbox="265.0,666.2,55.6,5.6" data-line="4" data-segment="0">= 7 + log</span>
<span class="f6" data-bbox="276.1,669.5,50.4,4.1" data-line="5" data-segment="0">7 8 8</span>
<span class="f6" data-bbox="276.1,678.7,3.7,4.1" data-line="6" data-segment="0">4</span>
<span class="f9" data-bbox="265.0,682.6,67.4,5.6" data-line="7" data-segment="0">= bits/symbol:</span>
<span class="f6" data-bbox="276.1,686.0,3.7,4.1" data-line="8" data-segment="0">7</span>
</p>
<p>
<span class="f4" data-bbox="91.9,702.7,427.6,5.6" data-line="0" data-segment="0">An efﬁcient code, allowing complete correction of errors and transmitting at the rate C, is the following</span>
</p>
<p>
<span class="f4" data-bbox="91.9,714.7,165.7,5.6" data-line="0" data-segment="0">(found by a method due to R. Hamming):</span>
</p>
<p>
<span class="f4" data-bbox="300.6,744.5,10.0,5.6" data-line="0" data-segment="0">27</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                                                                                                     <p>
<span class="f4" data-bbox="106.9,99.9,412.5,5.7" data-line="0" data-segment="0">Let a block of seven symbols be X1;X2;:::;X7. Of these X3, X5, X6 and X7 are message symbols and</span>
<span class="f4" data-bbox="91.9,111.9,355.3,5.6" data-line="1" data-segment="0">chosen arbitrarily by the source. The other three are redundant and calculated as follows:</span>
</p>
<p>
<span class="f7" data-bbox="195.8,131.7,219.7,5.7" data-line="0" data-segment="0">X4 is chosen to make   = X4 + X5 + X6 + X7 even</span>
<span class="f7" data-bbox="195.8,143.7,212.5,5.7" data-line="1" data-segment="0">X2 “ “ “ “   = X2 + X3 + X6 + X7 “</span>
<span class="f7" data-bbox="195.8,155.6,212.5,5.7" data-line="2" data-segment="0">X1 “ “ “ “  = X1 + X3 + X5 + X7 “</span>
</p>
<p>
<span class="f4" data-bbox="91.9,175.4,427.7,5.6" data-line="0" data-segment="0">When a block of seven is received  ;  and  are calculated and if even called zero, if odd called one. The</span>
<span class="f4" data-bbox="91.9,187.4,378.0,5.6" data-line="1" data-segment="0">binary number     then gives the subscript of the Xi that is incorrect (if 0 there was no error).</span>
</p>
<p>
<span class="f4" data-bbox="275.3,211.3,60.7,5.6" data-line="0" data-segment="0">APPENDIX 1</span>
</p>
<p>
<span class="f4" data-bbox="110.2,229.1,390.9,5.6" data-line="0" data-segment="0">THE GROWTH OF THE NUMBER OF BLOCKS OF SYMBOLS WITH A FINITE STATE CONDITION</span>
</p>
<p>
<span class="f4" data-bbox="91.9,247.1,356.6,5.6" data-line="0" data-segment="0">Let Ni (L) be the number of blocks of symbols of length L ending in state i. Then we have</span>
</p>
<p>
<span class="f19" data-bbox="313.9,261.7,39.4,14.9" data-line="0" data-segment="0">   </span>
<span class="f18" data-bbox="339.7,264.4,8.6,4.1" data-line="1" data-segment="0">(s)</span>
<span class="f7" data-bbox="258.0,269.7,81.7,8.5" data-line="2" data-segment="0">N j (L) = Ni L  b</span>
<span class="f20" data-bbox="293.8,271.9,51.1,9.4" data-line="3" data-segment="0">∑ i j</span>
<span class="f8" data-bbox="295.3,279.9,7.0,4.1" data-line="4" data-segment="0">i;s</span>
</p>
<p>
<span class="f6" data-bbox="123.7,296.5,50.2,4.1" data-line="0" data-segment="0">1 2 m</span>
<span class="f4" data-bbox="91.9,300.1,427.4,5.6" data-line="1" data-segment="0">where b ;b ;:::;b are the length of the symbols which may be chosen in state i and lead to state j. These</span>
<span class="f8" data-bbox="123.7,302.8,50.1,4.2" data-line="2" data-segment="0">i j i j i j</span>
<span class="f4" data-bbox="91.9,312.1,308.6,8.5" data-line="3" data-segment="0">are linear difference equations and the behavior as L ! ∞ must be of the type</span>
</p>
<p>
<span class="f8" data-bbox="322.6,328.1,4.1,4.1" data-line="0" data-segment="0">L</span>
<span class="f7" data-bbox="281.3,332.2,48.7,5.5" data-line="1" data-segment="0">N j = A jW :</span>
</p>
<p>
<span class="f4" data-bbox="91.9,352.5,152.7,5.6" data-line="0" data-segment="0">Substituting in the difference equation</span>
</p>
<p>
<span class="f21" data-bbox="340.8,368.5,6.8,3.3" data-line="0" data-segment="0">(s)</span>
<span class="f8" data-bbox="281.5,372.5,63.5,5.7" data-line="1" data-segment="0">L L bi j</span>
<span class="f7" data-bbox="262.6,377.3,63.5,5.5" data-line="2" data-segment="0">A jW = AiW</span>
<span class="f20" data-bbox="298.2,379.4,10.3,9.4" data-line="3" data-segment="0">∑</span>
<span class="f8" data-bbox="299.9,387.4,7.0,4.1" data-line="4" data-segment="0">i;s</span>
</p>
<p>
<span class="f4" data-bbox="91.9,406.6,8.4,5.6" data-line="0" data-segment="0">or</span>
</p>
<p>
<span class="f21" data-bbox="331.9,423.5,6.8,3.3" data-line="0" data-segment="0">(s)</span>
<span class="f14" data-bbox="322.4,427.6,9.5,4.1" data-line="1" data-segment="0"> b</span>
<span class="f17" data-bbox="331.9,430.0,4.2,3.3" data-line="2" data-segment="0">i j</span>
<span class="f7" data-bbox="271.4,432.4,49.9,5.5" data-line="3" data-segment="0">A j = AiW</span>
<span class="f20" data-bbox="293.5,434.5,10.3,9.4" data-line="4" data-segment="0">∑</span>
<span class="f8" data-bbox="295.1,442.5,7.0,4.1" data-line="5" data-segment="0">i;s</span>
<span class="f19" data-bbox="261.2,451.3,70.4,14.9" data-line="6" data-segment="0">   </span>
<span class="f21" data-bbox="297.0,453.4,6.8,3.3" data-line="7" data-segment="0">(s)</span>
<span class="f14" data-bbox="287.5,457.5,9.5,4.1" data-line="8" data-segment="0"> b</span>
<span class="f17" data-bbox="297.0,459.9,4.2,3.3" data-line="9" data-segment="0">i j</span>
<span class="f7" data-bbox="278.0,462.3,82.2,8.5" data-line="10" data-segment="0">W    A = 0:</span>
<span class="f20" data-bbox="251.0,463.7,88.8,10.1" data-line="11" data-segment="0">∑ ∑ i j i</span>
<span class="f8" data-bbox="255.1,471.9,18.6,4.6" data-line="12" data-segment="0">i s</span>
</p>
<p>
<span class="f4" data-bbox="91.9,489.5,152.9,5.6" data-line="0" data-segment="0">For this to be possible the determinant</span>
</p>
<p>
<span class="f19" data-bbox="305.4,502.9,65.2,14.9" data-line="0" data-segment="0">   </span>
<span class="f21" data-bbox="338.5,505.5,6.8,3.3" data-line="1" data-segment="0">(s)</span>
<span class="f19" data-bbox="305.4,508.9,65.2,14.9" data-line="2" data-segment="0">   b  </span>
<span class="f17" data-bbox="338.5,512.0,4.2,3.3" data-line="3" data-segment="0">i j</span>
<span class="f7" data-bbox="240.7,514.4,129.8,15.4" data-line="4" data-segment="0">D(W ) = jai j j =   W   i j  </span>
<span class="f20" data-bbox="308.6,516.5,10.3,9.4" data-line="5" data-segment="0">∑</span>
<span class="f8" data-bbox="312.4,524.0,2.9,4.1" data-line="6" data-segment="0">s</span>
</p>
<p>
<span class="f4" data-bbox="91.9,541.1,341.5,5.6" data-line="0" data-segment="0">must vanish and this determines W , which is, of course, the largest real root of D = 0.</span>
<span class="f4" data-bbox="106.9,553.1,125.0,5.6" data-line="1" data-segment="0">The quantity C is then given by</span>
</p>
<p>
<span class="f8" data-bbox="324.5,569.3,4.1,4.1" data-line="0" data-segment="0">L</span>
<span class="f4" data-bbox="283.4,572.9,39.9,7.2" data-line="1" data-segment="0">log ∑ A jW</span>
<span class="f7" data-bbox="245.4,579.9,118.7,5.6" data-line="2" data-segment="0">C = Lim = logW</span>
<span class="f8" data-bbox="264.5,586.3,44.6,5.9" data-line="3" data-segment="0">L!∞ L</span>
</p>
<p>
<span class="f4" data-bbox="91.9,603.3,427.3,5.6" data-line="0" data-segment="0">and we also note that the same growth properties result if we require that all blocks start in the same (arbi-</span>
<span class="f4" data-bbox="91.9,615.3,81.1,5.6" data-line="1" data-segment="0">trarily chosen) state.</span>
</p>
<p>
<span class="f4" data-bbox="275.3,639.2,60.7,5.6" data-line="0" data-segment="0">APPENDIX 2</span>
</p>
<p>
<span class="f4" data-bbox="236.8,657.2,137.5,8.5" data-line="0" data-segment="0">DERIVATION OF H =  ∑ pi log pi</span>
<span class="f19" data-bbox="115.8,668.6,57.2,14.9" data-line="1" data-segment="0">   </span>
<span class="f4" data-bbox="123.0,672.9,42.9,5.6" data-line="2" data-segment="0">1 1 1</span>
<span class="f8" data-bbox="429.6,676.0,5.3,4.1" data-line="3" data-segment="0">m</span>
<span class="f4" data-bbox="91.9,679.7,427.4,5.6" data-line="4" data-segment="0">Let H ; ;:::; = A(n). From condition (3) we can decompose a choice from s equally likely possi-</span>
<span class="f7" data-bbox="123.0,686.6,42.9,5.5" data-line="5" data-segment="0">n n n</span>
<span class="f4" data-bbox="91.9,694.4,309.6,5.6" data-line="6" data-segment="0">bilities into a series of m choices from s equally likely possibilities and obtain</span>
</p>
<p>
<span class="f8" data-bbox="287.8,710.6,5.3,4.1" data-line="0" data-segment="0">m</span>
<span class="f7" data-bbox="274.0,714.7,63.2,5.5" data-line="1" data-segment="0">A(s ) = mA(s):</span>
</p>
<p>
<span class="f4" data-bbox="300.6,744.5,10.0,5.6" data-line="0" data-segment="0">28</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                                                                                                                          <p>
<span class="f4" data-bbox="91.9,99.9,37.1,5.6" data-line="0" data-segment="0">Similarly</span>
<span class="f8" data-bbox="289.8,107.8,3.7,4.1" data-line="1" data-segment="0">n</span>
<span class="f7" data-bbox="276.6,111.9,58.2,5.5" data-line="2" data-segment="0">A(t ) = nA(t):</span>
</p>
<p>
<span class="f4" data-bbox="91.9,129.4,226.8,5.6" data-line="0" data-segment="0">We can choose n arbitrarily large and ﬁnd an m to satisfy</span>
</p>
<p>
<span class="f8" data-bbox="274.9,146.5,61.9,4.1" data-line="0" data-segment="0">m n (m+1)</span>
<span class="f7" data-bbox="271.1,150.5,69.0,8.5" data-line="1" data-segment="0">s  t &lt; s :</span>
</p>
<p>
<span class="f4" data-bbox="91.9,171.7,188.4,5.6" data-line="0" data-segment="0">Thus, taking logarithms and dividing by n log s,</span>
</p>
<p>
<span class="f19" data-bbox="328.9,186.1,48.0,14.9" data-line="0" data-segment="0">   </span>
<span class="f7" data-bbox="219.4,190.9,157.6,16.1" data-line="1" data-segment="0">m log t m 1  m log t  </span>
<span class="f16" data-bbox="229.9,197.6,163.2,15.4" data-line="2" data-segment="0">    + or       &lt;  </span>
<span class="f7" data-bbox="220.4,204.4,152.0,5.6" data-line="3" data-segment="0">n log s n n n log s</span>
</p>
<p>
<span class="f4" data-bbox="91.9,224.2,283.6,5.6" data-line="0" data-segment="0">where   is arbitrarily small. Now from the monotonic property of A(n),</span>
</p>
<p>
<span class="f8" data-bbox="268.4,241.3,83.7,4.1" data-line="0" data-segment="0">m n m+1</span>
<span class="f7" data-bbox="254.6,245.3,102.0,8.5" data-line="1" data-segment="0">A(s )  A(t )  A(s )</span>
</p>
<p>
<span class="f7" data-bbox="244.6,260.3,122.3,8.5" data-line="0" data-segment="0">mA(s)  nA(t)  (m + 1)A(s):</span>
</p>
<p>
<span class="f4" data-bbox="91.9,281.5,103.5,5.6" data-line="0" data-segment="0">Hence, dividing by nA(s),</span>
<span class="f19" data-bbox="328.9,287.7,46.9,14.9" data-line="1" data-segment="0">   </span>
<span class="f7" data-bbox="220.4,292.4,155.4,16.3" data-line="2" data-segment="0">m A(t) m 1  m A(t)  </span>
<span class="f16" data-bbox="231.1,299.1,160.9,15.5" data-line="3" data-segment="0">    + or       &lt;  </span>
<span class="f7" data-bbox="221.5,305.9,149.6,5.5" data-line="4" data-segment="0">n A(s) n n n A(s)</span>
<span class="f19" data-bbox="229.1,317.7,57.5,14.9" data-line="5" data-segment="0">   </span>
<span class="f19" data-bbox="229.1,322.5,57.5,16.1" data-line="6" data-segment="0"> A(t) log t  </span>
<span class="f19" data-bbox="229.1,329.2,152.4,15.4" data-line="7" data-segment="0">      &lt; 2  A(t) = K log t</span>
<span class="f7" data-bbox="233.6,336.1,48.3,5.6" data-line="8" data-segment="0">A(s) log s</span>
</p>
<p>
<span class="f4" data-bbox="91.9,351.8,156.9,5.6" data-line="0" data-segment="0">where K must be positive to satisfy (2).</span>
<span class="f7" data-bbox="479.4,359.3,7.1,5.5" data-line="1" data-segment="0">ni</span>
<span class="f4" data-bbox="106.9,366.1,412.5,5.6" data-line="2" data-segment="0">Now suppose we have a choice from n possibilities with commeasurable probabilities pi = where</span>
<span class="f25" data-bbox="475.3,372.9,15.3,7.2" data-line="3" data-segment="0">∑ ni</span>
<span class="f4" data-bbox="91.9,382.5,427.5,7.2" data-line="4" data-segment="0">the ni are integers. We can break down a choice from ∑ ni possibilities into a choice from n possibilities</span>
<span class="f4" data-bbox="91.9,394.4,427.4,5.6" data-line="5" data-segment="0">with probabilities p1;:::; pn and then, if the ith was chosen, a choice from ni with equal probabilities. Using</span>
<span class="f4" data-bbox="91.9,406.4,343.1,7.2" data-line="6" data-segment="0">condition (3) again, we equate the total choice from ∑ ni as computed by two methods</span>
</p>
<p>
<span class="f7" data-bbox="219.8,427.5,171.6,5.6" data-line="0" data-segment="0">K log ni = H(p1;:::; pn) + K pi log ni:</span>
<span class="f20" data-bbox="241.9,429.7,114.9,9.4" data-line="1" data-segment="0">∑ ∑</span>
</p>
<p>
<span class="f4" data-bbox="91.9,448.6,25.6,5.6" data-line="0" data-segment="0">Hence</span>
<span class="f19" data-bbox="251.6,458.7,115.3,14.9" data-line="1" data-segment="0">h i</span>
<span class="f7" data-bbox="224.3,469.9,137.5,8.5" data-line="2" data-segment="0">H = K pi log ni   pi log ni</span>
<span class="f20" data-bbox="256.3,471.9,74.2,9.4" data-line="3" data-segment="0">∑ ∑ ∑</span>
<span class="f7" data-bbox="300.2,484.6,7.1,5.5" data-line="4" data-segment="0">ni</span>
<span class="f9" data-bbox="234.5,491.3,152.4,8.5" data-line="5" data-segment="0">=  K pi log =  K pi log pi:</span>
<span class="f20" data-bbox="260.4,493.5,91.1,9.4" data-line="6" data-segment="0">∑ ∑</span>
<span class="f25" data-bbox="296.2,498.2,15.4,7.2" data-line="7" data-segment="0">∑ ni</span>
</p>
<p>
<span class="f4" data-bbox="91.9,516.9,427.7,5.6" data-line="0" data-segment="0">If the pi are incommeasurable, they may be approximated by rationals and the same expression must hold</span>
<span class="f4" data-bbox="91.9,528.9,427.5,5.6" data-line="1" data-segment="0">by our continuity assumption. Thus the expression holds in general. The choice of coefﬁcient K is a matter</span>
<span class="f4" data-bbox="91.9,540.9,251.5,5.6" data-line="2" data-segment="0">of convenience and amounts to the choice of a unit of measure.</span>
</p>
<p>
<span class="f4" data-bbox="275.3,564.8,60.7,5.6" data-line="0" data-segment="0">APPENDIX 3</span>
</p>
<p>
<span class="f4" data-bbox="231.6,582.7,148.0,5.6" data-line="0" data-segment="0">THEOREMS ON ERGODIC SOURCES</span>
</p>
<p>
<span class="f4" data-bbox="91.9,600.7,427.3,5.6" data-line="0" data-segment="0">If it is possible to go from any state with P &gt; 0 to any other along a path of probability p &gt; 0, the system is</span>
<span class="f4" data-bbox="91.9,612.5,427.4,5.6" data-line="1" data-segment="0">ergodic and the strong law of large numbers can be applied. Thus the number of times a given path pi j in</span>
<span class="f4" data-bbox="91.9,624.5,427.4,5.6" data-line="2" data-segment="0">the network is traversed in a long sequence of length N is about proportional to the probability of being at</span>
<span class="f7" data-bbox="91.9,636.5,427.4,8.5" data-line="3" data-segment="0">i, say Pi, and then choosing this path, Pi pi j N. If N is large enough the probability of percentage error    in</span>
<span class="f4" data-bbox="91.9,648.4,397.2,5.6" data-line="4" data-segment="0">this is less than   so that for all but a set of small probability the actual numbers lie within the limits</span>
</p>
<p>
<span class="f9" data-bbox="279.5,669.5,52.3,8.5" data-line="0" data-segment="0">(Pi pi j   )N:</span>
</p>
<p>
<span class="f4" data-bbox="91.9,690.8,227.2,5.6" data-line="0" data-segment="0">Hence nearly all sequences have a probability p given by</span>
</p>
<p>
<span class="f18" data-bbox="305.8,708.7,35.3,4.6" data-line="0" data-segment="0">(Pi pi j   )N</span>
<span class="f7" data-bbox="270.0,714.7,35.7,5.5" data-line="1" data-segment="0">p = p</span>
<span class="f20" data-bbox="287.2,716.8,23.8,9.4" data-line="2" data-segment="0">∏ i j</span>
</p>
<p>
<span class="f4" data-bbox="300.6,744.5,10.0,5.6" data-line="0" data-segment="0">29</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                                                                                                                                                                             <p>
<span class="f4" data-bbox="110.0,96.8,19.5,5.6" data-line="0" data-segment="0">log p</span>
<span class="f4" data-bbox="91.9,103.5,91.3,5.6" data-line="1" data-segment="0">and is limited by</span>
<span class="f7" data-bbox="116.2,110.3,6.7,5.5" data-line="2" data-segment="0">N</span>
<span class="f4" data-bbox="250.2,118.4,19.5,5.6" data-line="3" data-segment="0">log p</span>
<span class="f9" data-bbox="273.2,125.1,88.3,8.5" data-line="4" data-segment="0">= (Pi pi j   )log pi j</span>
<span class="f20" data-bbox="283.2,127.3,10.3,9.4" data-line="5" data-segment="0">∑</span>
<span class="f7" data-bbox="256.3,131.9,6.7,5.5" data-line="6" data-segment="0">N</span>
</p>
<p>
<span class="f4" data-bbox="91.9,143.0,8.4,5.6" data-line="0" data-segment="0">or</span>
<span class="f19" data-bbox="248.5,146.2,96.2,14.9" data-line="1" data-segment="0">   </span>
<span class="f19" data-bbox="248.5,150.9,96.2,16.3" data-line="2" data-segment="0"> log p  </span>
<span class="f19" data-bbox="248.5,157.7,114.2,15.4" data-line="3" data-segment="0">   ∑Pi pi j log pi j  &lt;  :</span>
<span class="f7" data-bbox="259.2,164.6,6.7,5.5" data-line="4" data-segment="0">N</span>
</p>
<p>
<span class="f4" data-bbox="91.9,178.5,94.9,5.6" data-line="0" data-segment="0">This proves Theorem 3.</span>
</p>
<p>
<span class="f4" data-bbox="106.9,190.4,412.4,5.6" data-line="0" data-segment="0">Theorem 4 follows immediately from this on calculating upper and lower bounds for n(q) based on the</span>
</p>
<p>
<span class="f4" data-bbox="91.9,202.4,173.5,5.6" data-line="0" data-segment="0">possible range of values of p in Theorem 3.</span>
</p>
<p>
<span class="f4" data-bbox="106.9,214.4,132.4,5.6" data-line="0" data-segment="0">In the mixed (not ergodic) case if</span>
</p>
<p>
<span class="f7" data-bbox="283.0,226.3,44.9,5.5" data-line="0" data-segment="0">L = piLi</span>
<span class="f20" data-bbox="300.6,228.4,10.3,9.4" data-line="1" data-segment="0">∑</span>
</p>
<p>
<span class="f4" data-bbox="91.9,244.3,291.5,8.5" data-line="0" data-segment="0">and the entropies of the components are H1  H2      Hn we have the</span>
</p>
<p>
<span class="f6" data-bbox="170.5,255.9,23.4,4.1" data-line="0" data-segment="0">log n(q)</span>
<span class="f7" data-bbox="106.9,260.8,237.7,5.7" data-line="1" data-segment="0">Theorem: Lim = '(q) is a decreasing step function,</span>
<span class="f8" data-bbox="179.5,264.3,4.9,4.1" data-line="2" data-segment="0">N</span>
<span class="f8" data-bbox="150.2,267.3,17.9,4.8" data-line="3" data-segment="0">N!∞</span>
</p>
<p>
<span class="f8" data-bbox="327.2,285.3,58.2,4.1" data-line="0" data-segment="0">s 1 s</span>
<span class="f10" data-bbox="209.4,295.9,192.5,5.7" data-line="1" data-segment="0">'(q) = Hs in the interval  i &lt; q &lt;  i :</span>
<span class="f20" data-bbox="328.2,298.0,60.9,9.4" data-line="2" data-segment="0">∑ ∑</span>
<span class="f6" data-bbox="331.6,306.2,54.2,4.1" data-line="3" data-segment="0">1 1</span>
</p>
<p>
<span class="f4" data-bbox="106.9,324.9,412.4,5.6" data-line="0" data-segment="0">To prove Theorems 5 and 6 ﬁrst note that FN is monotonic decreasing because increasing N adds a</span>
</p>
<p>
<span class="f4" data-bbox="91.9,336.9,403.9,5.6" data-line="0" data-segment="0">subscript to a conditional entropy. A simple substitution for pB (S j ) in the deﬁnition of FN shows that</span>
<span class="f17" data-bbox="342.4,339.7,1.7,3.3" data-line="1" data-segment="0">i</span>
</p>
<p>
<span class="f7" data-bbox="252.2,358.9,106.4,8.5" data-line="0" data-segment="0">FN = NGN  (N  1)GN 1</span>
</p>
<p>
<span class="f4" data-bbox="253.0,378.9,5.0,5.6" data-line="0" data-segment="0">1</span>
<span class="f4" data-bbox="91.9,385.6,427.6,8.5" data-line="1" data-segment="0">and summing this for all N gives GN = Fn. Hence GN  FN and GN monotonic decreasing. Also they</span>
<span class="f20" data-bbox="261.4,387.8,10.3,9.4" data-line="2" data-segment="0">∑</span>
<span class="f7" data-bbox="251.8,392.5,6.7,5.5" data-line="3" data-segment="0">N</span>
<span class="f4" data-bbox="91.9,400.5,308.5,5.6" data-line="4" data-segment="0">must approach the same limit. By using Theorem 3 we see that Lim GN = H.</span>
<span class="f8" data-bbox="345.7,406.9,17.9,4.8" data-line="5" data-segment="0">N!∞</span>
</p>
<p>
<span class="f4" data-bbox="275.3,427.7,60.7,5.6" data-line="0" data-segment="0">APPENDIX 4</span>
</p>
<p>
<span class="f4" data-bbox="182.9,445.7,245.4,5.6" data-line="0" data-segment="0">MAXIMIZING THE RATE FOR A SYSTEM OF CONSTRAINTS</span>
</p>
<p>
<span class="f4" data-bbox="91.9,463.6,427.5,5.6" data-line="0" data-segment="0">Suppose we have a set of constraints on sequences of symbols that is of the ﬁnite state type and can be</span>
<span class="f18" data-bbox="280.3,472.3,8.6,4.1" data-line="1" data-segment="0">(s)</span>
<span class="f4" data-bbox="91.9,477.7,427.5,5.6" data-line="2" data-segment="0">represented therefore by a linear graph. Let ` be the lengths of the various symbols that can occur in</span>
<span class="f8" data-bbox="280.3,480.5,5.2,4.1" data-line="3" data-segment="0">i j</span>
<span class="f18" data-bbox="494.9,488.6,8.6,4.1" data-line="4" data-segment="0">(s)</span>
<span class="f4" data-bbox="91.9,494.0,427.5,5.6" data-line="5" data-segment="0">passing from state i to state j. What distribution of probabilities Pi for the different states and p for</span>
<span class="f8" data-bbox="494.9,496.9,5.2,4.1" data-line="6" data-segment="0">i j</span>
<span class="f4" data-bbox="91.9,505.9,427.4,5.6" data-line="7" data-segment="0">choosing symbol s in state i and going to state j maximizes the rate of generating information under these</span>
</p>
<p>
<span class="f4" data-bbox="91.9,517.9,427.2,5.6" data-line="0" data-segment="0">constraints? The constraints deﬁne a discrete channel and the maximum rate must be less than or equal to</span>
</p>
<p>
<span class="f4" data-bbox="91.9,529.7,427.5,5.6" data-line="0" data-segment="0">the capacity C of this channel, since if all blocks of large length were equally likely, this rate would result,</span>
</p>
<p>
<span class="f4" data-bbox="91.9,541.7,427.5,5.6" data-line="0" data-segment="0">and if possible this would be best. We will show that this rate can be achieved by proper choice of the Pi and</span>
<span class="f18" data-bbox="97.7,550.4,8.6,4.1" data-line="1" data-segment="0">(s)</span>
<span class="f7" data-bbox="92.6,555.8,16.7,5.6" data-line="2" data-segment="0">p .</span>
<span class="f8" data-bbox="97.7,558.7,5.2,4.1" data-line="3" data-segment="0">i j</span>
<span class="f4" data-bbox="106.9,567.7,88.5,5.6" data-line="4" data-segment="0">The rate in question is</span>
<span class="f18" data-bbox="288.5,576.4,38.4,4.1" data-line="5" data-segment="0">(s) (s)</span>
<span class="f16" data-bbox="258.0,581.7,91.1,8.5" data-line="6" data-segment="0"> ∑ Pi p log p N</span>
<span class="f8" data-bbox="288.5,584.6,34.9,4.1" data-line="7" data-segment="0">i j i j</span>
<span class="f9" data-bbox="330.7,590.1,23.8,5.0" data-line="8" data-segment="0">= :</span>
<span class="f18" data-bbox="292.3,594.5,21.8,4.1" data-line="9" data-segment="0">(s) (s)</span>
<span class="f7" data-bbox="341.8,596.9,8.3,5.5" data-line="10" data-segment="0">M</span>
<span class="f25" data-bbox="270.7,599.8,34.8,7.2" data-line="11" data-segment="0">∑ Pi p `</span>
<span class="f8" data-bbox="292.3,602.7,18.4,4.1" data-line="12" data-segment="0">i j i j</span>
</p>
<p>
<span class="f18" data-bbox="160.3,616.7,179.9,4.1" data-line="0" data-segment="0">(s) (s) (s)</span>
<span class="f4" data-bbox="106.9,622.0,412.4,7.2" data-line="1" data-segment="0">Let `i j = ∑ ` . Evidently for a maximum p = k exp ` . The constraints on maximization are ∑ Pi =</span>
<span class="f8" data-bbox="151.7,624.2,185.1,4.9" data-line="2" data-segment="0">s i j i j i j</span>
<span class="f4" data-bbox="91.9,634.0,216.0,8.5" data-line="3" data-segment="0">1, ∑ pi j = 1, ∑ Pi(pi j   i j ) = 0. Hence we maximize</span>
<span class="f8" data-bbox="110.0,636.2,2.1,4.1" data-line="4" data-segment="0">j</span>
</p>
<p>
<span class="f16" data-bbox="211.3,655.5,62.2,8.5" data-line="0" data-segment="0"> ∑ Pi pi j log pi j</span>
<span class="f7" data-bbox="189.8,662.5,242.4,8.5" data-line="1" data-segment="0">U = +   Pi +  i pi j +  j Pi(pi j   i j )</span>
<span class="f20" data-bbox="292.8,664.5,81.9,9.4" data-line="2" data-segment="0">∑ ∑ ∑</span>
<span class="f25" data-bbox="224.0,669.2,36.9,7.2" data-line="3" data-segment="0">∑ Pi pi j `i j</span>
<span class="f8" data-bbox="296.9,672.5,2.1,4.1" data-line="4" data-segment="0">i</span>
</p>
<p>
<span class="f25" data-bbox="182.3,685.0,136.6,6.8" data-line="0" data-segment="0">∂U MPi(1 + log pi j ) + NPi`i j</span>
<span class="f9" data-bbox="200.2,692.0,201.5,8.5" data-line="1" data-segment="0">=   +  +  i +  iPi = 0:</span>
<span class="f6" data-bbox="271.4,695.9,3.7,4.1" data-line="2" data-segment="0">2</span>
<span class="f25" data-bbox="180.2,698.8,90.9,6.5" data-line="3" data-segment="0">∂pi j M</span>
</p>
<p>
<span class="f4" data-bbox="300.6,744.5,10.0,5.6" data-line="0" data-segment="0">30</span>
</p>
</div>
<div class="page">
                                                                                                                                     <p>
<span class="f4" data-bbox="91.9,99.9,58.6,5.6" data-line="0" data-segment="0">Solving for pi j</span>
<span class="f14" data-bbox="322.0,109.0,13.1,4.6" data-line="1" data-segment="0"> `i j</span>
<span class="f7" data-bbox="273.1,113.2,65.8,5.5" data-line="2" data-segment="0">pi j = AiB j D :</span>
</p>
<p>
<span class="f4" data-bbox="91.9,131.1,22.2,5.6" data-line="0" data-segment="0">Since</span>
</p>
<p>
<span class="f14" data-bbox="303.4,148.9,63.5,4.6" data-line="0" data-segment="0"> 1  `i j</span>
<span class="f7" data-bbox="255.4,153.1,98.4,5.6" data-line="1" data-segment="0">p = 1; A = B D</span>
<span class="f20" data-bbox="243.2,154.5,102.7,10.0" data-line="2" data-segment="0">∑ i j i ∑ j</span>
<span class="f8" data-bbox="247.8,163.1,84.1,4.1" data-line="3" data-segment="0">j j</span>
</p>
<p>
<span class="f14" data-bbox="317.6,174.8,13.1,4.6" data-line="0" data-segment="0"> `i j</span>
<span class="f7" data-bbox="300.5,178.4,17.2,5.5" data-line="1" data-segment="0">B j D</span>
<span class="f7" data-bbox="270.8,185.3,70.2,5.5" data-line="2" data-segment="0">pi j = :</span>
<span class="f14" data-bbox="323.4,189.3,12.9,4.6" data-line="3" data-segment="0"> `is</span>
<span class="f25" data-bbox="295.1,192.2,28.3,7.2" data-line="4" data-segment="0">∑s BsD</span>
</p>
<p>
<span class="f4" data-bbox="91.9,212.1,266.3,5.6" data-line="0" data-segment="0">The correct value of D is the capacity C and the B j are solutions of</span>
</p>
<p>
<span class="f14" data-bbox="322.8,231.2,13.1,4.6" data-line="0" data-segment="0"> `i j</span>
<span class="f7" data-bbox="274.3,235.4,48.3,5.5" data-line="1" data-segment="0">Bi = B jC</span>
<span class="f20" data-bbox="295.1,237.4,10.3,9.4" data-line="2" data-segment="0">∑</span>
</p>
<p>
<span class="f4" data-bbox="91.9,257.2,31.4,5.6" data-line="0" data-segment="0">for then</span>
</p>
<p>
<span class="f7" data-bbox="302.3,274.7,9.3,5.5" data-line="0" data-segment="0">B j</span>
<span class="f14" data-bbox="319.8,277.6,13.1,4.6" data-line="1" data-segment="0"> `i j</span>
<span class="f7" data-bbox="278.0,281.7,41.6,5.5" data-line="2" data-segment="0">pi j = C</span>
<span class="f7" data-bbox="302.9,288.5,8.2,5.5" data-line="3" data-segment="0">Bi</span>
</p>
<p>
<span class="f7" data-bbox="289.3,300.5,9.3,5.5" data-line="0" data-segment="0">B j</span>
<span class="f14" data-bbox="306.8,303.4,13.1,4.6" data-line="1" data-segment="0"> `i j</span>
<span class="f7" data-bbox="280.4,307.5,58.8,5.5" data-line="2" data-segment="0">P C = P</span>
<span class="f20" data-bbox="269.2,308.9,72.3,10.1" data-line="3" data-segment="0">∑ i j</span>
<span class="f7" data-bbox="289.9,314.3,8.2,5.5" data-line="4" data-segment="0">Bi</span>
</p>
<p>
<span class="f4" data-bbox="91.9,331.4,8.4,5.6" data-line="0" data-segment="0">or</span>
<span class="f7" data-bbox="283.6,338.9,52.1,5.8" data-line="1" data-segment="0">Pi Pj</span>
<span class="f14" data-bbox="299.3,341.8,13.1,4.6" data-line="2" data-segment="0"> `i j</span>
<span class="f7" data-bbox="292.4,345.9,48.2,5.5" data-line="3" data-segment="0">C = :</span>
<span class="f20" data-bbox="270.6,348.1,10.3,9.4" data-line="4" data-segment="0">∑</span>
<span class="f7" data-bbox="283.1,352.7,52.9,5.5" data-line="5" data-segment="0">Bi B j</span>
</p>
<p>
<span class="f4" data-bbox="91.9,369.7,76.0,5.6" data-line="0" data-segment="0">So that if  i satisfy</span>
</p>
<p>
<span class="f14" data-bbox="300.6,387.4,13.1,4.6" data-line="0" data-segment="0"> `i j</span>
<span class="f10" data-bbox="286.6,391.6,45.6,5.5" data-line="1" data-segment="0"> C = </span>
<span class="f20" data-bbox="275.3,393.1,60.0,10.0" data-line="2" data-segment="0">∑ i j</span>
</p>
<p>
<span class="f7" data-bbox="286.2,407.6,38.9,5.5" data-line="0" data-segment="0">Pi = Bii:</span>
</p>
<p>
<span class="f4" data-bbox="91.9,429.5,297.8,5.6" data-line="0" data-segment="0">Both the sets of equations for Bi and i can be satisﬁed since C is such that</span>
</p>
<p>
<span class="f14" data-bbox="281.4,447.3,13.1,4.6" data-line="0" data-segment="0"> `i j</span>
<span class="f16" data-bbox="272.3,451.4,66.8,8.5" data-line="1" data-segment="0">jC   i j j = 0:</span>
</p>
<p>
<span class="f4" data-bbox="91.9,473.3,86.0,5.6" data-line="0" data-segment="0">In this case the rate is</span>
<span class="f8" data-bbox="274.6,479.2,111.6,4.6" data-line="1" data-segment="0">B j B j</span>
<span class="f14" data-bbox="289.9,481.3,13.1,4.6" data-line="2" data-segment="0"> `i j</span>
<span class="f25" data-bbox="231.1,484.9,145.8,7.2" data-line="3" data-segment="0">∑ Pi pi j log C ∑ Pi pi j log</span>
<span class="f8" data-bbox="275.0,488.3,110.7,4.6" data-line="4" data-segment="0">Bi Bi</span>
<span class="f16" data-bbox="222.1,493.6,110.9,8.5" data-line="5" data-segment="0">  = C  </span>
<span class="f25" data-bbox="248.8,500.5,131.1,7.2" data-line="6" data-segment="0">∑ Pi pi j `i j ∑ Pi pi j `i j</span>
</p>
<p>
<span class="f4" data-bbox="91.9,517.3,12.6,5.6" data-line="0" data-segment="0">but</span>
</p>
<p>
<span class="f7" data-bbox="205.7,529.1,211.3,8.5" data-line="0" data-segment="0">P p (log B  log B ) = P log B   P log B = 0</span>
<span class="f20" data-bbox="194.4,530.6,204.9,10.1" data-line="1" data-segment="0">∑ i i j j i ∑ j j ∑ i i</span>
<span class="f8" data-bbox="306.1,539.2,2.1,4.1" data-line="2" data-segment="0">j</span>
</p>
<p>
<span class="f4" data-bbox="91.9,555.5,427.6,5.6" data-line="0" data-segment="0">Hence the rate is C and as this could never be exceeded this is the maximum, justifying the assumed solution.</span>
</p>
<p>
<span class="f4" data-bbox="300.6,744.5,10.0,5.6" data-line="0" data-segment="0">31</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               <p>
<span class="f13" data-bbox="186.7,99.9,237.9,6.7" data-line="0" data-segment="0">PART III: MATHEMATICAL PRELIMINARIES</span>
</p>
<p>
<span class="f4" data-bbox="91.9,123.9,427.3,5.6" data-line="0" data-segment="0">In this ﬁnal installment of the paper we consider the case where the signals or the messages or both are</span>
<span class="f4" data-bbox="91.9,135.8,427.3,5.6" data-line="1" data-segment="0">continuously variable, in contrast with the discrete nature assumed heretofore. To a considerable extent the</span>
<span class="f4" data-bbox="91.9,147.8,428.0,5.6" data-line="2" data-segment="0">continuous case can be obtained through a limiting process from the discrete case by dividing the continuum</span>
<span class="f4" data-bbox="91.9,159.8,426.9,5.6" data-line="3" data-segment="0">of messages and signals into a large but ﬁnite number of small regions and calculating the various parameters</span>
<span class="f4" data-bbox="91.9,171.7,427.3,5.6" data-line="4" data-segment="0">involved on a discrete basis. As the size of the regions is decreased these parameters in general approach as</span>
<span class="f4" data-bbox="91.9,183.7,427.3,5.6" data-line="5" data-segment="0">limits the proper values for the continuous case. There are, however, a few new effects that appear and also</span>
<span class="f4" data-bbox="91.9,195.5,411.1,5.6" data-line="6" data-segment="0">a general change of emphasis in the direction of specialization of the general results to particular cases.</span>
<span class="f4" data-bbox="106.9,207.5,412.1,5.6" data-line="7" data-segment="0">We will not attempt, in the continuous case, to obtain our results with the greatest generality, or with</span>
<span class="f4" data-bbox="91.9,219.5,427.4,5.6" data-line="8" data-segment="0">the extreme rigor of pure mathematics, since this would involve a great deal of abstract measure theory</span>
<span class="f4" data-bbox="91.9,231.4,427.5,5.6" data-line="9" data-segment="0">and would obscure the main thread of the analysis. A preliminary study, however, indicates that the theory</span>
<span class="f4" data-bbox="91.9,243.4,427.2,5.6" data-line="10" data-segment="0">can be formulated in a completely axiomatic and rigorous manner which includes both the continuous and</span>
<span class="f4" data-bbox="91.9,255.4,427.4,5.6" data-line="11" data-segment="0">discrete cases and many others. The occasional liberties taken with limiting processes in the present analysis</span>
<span class="f4" data-bbox="91.9,267.3,188.6,5.6" data-line="12" data-segment="0">can be justiﬁed in all cases of practical interest.</span>
</p>
<p>
<span class="f4" data-bbox="214.4,287.9,182.3,5.6" data-line="0" data-segment="0">18. SETS AND ENSEMBLES OF FUNCTIONS</span>
</p>
<p>
<span class="f4" data-bbox="91.9,306.7,427.4,5.6" data-line="0" data-segment="0">We shall have to deal in the continuous case with sets of functions and ensembles of functions. A set of</span>
<span class="f4" data-bbox="91.9,318.7,427.2,5.6" data-line="1" data-segment="0">functions, as the name implies, is merely a class or collection of functions, generally of one variable, time.</span>
<span class="f4" data-bbox="91.9,330.5,427.4,5.6" data-line="2" data-segment="0">It can be speciﬁed by giving an explicit representation of the various functions in the set, or implicitly by</span>
<span class="f4" data-bbox="91.9,342.5,362.8,5.6" data-line="3" data-segment="0">giving a property which functions in the set possess and others do not. Some examples are:</span>
<span class="f4" data-bbox="104.4,359.9,94.8,5.6" data-line="4" data-segment="0">1. The set of functions:</span>
<span class="f7" data-bbox="283.1,371.9,71.5,5.6" data-line="5" data-segment="0">f (t) = sin(t +  ):</span>
<span class="f4" data-bbox="116.9,388.0,272.8,5.6" data-line="6" data-segment="0">Each particular value of   determines a particular function in the set.</span>
</p>
<p>
<span class="f4" data-bbox="104.4,406.7,347.4,5.6" data-line="0" data-segment="0">2. The set of all functions of time containing no frequencies over W cycles per second.</span>
</p>
<p>
<span class="f4" data-bbox="104.4,425.5,279.7,5.6" data-line="0" data-segment="0">3. The set of all functions limited in band to W and in amplitude to A.</span>
</p>
<p>
<span class="f4" data-bbox="104.4,444.2,241.3,5.6" data-line="0" data-segment="0">4. The set of all English speech signals as functions of time.</span>
<span class="f4" data-bbox="106.9,461.6,412.4,5.6" data-line="1" data-segment="0">An ensemble of functions is a set of functions together with a probability measure whereby we may</span>
<span class="f6" data-bbox="388.6,470.0,3.7,4.1" data-line="2" data-segment="0">1</span>
<span class="f4" data-bbox="91.9,473.6,404.4,5.6" data-line="3" data-segment="0">determine the probability of a function in the set having certain properties. For example with the set,</span>
</p>
<p>
<span class="f7" data-bbox="270.6,492.4,71.5,5.6" data-line="0" data-segment="0">f (t) = sin(t +  );</span>
</p>
<p>
<span class="f4" data-bbox="91.9,511.3,343.5,5.6" data-line="0" data-segment="0">we may give a probability distribution for  , P( ). The set then becomes an ensemble.</span>
<span class="f4" data-bbox="106.9,523.3,214.5,5.6" data-line="1" data-segment="0">Some further examples of ensembles of functions are:</span>
<span class="f4" data-bbox="104.4,540.7,334.4,5.6" data-line="2" data-segment="0">1. A ﬁnite set of functions fk (t) (k = 1;2;:::;n) with the probability of fk being pk .</span>
</p>
<p>
<span class="f4" data-bbox="104.4,559.4,172.9,5.6" data-line="0" data-segment="0">2. A ﬁnite dimensional family of functions</span>
</p>
<p>
<span class="f7" data-bbox="281.8,578.2,74.0,5.6" data-line="0" data-segment="0">f ( 1 ; 2 ;:::; n ; t)</span>
</p>
<p>
<span class="f4" data-bbox="116.9,597.1,206.5,5.6" data-line="0" data-segment="0">with a probability distribution on the parameters  i:</span>
</p>
<p>
<span class="f7" data-bbox="290.5,615.9,55.9,5.6" data-line="0" data-segment="0">p( 1;:::; n ):</span>
</p>
<p>
<span class="f4" data-bbox="116.9,634.7,223.8,5.6" data-line="0" data-segment="0">For example we could consider the ensemble deﬁned by</span>
<span class="f8" data-bbox="341.6,649.0,3.7,4.1" data-line="1" data-segment="0">n</span>
<span class="f7" data-bbox="227.9,659.7,181.8,5.6" data-line="2" data-segment="0">f (a1 ;:::;an; 1;:::; n; t) = ai sin i(!t +  i)</span>
<span class="f20" data-bbox="338.4,661.9,10.3,9.4" data-line="3" data-segment="0">∑</span>
<span class="f8" data-bbox="337.8,669.9,11.5,4.1" data-line="4" data-segment="0">i=1</span>
<span class="f4" data-bbox="116.9,685.9,402.8,5.6" data-line="5" data-segment="0">with the amplitudes ai distributed normally and independently, and the phases  i distributed uniformly</span>
<span class="f4" data-bbox="116.9,697.7,135.3,5.6" data-line="6" data-segment="0">(from 0 to 2 ) and independently.</span>
<span class="f11" data-bbox="102.8,711.8,3.0,3.3" data-line="7" data-segment="0">1</span>
<span class="f1" data-bbox="106.3,714.7,315.3,4.5" data-line="8" data-segment="0">In mathematical terminology the functions belong to a measure space whose total measure is unity.</span>
</p>
<p>
<span class="f4" data-bbox="300.6,744.5,10.0,5.6" data-line="0" data-segment="0">32</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     <p>
<span class="f4" data-bbox="104.4,99.9,68.2,5.6" data-line="0" data-segment="0">3. The ensemble</span>
<span class="f18" data-bbox="296.8,106.4,11.0,4.8" data-line="1" data-segment="0">+∞</span>
<span class="f4" data-bbox="323.9,110.2,59.4,8.5" data-line="2" data-segment="0">sin  (2W t  n)</span>
<span class="f7" data-bbox="253.3,116.9,69.0,5.5" data-line="3" data-segment="0">f (ai;t) = an</span>
<span class="f20" data-bbox="297.1,119.1,10.3,9.4" data-line="4" data-segment="0">∑</span>
<span class="f10" data-bbox="330.2,123.8,46.7,8.5" data-line="5" data-segment="0"> (2W t  n)</span>
<span class="f8" data-bbox="292.1,126.7,20.5,4.8" data-line="6" data-segment="0">n= ∞</span>
<span class="f16" data-bbox="405.7,135.5,8.3,8.5" data-line="7" data-segment="0">p</span>
<span class="f4" data-bbox="116.9,143.7,402.7,5.6" data-line="8" data-segment="0">with the ai normal and independent all with the same standard deviation N. This is a representation</span>
<span class="f6" data-bbox="515.2,152.1,3.7,4.1" data-line="9" data-segment="0">2</span>
<span class="f4" data-bbox="116.9,155.7,398.2,5.6" data-line="10" data-segment="0">of “white” noise, band limited to the band from 0 to W cycles per second and with average power N.</span>
</p>
<p>
<span class="f4" data-bbox="104.4,175.3,414.8,5.6" data-line="0" data-segment="0">4. Let points be distributed on the t axis according to a Poisson distribution. At each selected point the</span>
<span class="f4" data-bbox="116.9,187.3,309.2,5.6" data-line="1" data-segment="0">function f (t) is placed and the different functions added, giving the ensemble</span>
</p>
<p>
<span class="f15" data-bbox="298.1,203.8,5.3,4.8" data-line="0" data-segment="0">∞</span>
<span class="f7" data-bbox="313.3,214.5,32.2,5.5" data-line="1" data-segment="0">f (t +t )</span>
<span class="f20" data-bbox="295.7,216.1,45.4,10.0" data-line="2" data-segment="0">∑ k</span>
<span class="f8" data-bbox="290.6,224.9,20.2,4.8" data-line="3" data-segment="0">k= ∞</span>
</p>
<p>
<span class="f4" data-bbox="116.9,243.5,402.6,5.6" data-line="0" data-segment="0">where the tk are the points of the Poisson distribution. This ensemble can be considered as a type of</span>
<span class="f4" data-bbox="116.9,255.4,230.9,5.6" data-line="1" data-segment="0">impulse or shot noise where all the impulses are identical.</span>
</p>
<p>
<span class="f4" data-bbox="104.4,275.1,415.1,5.6" data-line="0" data-segment="0">5. The set of English speech functions with the probability measure given by the frequency of occurrence</span>
<span class="f4" data-bbox="116.9,287.0,62.3,5.6" data-line="1" data-segment="0">in ordinary use.</span>
</p>
<p>
<span class="f4" data-bbox="106.9,306.3,412.3,5.6" data-line="0" data-segment="0">An ensemble of functions f (t) is stationary if the same ensemble results when all functions are shifted</span>
<span class="f4" data-bbox="91.9,318.3,160.6,5.6" data-line="1" data-segment="0">any ﬁxed amount in time. The ensemble</span>
</p>
<p>
<span class="f7" data-bbox="272.0,339.4,68.8,5.6" data-line="0" data-segment="0">f (t) = sin(t +  )</span>
</p>
<p>
<span class="f4" data-bbox="91.9,360.5,380.1,5.6" data-line="0" data-segment="0">is stationary if   is distributed uniformly from 0 to 2 . If we shift each function by t1 we obtain</span>
</p>
<p>
<span class="f7" data-bbox="254.8,381.7,103.2,5.6" data-line="0" data-segment="0">f (t +t1) = sin(t +t1 +  )</span>
</p>
<p>
<span class="f9" data-bbox="292.7,396.5,49.7,5.6" data-line="0" data-segment="0">= sin(t + ')</span>
</p>
<p>
<span class="f4" data-bbox="91.9,417.7,427.3,5.6" data-line="0" data-segment="0">with ' distributed uniformly from 0 to 2 . Each function has changed but the ensemble as a whole is</span>
<span class="f4" data-bbox="91.9,429.7,329.8,5.6" data-line="1" data-segment="0">invariant under the translation. The other examples given above are also stationary.</span>
<span class="f4" data-bbox="106.9,441.5,412.5,5.6" data-line="2" data-segment="0">An ensemble is ergodic if it is stationary, and there is no subset of the functions in the set with a</span>
<span class="f4" data-bbox="91.9,453.5,272.5,5.6" data-line="3" data-segment="0">probability different from 0 and 1 which is stationary. The ensemble</span>
</p>
<p>
<span class="f4" data-bbox="286.6,474.7,38.2,5.6" data-line="0" data-segment="0">sin(t +  )</span>
</p>
<p>
<span class="f4" data-bbox="91.9,495.8,427.4,8.5" data-line="0" data-segment="0">is ergodic. No subset of these functions of probability 6= 0;1 is transformed into itself under all time trans-</span>
<span class="f4" data-bbox="91.9,507.8,158.4,5.6" data-line="1" data-segment="0">lations. On the other hand the ensemble</span>
<span class="f7" data-bbox="283.6,519.7,44.2,5.6" data-line="2" data-segment="0">a sin(t +  )</span>
</p>
<p>
<span class="f4" data-bbox="91.9,537.2,427.5,5.6" data-line="0" data-segment="0">with a distributed normally and   uniform is stationary but not ergodic. The subset of these functions with</span>
<span class="f7" data-bbox="91.9,549.1,175.7,5.6" data-line="1" data-segment="0">a between 0 and 1 for example is stationary.</span>
<span class="f4" data-bbox="106.9,561.1,413.0,5.6" data-line="2" data-segment="0">Of the examples given, 3 and 4 are ergodic, and 5 may perhaps be considered so. If an ensemble is</span>
<span class="f4" data-bbox="91.9,572.9,427.1,5.6" data-line="3" data-segment="0">ergodic we may say roughly that each function in the set is typical of the ensemble. More precisely it is</span>
<span class="f4" data-bbox="91.9,584.9,427.7,5.6" data-line="4" data-segment="0">known that with an ergodic ensemble an average of any statistic over the ensemble is equal (with probability</span>
<span class="f6" data-bbox="410.5,593.3,3.7,4.1" data-line="5" data-segment="0">3</span>
<span class="f4" data-bbox="91.9,596.9,427.5,5.6" data-line="6" data-segment="0">1) to an average over the time translations of a particular function of the set. Roughly speaking, each</span>
<span class="f4" data-bbox="91.9,608.8,428.0,5.6" data-line="7" data-segment="0">function can be expected, as time progresses, to go through, with the proper frequency, all the convolutions</span>
<span class="f4" data-bbox="91.9,620.8,130.8,5.6" data-line="8" data-segment="0">of any of the functions in the set.</span>
</p>
<p>
<span class="f11" data-bbox="102.8,635.7,3.0,3.3" data-line="0" data-segment="0">2</span>
<span class="f1" data-bbox="106.3,638.6,412.9,4.5" data-line="1" data-segment="0">This representation can be used as a deﬁnition of band limited white noise. It has certain advantages in that it involves fewer</span>
<span class="f1" data-bbox="91.9,648.1,427.3,4.5" data-line="2" data-segment="0">limiting operations than do deﬁnitions that have been used in the past. The name “white noise,” already ﬁrmly entrenched in the</span>
<span class="f1" data-bbox="91.9,657.5,426.9,4.5" data-line="3" data-segment="0">literature, is perhaps somewhat unfortunate. In optics white light means either any continuous spectrum as contrasted with a point</span>
<span class="f1" data-bbox="91.9,667.0,357.5,4.5" data-line="4" data-segment="0">spectrum, or a spectrum which is ﬂat with wavelength (which is not the same as a spectrum ﬂat with frequency).</span>
<span class="f11" data-bbox="102.8,673.9,3.0,3.3" data-line="5" data-segment="0">3</span>
<span class="f1" data-bbox="106.3,676.7,412.3,4.5" data-line="6" data-segment="0">This is the famous ergodic theorem or rather one aspect of this theorem which was proved in somewhat different formulations</span>
<span class="f1" data-bbox="91.9,686.2,427.5,4.5" data-line="7" data-segment="0">by Birkoff, von Neumann, and Koopman, and subsequently generalized by Wiener, Hopf, Hurewicz and others. The literature on</span>
<span class="f1" data-bbox="91.9,695.7,427.5,4.5" data-line="8" data-segment="0">ergodic theory is quite extensive and the reader is referred to the papers of these writers for precise and general formulations; e.g.,</span>
<span class="f1" data-bbox="91.9,705.2,427.3,4.5" data-line="9" data-segment="0">E. Hopf, “Ergodentheorie,” Ergebnisse der Mathematik und ihrer Grenzgebiete, v. 5; “On Causality Statistics and Probability,” Journal</span>
<span class="f2" data-bbox="91.9,714.7,404.7,4.5" data-line="10" data-segment="0">of Mathematics and Physics, v. XIII, No. 1, 1934; N. Wiener, “The Ergodic Theorem,” Duke Mathematical Journal, v. 5, 1939.</span>
</p>
<p>
<span class="f4" data-bbox="300.6,744.5,10.0,5.6" data-line="0" data-segment="0">33</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <p>
<span class="f4" data-bbox="106.9,99.9,412.7,5.6" data-line="0" data-segment="0">Just as we may perform various operations on numbers or functions to obtain new numbers or functions,</span>
<span class="f4" data-bbox="91.9,111.9,427.5,5.6" data-line="1" data-segment="0">we can perform operations on ensembles to obtain new ensembles. Suppose, for example, we have an</span>
<span class="f4" data-bbox="91.9,123.9,427.6,5.6" data-line="2" data-segment="0">ensemble of functions f (t) and an operator T which gives for each function f (t) a resulting function</span>
<span class="f7" data-bbox="91.9,135.8,24.4,5.6" data-line="3" data-segment="0">g (t):</span>
<span class="f7" data-bbox="273.4,147.8,64.4,5.5" data-line="4" data-segment="0">g (t) = T f (t):</span>
</p>
<p>
<span class="f4" data-bbox="91.9,165.7,427.5,5.6" data-line="0" data-segment="0">Probability measure is deﬁned for the set g (t) by means of that for the set f (t). The probability of a certain</span>
<span class="f4" data-bbox="91.9,177.7,427.6,5.6" data-line="1" data-segment="0">subset of the g (t) functions is equal to that of the subset of the f (t) functions which produce members of</span>
<span class="f4" data-bbox="91.9,189.7,427.6,5.6" data-line="2" data-segment="0">the given subset of g functions under the operation T . Physically this corresponds to passing the ensemble</span>
<span class="f4" data-bbox="91.9,201.5,427.5,5.6" data-line="3" data-segment="0">through some device, for example, a ﬁlter, a rectiﬁer or a modulator. The output functions of the device</span>
<span class="f4" data-bbox="91.9,213.5,100.8,5.6" data-line="4" data-segment="0">form the ensemble g (t).</span>
<span class="f4" data-bbox="106.9,225.5,388.9,5.6" data-line="5" data-segment="0">A device or operator T will be called invariant if shifting the input merely shifts the output, i.e., if</span>
</p>
<p>
<span class="f7" data-bbox="274.8,247.4,61.7,5.5" data-line="0" data-segment="0">g (t) = T f (t)</span>
</p>
<p>
<span class="f4" data-bbox="91.9,269.3,29.4,5.6" data-line="0" data-segment="0">implies</span>
<span class="f7" data-bbox="257.5,281.2,96.1,5.6" data-line="1" data-segment="0">g (t +t1) = T f (t +t1)</span>
</p>
<p>
<span class="f4" data-bbox="91.9,299.2,427.3,5.6" data-line="0" data-segment="0">for all f (t) and all t1. It is easily shown (see Appendix 5 that if T is invariant and the input ensemble is</span>
<span class="f4" data-bbox="91.9,311.2,427.3,5.6" data-line="1" data-segment="0">stationary then the output ensemble is stationary. Likewise if the input is ergodic the output will also be</span>
<span class="f4" data-bbox="91.9,323.1,32.5,5.6" data-line="2" data-segment="0">ergodic.</span>
<span class="f4" data-bbox="106.9,335.1,411.6,5.6" data-line="3" data-segment="0">A ﬁlter or a rectiﬁer is invariant under all time translations. The operation of modulation is not since the</span>
<span class="f4" data-bbox="91.9,347.0,427.4,5.6" data-line="4" data-segment="0">carrier phase gives a certain time structure. However, modulation is invariant under all translations which</span>
<span class="f4" data-bbox="91.9,359.0,161.1,5.6" data-line="5" data-segment="0">are multiples of the period of the carrier.</span>
<span class="f4" data-bbox="106.9,371.0,412.4,5.6" data-line="6" data-segment="0">Wiener has pointed out the intimate relation between the invariance of physical devices under time</span>
<span class="f6" data-bbox="216.8,379.3,3.7,4.1" data-line="7" data-segment="0">4</span>
<span class="f4" data-bbox="91.9,382.9,427.6,5.6" data-line="8" data-segment="0">translations and Fourier theory. He has shown, in fact, that if a device is linear as well as invariant Fourier</span>
<span class="f4" data-bbox="91.9,394.9,316.6,5.6" data-line="9" data-segment="0">analysis is then the appropriate mathematical tool for dealing with the problem.</span>
<span class="f4" data-bbox="106.9,406.9,412.3,5.6" data-line="10" data-segment="0">An ensemble of functions is the appropriate mathematical representation of the messages produced by</span>
<span class="f4" data-bbox="91.9,418.7,427.8,5.6" data-line="11" data-segment="0">a continuous source (for example, speech), of the signals produced by a transmitter, and of the perturbing</span>
<span class="f4" data-bbox="91.9,430.7,427.8,5.6" data-line="12" data-segment="0">noise. Communication theory is properly concerned, as has been emphasized by Wiener, not with operations</span>
<span class="f4" data-bbox="91.9,442.6,427.5,5.6" data-line="13" data-segment="0">on particular functions, but with operations on ensembles of functions. A communication system is designed</span>
<span class="f4" data-bbox="91.9,454.6,423.3,5.6" data-line="14" data-segment="0">not for a particular speech function and still less for a sine wave, but for the ensemble of speech functions.</span>
</p>
<p>
<span class="f4" data-bbox="203.2,475.5,204.8,5.6" data-line="0" data-segment="0">19. BAND LIMITED ENSEMBLES OF FUNCTIONS</span>
</p>
<p>
<span class="f4" data-bbox="91.9,494.2,427.5,5.6" data-line="0" data-segment="0">If a function of time f (t) is limited to the band from 0 to W cycles per second it is completely determined</span>
<span class="f6" data-bbox="330.7,502.3,3.7,4.1" data-line="1" data-segment="0">1</span>
<span class="f4" data-bbox="91.9,506.2,427.4,5.6" data-line="2" data-segment="0">by giving its ordinates at a series of discrete points spaced seconds apart in the manner indicated by the</span>
<span class="f6" data-bbox="327.4,509.6,9.5,4.1" data-line="3" data-segment="0">2W</span>
<span class="f6" data-bbox="157.7,515.8,3.7,4.1" data-line="4" data-segment="0">5</span>
<span class="f4" data-bbox="91.9,519.4,65.8,5.6" data-line="5" data-segment="0">following result.</span>
</p>
<p>
<span class="f7" data-bbox="106.9,534.3,238.4,5.7" data-line="0" data-segment="0">Theorem 13: Let f (t) contain no frequencies over W . Then</span>
</p>
<p>
<span class="f15" data-bbox="281.0,551.7,5.3,4.8" data-line="0" data-segment="0">∞</span>
<span class="f4" data-bbox="301.4,555.7,59.4,8.5" data-line="1" data-segment="0">sin  (2W t  n)</span>
<span class="f7" data-bbox="250.8,562.4,49.1,5.5" data-line="2" data-segment="0">f (t) = Xn</span>
<span class="f20" data-bbox="278.5,564.5,10.3,9.4" data-line="3" data-segment="0">∑</span>
<span class="f10" data-bbox="307.8,569.2,46.7,8.5" data-line="4" data-segment="0"> (2W t  n)</span>
<span class="f14" data-bbox="278.2,572.1,11.0,4.8" data-line="5" data-segment="0"> ∞</span>
</p>
<p>
<span class="f4" data-bbox="91.9,591.4,240.5,15.9" data-line="0" data-segment="0">where    </span>
<span class="f7" data-bbox="315.7,596.7,5.0,5.5" data-line="1" data-segment="0">n</span>
<span class="f7" data-bbox="276.1,603.4,59.0,5.5" data-line="2" data-segment="0">Xn = f :</span>
<span class="f4" data-bbox="311.2,610.3,12.9,5.6" data-line="3" data-segment="0">2W</span>
<span class="f11" data-bbox="102.8,623.5,3.0,3.3" data-line="4" data-segment="0">4</span>
<span class="f1" data-bbox="106.3,626.3,412.4,4.5" data-line="5" data-segment="0">Communication theory is heavily indebted to Wiener for much of its basic philosophy and theory. His classic NDRC report,</span>
<span class="f2" data-bbox="91.9,635.8,427.3,4.5" data-line="6" data-segment="0">The Interpolation, Extrapolation and Smoothing of Stationary Time Series (Wiley, 1949), contains the ﬁrst clear-cut formulation of</span>
<span class="f1" data-bbox="91.9,645.3,427.1,4.5" data-line="7" data-segment="0">communication theory as a statistical problem, the study of operations on time series. This work, although chieﬂy concerned with the</span>
<span class="f1" data-bbox="91.9,654.8,427.0,4.5" data-line="8" data-segment="0">linear prediction and ﬁltering problem, is an important collateral reference in connection with the present paper. We may also refer</span>
<span class="f1" data-bbox="91.9,664.1,350.4,4.5" data-line="9" data-segment="0">here to Wiener’s Cybernetics (Wiley, 1948), dealing with the general problems of communication and control.</span>
<span class="f11" data-bbox="102.8,671.1,3.0,3.3" data-line="10" data-segment="0">5</span>
<span class="f1" data-bbox="106.3,674.0,413.0,4.5" data-line="11" data-segment="0">For a proof of this theorem and further discussion see the author’s paper “Communication in the Presence of Noise” published in</span>
<span class="f1" data-bbox="91.9,683.5,278.1,4.5" data-line="12" data-segment="0">the Proceedings of the Institute of Radio Engineers, v. 37, No. 1, Jan., 1949, pp. 10–21.</span>
</p>
<p>
<span class="f4" data-bbox="300.6,744.5,10.0,5.6" data-line="0" data-segment="0">34</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                                                                                                                                                              <p>
<span class="f4" data-bbox="106.9,99.9,412.6,5.6" data-line="0" data-segment="0">In this expansion f (t) is represented as a sum of orthogonal functions. The coefﬁcients Xn of the various</span>
<span class="f4" data-bbox="91.9,111.9,427.5,5.6" data-line="1" data-segment="0">terms can be considered as coordinates in an inﬁnite dimensional “function space.” In this space each</span>
<span class="f4" data-bbox="91.9,123.9,298.6,5.6" data-line="2" data-segment="0">function corresponds to precisely one point and each point to one function.</span>
<span class="f4" data-bbox="106.9,135.8,412.5,5.6" data-line="3" data-segment="0">A function can be considered to be substantially limited to a time T if all the ordinates Xn outside this</span>
<span class="f4" data-bbox="91.9,147.8,427.3,5.6" data-line="4" data-segment="0">interval of time are zero. In this case all but 2TW of the coordinates will be zero. Thus functions limited to</span>
<span class="f4" data-bbox="91.9,159.8,309.9,5.6" data-line="5" data-segment="0">a band W and duration T correspond to points in a space of 2TW dimensions.</span>
<span class="f4" data-bbox="106.9,171.7,412.7,5.6" data-line="6" data-segment="0">A subset of the functions of band W and duration T corresponds to a region in this space. For example,</span>
<span class="f4" data-bbox="91.9,183.7,427.6,5.6" data-line="7" data-segment="0">the functions whose total energy is less than or equal to E correspond to points in a 2TW dimensional sphere</span>
<span class="f16" data-bbox="155.3,187.3,8.3,8.5" data-line="8" data-segment="0">p</span>
<span class="f4" data-bbox="91.9,195.5,94.9,5.6" data-line="9" data-segment="0">with radius r = 2W E.</span>
<span class="f4" data-bbox="106.9,207.5,412.5,5.6" data-line="10" data-segment="0">An ensemble of functions of limited duration and band will be represented by a probability distribution</span>
<span class="f7" data-bbox="92.6,219.5,426.9,5.6" data-line="11" data-segment="0">p(x1;:::;xn) in the corresponding n dimensional space. If the ensemble is not limited in time we can consider</span>
<span class="f4" data-bbox="91.9,231.4,426.1,5.6" data-line="12" data-segment="0">the 2TW coordinates in a given interval T to represent substantially the part of the function in the interval T</span>
<span class="f4" data-bbox="91.9,243.4,427.5,5.6" data-line="13" data-segment="0">and the probability distribution p(x1;:::;xn) to give the statistical structure of the ensemble for intervals of</span>
<span class="f4" data-bbox="91.9,255.4,53.4,5.6" data-line="14" data-segment="0">that duration.</span>
</p>
<p>
<span class="f4" data-bbox="201.6,276.3,207.9,5.6" data-line="0" data-segment="0">20. ENTROPY OF A CONTINUOUS DISTRIBUTION</span>
</p>
<p>
<span class="f4" data-bbox="91.9,295.0,300.2,5.6" data-line="0" data-segment="0">The entropy of a discrete set of probabilities p1;:::; pn has been deﬁned as:</span>
</p>
<p>
<span class="f7" data-bbox="268.3,316.9,74.5,8.5" data-line="0" data-segment="0">H =   pi log pi:</span>
<span class="f20" data-bbox="297.4,319.0,10.3,9.4" data-line="1" data-segment="0">∑</span>
</p>
<p>
<span class="f4" data-bbox="91.9,338.8,427.4,5.6" data-line="0" data-segment="0">In an analogous manner we deﬁne the entropy of a continuous distribution with the density distribution</span>
<span class="f4" data-bbox="91.9,350.8,69.0,5.6" data-line="1" data-segment="0">function p(x) by:</span>
<span class="f33" data-bbox="279.0,354.4,5.2,10.0" data-line="2" data-segment="0">Z</span>
<span class="f15" data-bbox="288.1,357.2,5.3,4.8" data-line="3" data-segment="0">∞</span>
<span class="f7" data-bbox="250.1,366.8,111.1,8.5" data-line="4" data-segment="0">H =   p(x)log p(x)dx:</span>
<span class="f14" data-bbox="284.0,374.7,11.0,4.8" data-line="5" data-segment="0"> ∞</span>
<span class="f4" data-bbox="91.9,389.7,226.1,5.6" data-line="6" data-segment="0">With an n dimensional distribution p(x1;:::;xn) we have</span>
<span class="f33" data-bbox="224.6,403.3,25.9,10.0" data-line="7" data-segment="0">Z Z</span>
<span class="f7" data-bbox="195.7,415.6,219.7,8.5" data-line="8" data-segment="0">H =       p(x1;:::;xn)log p(x1;:::;xn)dx1    dxn:</span>
</p>
<p>
<span class="f4" data-bbox="91.9,440.8,427.6,5.6" data-line="0" data-segment="0">If we have two arguments x and y (which may themselves be multidimensional) the joint and conditional</span>
<span class="f4" data-bbox="91.9,452.8,127.4,5.6" data-line="1" data-segment="0">entropies of p(x;y) are given by</span>
<span class="f33" data-bbox="276.6,466.3,10.4,10.0" data-line="2" data-segment="0">ZZ</span>
<span class="f7" data-bbox="227.2,478.7,154.4,8.5" data-line="3" data-segment="0">H(x;y) =   p(x;y)log p(x;y)dx dy</span>
</p>
<p>
<span class="f4" data-bbox="91.9,506.9,14.5,5.6" data-line="0" data-segment="0">and</span>
<span class="f33" data-bbox="276.6,522.8,10.4,10.0" data-line="1" data-segment="0">ZZ</span>
<span class="f7" data-bbox="335.3,528.4,25.4,5.5" data-line="2" data-segment="0">p(x;y)</span>
<span class="f7" data-bbox="232.7,535.3,151.3,8.5" data-line="3" data-segment="0">Hx(y) =   p(x;y)log dx dy</span>
<span class="f7" data-bbox="339.4,542.1,17.2,5.5" data-line="4" data-segment="0">p(x)</span>
<span class="f33" data-bbox="276.6,550.3,10.4,10.0" data-line="5" data-segment="0">ZZ</span>
<span class="f7" data-bbox="335.3,556.0,25.4,5.5" data-line="6" data-segment="0">p(x;y)</span>
<span class="f7" data-bbox="232.7,562.7,151.3,8.5" data-line="7" data-segment="0">Hy(x) =   p(x;y)log dx dy</span>
<span class="f7" data-bbox="339.5,569.6,17.2,5.5" data-line="8" data-segment="0">p(y)</span>
</p>
<p>
<span class="f4" data-bbox="91.9,589.9,24.5,5.6" data-line="0" data-segment="0">where</span>
<span class="f33" data-bbox="295.6,600.9,5.2,10.0" data-line="1" data-segment="0">Z</span>
<span class="f7" data-bbox="266.2,613.4,76.8,5.5" data-line="2" data-segment="0">p(x) = p(x;y)dy</span>
<span class="f33" data-bbox="295.6,624.8,5.2,10.0" data-line="3" data-segment="0">Z</span>
<span class="f7" data-bbox="266.2,637.3,79.6,5.5" data-line="4" data-segment="0">p(y) = p(x;y)dx:</span>
</p>
<p>
<span class="f4" data-bbox="106.9,662.5,412.4,5.6" data-line="0" data-segment="0">The entropies of continuous distributions have most (but not all) of the properties of the discrete case.</span>
<span class="f4" data-bbox="91.9,674.5,142.7,5.6" data-line="1" data-segment="0">In particular we have the following:</span>
</p>
<p>
<span class="f4" data-bbox="104.4,694.4,414.8,5.6" data-line="0" data-segment="0">1. If x is limited to a certain volume v in its space, then H(x) is a maximum and equal to log v when p(x)</span>
<span class="f4" data-bbox="116.9,706.3,125.6,5.6" data-line="1" data-segment="0">is constant (1=v) in the volume.</span>
</p>
<p>
<span class="f4" data-bbox="300.6,744.5,10.0,5.6" data-line="0" data-segment="0">35</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                           <p>
<span class="f4" data-bbox="104.4,99.9,155.8,5.6" data-line="0" data-segment="0">2. With any two variables x, y we have</span>
</p>
<p>
<span class="f7" data-bbox="272.5,121.9,91.3,8.5" data-line="0" data-segment="0">H(x;y)  H(x) + H(y)</span>
</p>
<p>
<span class="f4" data-bbox="116.9,143.8,402.5,5.6" data-line="0" data-segment="0">with equality if (and only if) x and y are independent, i.e., p(x;y) = p(x)p(y) (apart possibly from a</span>
<span class="f4" data-bbox="116.9,155.7,131.2,5.6" data-line="1" data-segment="0">set of points of probability zero).</span>
</p>
<p>
<span class="f4" data-bbox="104.4,175.6,274.1,5.6" data-line="0" data-segment="0">3. Consider a generalized averaging operation of the following type:</span>
<span class="f33" data-bbox="302.0,188.8,5.2,10.0" data-line="1" data-segment="0">Z</span>
<span class="f14" data-bbox="275.2,197.1,2.0,2.5" data-line="2" data-segment="0">0</span>
<span class="f7" data-bbox="270.1,201.2,96.6,5.5" data-line="3" data-segment="0">p (y) = a(x;y)p(x)dx</span>
</p>
<p>
<span class="f4" data-bbox="116.9,226.5,159.4,11.0" data-line="0" data-segment="0">with Z Z</span>
<span class="f7" data-bbox="222.4,240.1,201.7,8.5" data-line="1" data-segment="0">a(x;y)dx = a(x;y)dy = 1; a(x;y)  0:</span>
</p>
<p>
<span class="f14" data-bbox="312.8,258.5,2.0,2.5" data-line="0" data-segment="0">0</span>
<span class="f4" data-bbox="116.9,262.1,402.6,5.6" data-line="1" data-segment="0">Then the entropy of the averaged distribution p (y) is equal to or greater than that of the original</span>
<span class="f4" data-bbox="116.9,274.0,68.6,5.6" data-line="2" data-segment="0">distribution p(x).</span>
</p>
<p>
<span class="f4" data-bbox="104.4,293.9,46.6,5.6" data-line="0" data-segment="0">4. We have</span>
</p>
<p>
<span class="f7" data-bbox="238.3,315.9,159.5,5.5" data-line="0" data-segment="0">H(x;y) = H(x) + Hx(y) = H(y) + Hy(x)</span>
</p>
<p>
<span class="f4" data-bbox="116.9,340.9,14.5,5.6" data-line="0" data-segment="0">and</span>
</p>
<p>
<span class="f7" data-bbox="289.2,365.7,57.8,8.5" data-line="0" data-segment="0">Hx(y)  H(y):</span>
</p>
<p>
<span class="f4" data-bbox="104.4,391.6,415.0,5.6" data-line="0" data-segment="0">5. Let p(x) be a one-dimensional distribution. The form of p(x) giving a maximum entropy subject to the</span>
<span class="f4" data-bbox="116.9,403.6,397.5,5.6" data-line="1" data-segment="0">condition that the standard deviation of x be ﬁxed at   is Gaussian. To show this we must maximize</span>
<span class="f33" data-bbox="302.6,414.7,5.2,10.0" data-line="2" data-segment="0">Z</span>
<span class="f7" data-bbox="261.5,427.1,113.2,8.5" data-line="3" data-segment="0">H(x) =   p(x)log p(x)dx</span>
</p>
<p>
<span class="f4" data-bbox="116.9,452.3,247.6,11.0" data-line="0" data-segment="0">with Z Z</span>
<span class="f6" data-bbox="243.6,461.8,52.7,4.1" data-line="1" data-segment="0">2 2</span>
<span class="f10" data-bbox="237.6,465.9,160.9,5.6" data-line="2" data-segment="0">  = p(x)x dx and 1 = p(x)dx</span>
</p>
<p>
<span class="f4" data-bbox="116.9,487.1,281.6,5.6" data-line="0" data-segment="0">as constraints. This requires, by the calculus of variations, maximizing</span>
<span class="f33" data-bbox="234.5,500.2,5.2,10.0" data-line="1" data-segment="0">Z</span>
<span class="f19" data-bbox="243.7,504.7,144.2,14.9" data-line="2" data-segment="0">   </span>
<span class="f6" data-bbox="345.1,508.6,3.7,4.1" data-line="3" data-segment="0">2</span>
<span class="f16" data-bbox="247.9,512.7,153.7,8.5" data-line="4" data-segment="0"> p(x)log p(x) +  p(x)x +  p(x) dx:</span>
</p>
<p>
<span class="f4" data-bbox="116.9,537.9,95.6,5.6" data-line="0" data-segment="0">The condition for this is</span>
<span class="f6" data-bbox="337.1,545.7,3.7,4.1" data-line="1" data-segment="0">2</span>
<span class="f16" data-bbox="261.2,549.9,113.6,8.5" data-line="2" data-segment="0"> 1  log p(x) +  x +   = 0</span>
</p>
<p>
<span class="f4" data-bbox="116.9,567.8,268.6,5.6" data-line="0" data-segment="0">and consequently (adjusting the constants to satisfy the constraints)</span>
</p>
<p>
<span class="f4" data-bbox="309.8,587.6,51.4,5.6" data-line="0" data-segment="0">1 2 2</span>
<span class="f14" data-bbox="330.6,590.2,34.0,4.1" data-line="1" data-segment="0"> (x =2  )</span>
<span class="f7" data-bbox="269.0,594.1,98.8,8.5" data-line="2" data-segment="0">p(x) = p e :</span>
<span class="f4" data-bbox="307.9,602.3,16.7,5.6" data-line="3" data-segment="0">2  </span>
</p>
<p>
<span class="f4" data-bbox="116.9,622.0,374.2,5.6" data-line="0" data-segment="0">Similarly in n dimensions, suppose the second order moments of p(x1;:::;xn) are ﬁxed at Ai j :</span>
<span class="f33" data-bbox="259.1,635.9,25.9,10.0" data-line="1" data-segment="0">Z Z</span>
<span class="f7" data-bbox="234.8,648.4,166.4,8.5" data-line="2" data-segment="0">Ai j =     xix j p(x1;:::;xn)dx1     dxn:</span>
</p>
<p>
<span class="f4" data-bbox="116.9,674.3,402.8,5.6" data-line="0" data-segment="0">Then the maximum entropy occurs (by a similar calculation) when p(x1;:::;xn) is the n dimensional</span>
<span class="f4" data-bbox="116.9,686.3,229.2,5.6" data-line="1" data-segment="0">Gaussian distribution with the second order moments Ai j .</span>
</p>
<p>
<span class="f4" data-bbox="300.6,744.5,10.0,5.6" data-line="0" data-segment="0">36</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                     <p>
<span class="f4" data-bbox="104.4,99.9,401.7,5.6" data-line="0" data-segment="0">6. The entropy of a one-dimensional Gaussian distribution whose standard deviation is   is given by</span>
</p>
<p>
<span class="f16" data-bbox="324.8,113.1,8.3,8.5" data-line="0" data-segment="0">p</span>
<span class="f7" data-bbox="278.8,121.9,78.6,5.6" data-line="1" data-segment="0">H(x) = log 2 e :</span>
</p>
<p>
<span class="f4" data-bbox="116.9,143.8,115.5,5.6" data-line="0" data-segment="0">This is calculated as follows:</span>
</p>
<p>
<span class="f4" data-bbox="282.4,161.5,51.4,5.7" data-line="0" data-segment="0">1 2 2</span>
<span class="f14" data-bbox="303.1,164.2,34.0,4.1" data-line="1" data-segment="0"> (x =2  )</span>
<span class="f7" data-bbox="241.6,167.9,61.6,8.5" data-line="2" data-segment="0">p(x) = p e</span>
<span class="f4" data-bbox="280.4,176.3,16.7,5.6" data-line="3" data-segment="0">2  </span>
</p>
<p>
<span class="f16" data-bbox="284.8,186.4,48.6,10.1" data-line="0" data-segment="0">p 2</span>
<span class="f7" data-bbox="325.2,190.0,4.4,5.5" data-line="1" data-segment="0">x</span>
<span class="f16" data-bbox="218.2,196.9,101.2,8.5" data-line="2" data-segment="0"> log p(x) = log 2   +</span>
<span class="f6" data-bbox="332.9,200.7,3.7,4.1" data-line="3" data-segment="0">2</span>
<span class="f4" data-bbox="321.8,203.6,10.7,5.6" data-line="4" data-segment="0">2 </span>
<span class="f33" data-bbox="279.8,207.7,5.2,10.0" data-line="5" data-segment="0">Z</span>
</p>
<p>
<span class="f7" data-bbox="238.7,220.0,113.3,8.5" data-line="0" data-segment="0">H(x) =   p(x)log p(x)dx</span>
</p>
<p>
<span class="f33" data-bbox="271.0,234.4,95.4,10.0" data-line="0" data-segment="0">Z Z</span>
<span class="f16" data-bbox="314.2,236.5,87.7,10.0" data-line="1" data-segment="0">p 2</span>
<span class="f7" data-bbox="393.7,240.1,4.4,5.5" data-line="2" data-segment="0">x</span>
<span class="f9" data-bbox="261.0,246.8,157.0,5.6" data-line="3" data-segment="0">= p(x)log 2  dx + p(x) dx</span>
<span class="f6" data-bbox="401.5,250.7,3.7,4.1" data-line="4" data-segment="0">2</span>
<span class="f4" data-bbox="390.5,253.6,10.7,5.6" data-line="5" data-segment="0">2 </span>
<span class="f16" data-bbox="284.8,263.2,49.3,10.0" data-line="6" data-segment="0">p 2</span>
<span class="f10" data-bbox="324.4,266.8,5.6,5.0" data-line="7" data-segment="0"> </span>
<span class="f9" data-bbox="261.0,273.5,58.3,5.6" data-line="8" data-segment="0">= log 2   +</span>
<span class="f6" data-bbox="332.9,277.5,3.7,4.1" data-line="9" data-segment="0">2</span>
<span class="f4" data-bbox="321.8,280.4,10.7,5.6" data-line="10" data-segment="0">2 </span>
<span class="f16" data-bbox="284.8,285.2,58.1,9.7" data-line="11" data-segment="0">p p</span>
<span class="f9" data-bbox="261.0,294.1,86.3,5.6" data-line="12" data-segment="0">= log 2   + log e</span>
<span class="f16" data-bbox="284.8,302.3,8.3,8.5" data-line="13" data-segment="0">p</span>
<span class="f9" data-bbox="261.0,311.2,56.3,5.6" data-line="14" data-segment="0">= log 2 e :</span>
</p>
<p>
<span class="f4" data-bbox="116.9,333.1,381.9,5.6" data-line="0" data-segment="0">Similarly the n dimensional Gaussian distribution with associated quadratic form ai j is given by</span>
</p>
<p>
<span class="f11" data-bbox="315.8,351.1,3.0,3.3" data-line="0" data-segment="0">1</span>
<span class="f19" data-bbox="341.0,353.6,64.3,14.9" data-line="1" data-segment="0">   </span>
<span class="f16" data-bbox="298.2,356.6,20.6,9.6" data-line="2" data-segment="0">jai j j2</span>
<span class="f6" data-bbox="356.0,360.7,3.7,4.1" data-line="3" data-segment="0">1</span>
<span class="f7" data-bbox="231.6,364.6,167.1,8.5" data-line="4" data-segment="0">p(x1;:::;xn) = exp   ai j xix j</span>
<span class="f20" data-bbox="361.9,366.7,10.3,9.4" data-line="5" data-segment="0">∑</span>
<span class="f8" data-bbox="313.0,368.0,46.8,5.1" data-line="6" data-segment="0">n=2 2</span>
<span class="f9" data-bbox="294.2,371.9,18.7,5.6" data-line="7" data-segment="0">(2 )</span>
</p>
<p>
<span class="f4" data-bbox="116.9,392.2,143.7,5.6" data-line="0" data-segment="0">and the entropy can be calculated as</span>
</p>
<p>
<span class="f11" data-bbox="361.2,409.3,3.0,3.3" data-line="0" data-segment="0">1</span>
<span class="f8" data-bbox="326.3,412.1,33.7,4.1" data-line="1" data-segment="0">n=2  </span>
<span class="f7" data-bbox="270.4,414.8,93.8,10.1" data-line="2" data-segment="0">H = log(2 e) jai j j 2</span>
</p>
<p>
<span class="f4" data-bbox="116.9,438.2,213.5,8.5" data-line="0" data-segment="0">where jai j j is the determinant whose elements are ai j .</span>
</p>
<p>
<span class="f4" data-bbox="104.4,458.1,354.6,8.5" data-line="0" data-segment="0">7. If x is limited to a half line ( p(x) = 0 for x  0) and the ﬁrst moment of x is ﬁxed at a:</span>
</p>
<p>
<span class="f33" data-bbox="300.5,471.8,5.2,10.0" data-line="0" data-segment="0">Z</span>
<span class="f15" data-bbox="309.7,474.5,5.3,4.8" data-line="1" data-segment="0">∞</span>
<span class="f7" data-bbox="283.4,484.1,69.2,5.5" data-line="2" data-segment="0">a = p(x)x dx;</span>
<span class="f6" data-bbox="305.6,492.1,3.7,4.1" data-line="3" data-segment="0">0</span>
</p>
<p>
<span class="f4" data-bbox="116.9,509.8,160.3,5.6" data-line="0" data-segment="0">then the maximum entropy occurs when</span>
</p>
<p>
<span class="f4" data-bbox="317.2,529.6,5.0,5.6" data-line="0" data-segment="0">1</span>
<span class="f14" data-bbox="327.7,532.3,22.2,4.1" data-line="1" data-segment="0"> (x=a)</span>
<span class="f7" data-bbox="286.6,536.3,41.2,5.5" data-line="2" data-segment="0">p(x) = e</span>
<span class="f7" data-bbox="317.2,543.2,5.0,5.5" data-line="3" data-segment="0">a</span>
</p>
<p>
<span class="f4" data-bbox="116.9,561.1,86.1,5.6" data-line="0" data-segment="0">and is equal to log ea.</span>
</p>
<p>
<span class="f4" data-bbox="104.4,581.0,415.1,5.6" data-line="0" data-segment="0">8. There is one important difference between the continuous and discrete entropies. In the discrete case</span>
</p>
<p>
<span class="f4" data-bbox="116.9,593.0,402.8,5.6" data-line="0" data-segment="0">the entropy measures in an absolute way the randomness of the chance variable. In the continuous</span>
</p>
<p>
<span class="f4" data-bbox="116.9,605.0,402.4,5.6" data-line="0" data-segment="0">case the measurement is relative to the coordinate system. If we change coordinates the entropy will</span>
</p>
<p>
<span class="f4" data-bbox="116.9,616.9,358.2,8.5" data-line="0" data-segment="0">in general change. In fact if we change to coordinates y1    yn the new entropy is given by</span>
</p>
<p>
<span class="f33" data-bbox="216.5,630.1,193.4,16.3" data-line="0" data-segment="0">Z Z        </span>
<span class="f7" data-bbox="309.6,635.7,93.1,5.5" data-line="1" data-segment="0">x x</span>
<span class="f7" data-bbox="184.3,642.4,267.1,8.5" data-line="2" data-segment="0">H(y) =     p(x1;:::;xn)J log p(x1;:::;xn)J dy1    dyn</span>
<span class="f7" data-bbox="309.6,649.3,93.1,5.5" data-line="3" data-segment="0">y y</span>
</p>
<p>
<span class="f19" data-bbox="148.6,662.7,14.8,14.9" data-line="0" data-segment="0">   </span>
<span class="f8" data-bbox="154.3,666.8,3.3,4.1" data-line="1" data-segment="0">x</span>
<span class="f4" data-bbox="116.9,670.7,402.7,5.6" data-line="2" data-segment="0">where J is the Jacobian of the coordinate transformation. On expanding the logarithm and chang-</span>
<span class="f8" data-bbox="154.3,674.2,3.3,4.1" data-line="3" data-segment="0">y</span>
<span class="f4" data-bbox="116.9,682.7,155.2,8.5" data-line="4" data-segment="0">ing the variables to x1    xn, we obtain:</span>
</p>
<p>
<span class="f33" data-bbox="267.4,695.8,119.6,16.4" data-line="0" data-segment="0">Z Z    </span>
<span class="f7" data-bbox="375.4,701.6,4.4,5.5" data-line="1" data-segment="0">x</span>
<span class="f7" data-bbox="204.5,708.3,227.3,8.5" data-line="2" data-segment="0">H(y) = H(x)       p(x1;:::;xn)log J dx1 :::dxn:</span>
<span class="f7" data-bbox="375.4,715.1,4.4,5.5" data-line="3" data-segment="0">y</span>
</p>
<p>
<span class="f4" data-bbox="300.6,744.5,10.0,5.6" data-line="0" data-segment="0">37</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           <p>
<span class="f4" data-bbox="116.9,99.9,402.8,5.6" data-line="0" data-segment="0">Thus the new entropy is the old entropy less the expected logarithm of the Jacobian. In the continuous</span>
<span class="f4" data-bbox="116.9,111.9,402.5,5.6" data-line="1" data-segment="0">case the entropy can be considered a measure of randomness relative to an assumed standard, namely</span>
<span class="f4" data-bbox="116.9,123.9,402.5,8.5" data-line="2" data-segment="0">the coordinate system chosen with each small volume element dx1    dxn given equal weight. When</span>
<span class="f4" data-bbox="116.9,135.8,403.3,5.6" data-line="3" data-segment="0">we change the coordinate system the entropy in the new system measures the randomness when equal</span>
<span class="f4" data-bbox="116.9,147.8,280.0,8.5" data-line="4" data-segment="0">volume elements dy1    dyn in the new system are given equal weight.</span>
</p>
<p>
<span class="f4" data-bbox="116.9,163.6,402.6,5.6" data-line="0" data-segment="0">In spite of this dependence on the coordinate system the entropy concept is as important in the con-</span>
<span class="f4" data-bbox="116.9,175.5,403.3,5.6" data-line="1" data-segment="0">tinuous case as the discrete case. This is due to the fact that the derived concepts of information rate</span>
<span class="f4" data-bbox="116.9,187.5,402.7,5.6" data-line="2" data-segment="0">and channel capacity depend on the difference of two entropies and this difference does not depend</span>
<span class="f4" data-bbox="116.9,199.5,331.8,5.6" data-line="3" data-segment="0">on the coordinate frame, each of the two terms being changed by the same amount.</span>
</p>
<p>
<span class="f4" data-bbox="116.9,215.2,403.3,5.6" data-line="0" data-segment="0">The entropy of a continuous distribution can be negative. The scale of measurements sets an arbitrary</span>
<span class="f4" data-bbox="116.9,227.2,402.7,5.6" data-line="1" data-segment="0">zero corresponding to a uniform distribution over a unit volume. A distribution which is more conﬁned</span>
<span class="f4" data-bbox="116.9,239.2,402.7,5.6" data-line="2" data-segment="0">than this has less entropy and will be negative. The rates and capacities will, however, always be non-</span>
<span class="f4" data-bbox="116.9,251.1,36.0,5.6" data-line="3" data-segment="0">negative.</span>
</p>
<p>
<span class="f4" data-bbox="104.4,270.8,289.1,5.6" data-line="0" data-segment="0">9. A particular case of changing coordinates is the linear transformation</span>
</p>
<p>
<span class="f7" data-bbox="291.8,292.1,52.4,5.5" data-line="0" data-segment="0">y j = ai j xi:</span>
<span class="f20" data-bbox="312.4,294.2,10.3,9.4" data-line="1" data-segment="0">∑</span>
<span class="f8" data-bbox="316.4,302.2,2.1,4.1" data-line="2" data-segment="0">i</span>
</p>
<p>
<span class="f14" data-bbox="336.0,318.5,9.5,4.1" data-line="0" data-segment="0"> 1</span>
<span class="f4" data-bbox="116.9,322.1,246.1,8.5" data-line="1" data-segment="0">In this case the Jacobian is simply the determinant jai j j and</span>
</p>
<p>
<span class="f7" data-bbox="270.2,343.4,95.8,8.5" data-line="0" data-segment="0">H(y) = H(x) + log jai j j:</span>
</p>
<p>
<span class="f4" data-bbox="116.9,364.7,402.5,5.6" data-line="0" data-segment="0">In the case of a rotation of coordinates (or any measure preserving transformation) J = 1 and H(y) =</span>
<span class="f7" data-bbox="116.9,376.6,22.5,5.6" data-line="1" data-segment="0">H(x).</span>
</p>
<p>
<span class="f4" data-bbox="203.5,397.5,204.1,5.6" data-line="0" data-segment="0">21. ENTROPY OF AN ENSEMBLE OF FUNCTIONS</span>
</p>
<p>
<span class="f4" data-bbox="91.9,416.2,408.9,5.6" data-line="0" data-segment="0">Consider an ergodic ensemble of functions limited to a certain band of width W cycles per second. Let</span>
</p>
<p>
<span class="f7" data-bbox="281.4,437.5,49.2,5.6" data-line="0" data-segment="0">p(x1;:::;xn)</span>
</p>
<p>
<span class="f4" data-bbox="91.9,458.8,427.3,5.6" data-line="0" data-segment="0">be the density distribution function for amplitudes x1;:::;xn at n successive sample points. We deﬁne the</span>
<span class="f4" data-bbox="91.9,470.7,199.1,5.6" data-line="1" data-segment="0">entropy of the ensemble per degree of freedom by</span>
<span class="f33" data-bbox="239.0,484.1,25.9,10.0" data-line="2" data-segment="0">Z Z</span>
<span class="f4" data-bbox="231.7,489.9,5.0,5.6" data-line="3" data-segment="0">1</span>
<span class="f14" data-bbox="189.4,492.5,2.0,2.5" data-line="4" data-segment="0">0</span>
<span class="f7" data-bbox="181.4,496.6,248.4,8.5" data-line="5" data-segment="0">H =  Lim     p(x1;:::;xn)log p(x1;:::;xn)dx1 :::dxn:</span>
<span class="f8" data-bbox="213.0,502.6,23.7,6.3" data-line="6" data-segment="0">n!∞ n</span>
</p>
<p>
<span class="f4" data-bbox="91.9,521.2,427.4,5.6" data-line="0" data-segment="0">We may also deﬁne an entropy H per second by dividing, not by n, but by the time T in seconds for n</span>
<span class="f14" data-bbox="239.2,529.5,2.0,2.5" data-line="1" data-segment="0">0</span>
<span class="f4" data-bbox="91.9,533.2,152.3,5.6" data-line="2" data-segment="0">samples. Since n = 2TW , H = 2W H .</span>
<span class="f4" data-bbox="106.9,545.1,209.0,5.6" data-line="3" data-segment="0">With white thermal noise p is Gaussian and we have</span>
<span class="f16" data-bbox="306.0,557.6,8.3,8.5" data-line="4" data-segment="0">p</span>
<span class="f14" data-bbox="277.6,562.3,2.0,2.5" data-line="5" data-segment="0">0</span>
<span class="f7" data-bbox="269.6,566.5,70.2,5.6" data-line="6" data-segment="0">H = log 2 eN;</span>
<span class="f7" data-bbox="272.2,581.3,69.5,5.6" data-line="7" data-segment="0">H = W log 2 eN:</span>
</p>
<p>
<span class="f4" data-bbox="106.9,602.6,412.6,5.6" data-line="0" data-segment="0">For a given average power N, white noise has the maximum possible entropy. This follows from the</span>
<span class="f4" data-bbox="91.9,614.6,256.1,5.6" data-line="1" data-segment="0">maximizing properties of the Gaussian distribution noted above.</span>
<span class="f4" data-bbox="106.9,626.6,412.6,5.6" data-line="2" data-segment="0">The entropy for a continuous stochastic process has many properties analogous to that for discrete pro-</span>
<span class="f4" data-bbox="91.9,638.5,427.5,5.6" data-line="3" data-segment="0">cesses. In the discrete case the entropy was related to the logarithm of the probability of long sequences,</span>
<span class="f4" data-bbox="91.9,650.5,427.9,5.6" data-line="4" data-segment="0">and to the number of reasonably probable sequences of long length. In the continuous case it is related in</span>
<span class="f4" data-bbox="91.9,662.5,427.5,5.6" data-line="5" data-segment="0">a similar fashion to the logarithm of the probability density for a long series of samples, and the volume of</span>
<span class="f4" data-bbox="91.9,674.3,197.4,5.6" data-line="6" data-segment="0">reasonably high probability in the function space.</span>
<span class="f4" data-bbox="106.9,686.3,408.9,5.6" data-line="7" data-segment="0">More precisely, if we assume p(x1;:::;xn) continuous in all the xi for all n, then for sufﬁciently large n</span>
<span class="f19" data-bbox="272.8,701.2,49.6,14.9" data-line="8" data-segment="0">   </span>
<span class="f19" data-bbox="272.8,705.9,49.6,16.3" data-line="9" data-segment="0"> log p 0 </span>
<span class="f19" data-bbox="272.8,712.7,65.8,15.4" data-line="10" data-segment="0">   H   &lt;  </span>
<span class="f7" data-bbox="284.5,719.6,5.0,5.5" data-line="11" data-segment="0">n</span>
</p>
<p>
<span class="f4" data-bbox="300.6,744.5,10.0,5.6" data-line="0" data-segment="0">38</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <p>
<span class="f4" data-bbox="91.9,99.9,427.6,5.6" data-line="0" data-segment="0">for all choices of (x1;:::;xn) apart from a set whose total probability is less than  , with   and   arbitrarily</span>
</p>
<p>
<span class="f4" data-bbox="91.9,111.9,409.1,5.6" data-line="0" data-segment="0">small. This follows form the ergodic property if we divide the space into a large number of small cells.</span>
</p>
<p>
<span class="f4" data-bbox="106.9,123.9,412.4,5.6" data-line="0" data-segment="0">The relation of H to volume can be stated as follows: Under the same assumptions consider the n</span>
</p>
<p>
<span class="f4" data-bbox="91.9,135.8,427.4,5.6" data-line="0" data-segment="0">dimensional space corresponding to p(x1;:::;xn). Let Vn(q) be the smallest volume in this space which</span>
</p>
<p>
<span class="f4" data-bbox="91.9,147.8,194.1,5.6" data-line="0" data-segment="0">includes in its interior a total probability q. Then</span>
</p>
<p>
<span class="f4" data-bbox="285.2,165.1,35.9,5.6" data-line="0" data-segment="0">logVn(q)</span>
<span class="f14" data-bbox="342.4,167.7,2.0,2.5" data-line="1" data-segment="0">0</span>
<span class="f4" data-bbox="266.4,171.9,75.3,5.6" data-line="2" data-segment="0">Lim = H</span>
<span class="f8" data-bbox="266.5,177.8,39.2,6.5" data-line="3" data-segment="0">n!∞ n</span>
</p>
<p>
<span class="f4" data-bbox="91.9,193.4,131.1,5.6" data-line="0" data-segment="0">provided q does not equal 0 or 1.</span>
</p>
<p>
<span class="f4" data-bbox="106.9,205.4,412.2,5.6" data-line="0" data-segment="0">These results show that for large n there is a rather well-deﬁned volume (at least in the logarithmic sense)</span>
</p>
<p>
<span class="f4" data-bbox="91.9,217.4,427.4,5.6" data-line="0" data-segment="0">of high probability, and that within this volume the probability density is relatively uniform (again in the</span>
</p>
<p>
<span class="f4" data-bbox="91.9,229.3,75.9,5.6" data-line="0" data-segment="0">logarithmic sense).</span>
</p>
<p>
<span class="f4" data-bbox="106.9,241.3,235.2,5.6" data-line="0" data-segment="0">In the white noise case the distribution function is given by</span>
</p>
<p>
<span class="f4" data-bbox="302.9,257.8,56.5,5.6" data-line="0" data-segment="0">1 1</span>
<span class="f6" data-bbox="381.0,260.3,3.7,4.1" data-line="1" data-segment="0">2</span>
<span class="f7" data-bbox="223.9,264.5,164.0,8.5" data-line="2" data-segment="0">p(x1;:::;xn) = exp   x :</span>
<span class="f20" data-bbox="365.3,266.6,10.3,9.4" data-line="3" data-segment="0">∑</span>
<span class="f8" data-bbox="312.5,266.9,70.6,6.0" data-line="4" data-segment="0">n=2 i</span>
<span class="f9" data-bbox="286.6,271.4,75.9,6.1" data-line="5" data-segment="0">(2 N) 2N</span>
</p>
<p>
<span class="f6" data-bbox="216.6,287.0,3.7,4.1" data-line="0" data-segment="0">2</span>
<span class="f4" data-bbox="91.9,290.6,427.4,7.2" data-line="1" data-segment="0">Since this depends only on ∑ x the surfaces of equal probability density are spheres and the entire distri-</span>
<span class="f8" data-bbox="216.6,293.3,226.2,9.5" data-line="2" data-segment="0">i p</span>
<span class="f4" data-bbox="91.9,302.6,427.4,8.5" data-line="3" data-segment="0">bution has spherical symmetry. The region of high probability is a sphere of radius nN. As n ! ∞ the</span>
<span class="f19" data-bbox="277.4,305.9,10.0,14.9" data-line="4" data-segment="0">p</span>
<span class="f6" data-bbox="409.3,310.6,3.7,4.1" data-line="5" data-segment="0">1</span>
<span class="f4" data-bbox="91.9,314.5,427.4,5.6" data-line="6" data-segment="0">probability of being outside a sphere of radius n(N +  ) approaches zero and times the logarithm of the</span>
<span class="f16" data-bbox="239.2,317.9,173.9,10.3" data-line="7" data-segment="0">p n</span>
<span class="f4" data-bbox="91.9,328.0,180.7,5.6" data-line="8" data-segment="0">volume of the sphere approaches log 2 eN.</span>
</p>
<p>
<span class="f4" data-bbox="106.9,340.0,412.6,5.6" data-line="0" data-segment="0">In the continuous case it is convenient to work not with the entropy H of an ensemble but with a derived</span>
</p>
<p>
<span class="f4" data-bbox="91.9,352.0,427.5,5.6" data-line="0" data-segment="0">quantity which we will call the entropy power. This is deﬁned as the power in a white noise limited to the</span>
<span class="f14" data-bbox="436.2,360.3,2.0,2.5" data-line="1" data-segment="0">0</span>
<span class="f4" data-bbox="91.9,363.9,427.4,5.6" data-line="2" data-segment="0">same band as the original ensemble and having the same entropy. In other words if H is the entropy of an</span>
</p>
<p>
<span class="f4" data-bbox="91.9,375.9,118.6,5.6" data-line="0" data-segment="0">ensemble its entropy power is</span>
<span class="f4" data-bbox="297.2,385.7,5.0,5.6" data-line="1" data-segment="0">1</span>
<span class="f14" data-bbox="338.0,388.4,2.0,2.5" data-line="2" data-segment="0">0</span>
<span class="f7" data-bbox="268.1,392.5,75.1,5.6" data-line="3" data-segment="0">N1 = exp 2H :</span>
<span class="f4" data-bbox="292.0,399.3,15.5,5.6" data-line="4" data-segment="0">2 e</span>
</p>
<p>
<span class="f4" data-bbox="91.9,411.2,427.7,5.6" data-line="0" data-segment="0">In the geometrical picture this amounts to measuring the high probability volume by the squared radius of a</span>
</p>
<p>
<span class="f4" data-bbox="91.9,423.2,426.8,5.6" data-line="0" data-segment="0">sphere having the same volume. Since white noise has the maximum entropy for a given power, the entropy</span>
</p>
<p>
<span class="f4" data-bbox="91.9,435.1,234.4,5.6" data-line="0" data-segment="0">power of any noise is less than or equal to its actual power.</span>
</p>
<p>
<span class="f4" data-bbox="220.6,455.7,170.2,5.6" data-line="0" data-segment="0">22. ENTROPY LOSS IN LINEAR FILTERS</span>
</p>
<p>
<span class="f7" data-bbox="106.9,474.4,412.4,5.7" data-line="0" data-segment="0">Theorem 14: If an ensemble having an entropy H1 per degree of freedom in band W is passed through a</span>
</p>
<p>
<span class="f4" data-bbox="91.9,486.4,260.3,5.7" data-line="0" data-segment="0">ﬁlter with characteristic Y ( f ) the output ensemble has an entropy</span>
<span class="f33" data-bbox="298.6,497.6,5.2,10.0" data-line="1" data-segment="0">Z</span>
<span class="f4" data-bbox="289.2,503.2,5.0,5.6" data-line="2" data-segment="0">1</span>
<span class="f6" data-bbox="350.9,505.9,3.7,4.1" data-line="3" data-segment="0">2</span>
<span class="f7" data-bbox="241.1,509.9,129.0,8.5" data-line="4" data-segment="0">H2 = H1 + log jY ( f )j d f :</span>
<span class="f7" data-bbox="286.7,516.8,22.7,5.5" data-line="5" data-segment="0">W W</span>
</p>
<p>
<span class="f4" data-bbox="106.9,532.5,412.6,5.6" data-line="0" data-segment="0">The operation of the ﬁlter is essentially a linear transformation of coordinates. If we think of the different</span>
</p>
<p>
<span class="f4" data-bbox="91.9,544.4,427.6,5.6" data-line="0" data-segment="0">frequency components as the original coordinate system, the new frequency components are merely the old</span>
</p>
<p>
<span class="f4" data-bbox="91.9,556.4,427.5,5.6" data-line="0" data-segment="0">ones multiplied by factors. The coordinate transformation matrix is thus essentially diagonalized in terms</span>
</p>
<p>
<span class="f4" data-bbox="91.9,568.3,383.9,5.6" data-line="0" data-segment="0">of these coordinates. The Jacobian of the transformation is (for n sine and n cosine components)</span>
</p>
<p>
<span class="f8" data-bbox="296.4,582.4,3.7,4.1" data-line="0" data-segment="0">n</span>
<span class="f6" data-bbox="331.8,588.9,3.7,4.1" data-line="1" data-segment="0">2</span>
<span class="f7" data-bbox="275.3,593.0,56.5,8.5" data-line="2" data-segment="0">J = jY ( fi )j</span>
<span class="f20" data-bbox="292.3,595.1,11.9,9.4" data-line="3" data-segment="0">∏</span>
<span class="f8" data-bbox="292.4,603.2,11.5,4.1" data-line="4" data-segment="0">i=1</span>
</p>
<p>
<span class="f4" data-bbox="91.9,618.8,313.1,5.6" data-line="0" data-segment="0">where the fi are equally spaced through the band W . This becomes in the limit</span>
<span class="f33" data-bbox="283.8,629.6,5.2,10.0" data-line="1" data-segment="0">Z</span>
<span class="f4" data-bbox="274.4,635.3,5.0,5.6" data-line="2" data-segment="0">1</span>
<span class="f6" data-bbox="336.1,637.9,3.7,4.1" data-line="3" data-segment="0">2</span>
<span class="f4" data-bbox="255.8,642.1,99.5,8.5" data-line="4" data-segment="0">exp log jY ( f )j d f :</span>
<span class="f7" data-bbox="271.9,648.9,22.7,5.5" data-line="5" data-segment="0">W W</span>
</p>
<p>
<span class="f4" data-bbox="91.9,664.5,427.5,5.6" data-line="0" data-segment="0">Since J is constant its average value is the same quantity and applying the theorem on the change of entropy</span>
</p>
<p>
<span class="f4" data-bbox="91.9,676.5,427.7,5.6" data-line="0" data-segment="0">with a change of coordinates, the result follows. We may also phrase it in terms of the entropy power. Thus</span>
</p>
<p>
<span class="f4" data-bbox="91.9,688.4,268.7,5.6" data-line="0" data-segment="0">if the entropy power of the ﬁrst ensemble is N1 that of the second is</span>
<span class="f33" data-bbox="289.6,699.2,5.2,10.0" data-line="1" data-segment="0">Z</span>
<span class="f4" data-bbox="280.3,704.9,5.0,5.6" data-line="2" data-segment="0">1</span>
<span class="f6" data-bbox="342.0,707.6,3.7,4.1" data-line="3" data-segment="0">2</span>
<span class="f7" data-bbox="250.1,711.7,111.1,8.5" data-line="4" data-segment="0">N1 exp log jY ( f )j d f :</span>
<span class="f7" data-bbox="277.8,718.5,22.7,5.5" data-line="5" data-segment="0">W W</span>
</p>
<p>
<span class="f4" data-bbox="300.6,744.5,10.0,5.6" data-line="0" data-segment="0">39</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                                   <p>
<span class="f12" data-bbox="289.0,95.9,33.4,5.0" data-line="0" data-segment="0">TABLE I</span>
</p>
<p>
<span class="f11" data-bbox="281.9,122.0,73.6,3.6" data-line="0" data-segment="0">ENTROP Y ENTROP Y</span>
<span class="f11" data-bbox="208.1,128.6,236.3,3.6" data-line="1" data-segment="0">GAI N P OWER P OWER GAI N I MP ULS E RES P ONS E</span>
<span class="f11" data-bbox="284.5,135.2,76.5,3.6" data-line="2" data-segment="0">FACTOR I N DECI BELS</span>
<span class="f34" data-bbox="203.5,148.0,4.2,2.8" data-line="3" data-segment="0">1</span>
</p>
<p>
<span class="f11" data-bbox="404.9,169.9,3.0,3.3" data-line="0" data-segment="0">2</span>
<span class="f1" data-bbox="167.8,172.7,257.3,6.4" data-line="1" data-segment="0">1  ! 1 sin (t=2)</span>
<span class="f36" data-bbox="329.2,178.4,20.3,6.4" data-line="2" data-segment="0"> 8:69</span>
<span class="f11" data-bbox="297.6,181.5,111.2,3.3" data-line="3" data-segment="0">2 2</span>
<span class="f2" data-bbox="294.1,183.8,123.2,4.5" data-line="4" data-segment="0">e t =2</span>
</p>
<p>
<span class="f34" data-bbox="208.6,207.1,64.2,3.7" data-line="0" data-segment="0">0 ! 1</span>
<span class="f34" data-bbox="203.5,221.2,4.2,2.8" data-line="1" data-segment="0">1</span>
</p>
<p>
<span class="f39" data-bbox="388.4,240.3,48.5,9.5" data-line="0" data-segment="0">   </span>
<span class="f11" data-bbox="182.2,242.7,121.7,9.5" data-line="1" data-segment="0">2    </span>
<span class="f1" data-bbox="164.3,244.7,266.7,8.3" data-line="2" data-segment="0">1  ! 2 4 sin t cos t</span>
<span class="f36" data-bbox="329.2,251.6,86.0,6.4" data-line="3" data-segment="0"> 5:33 2  </span>
<span class="f11" data-bbox="399.8,254.8,27.2,3.3" data-line="4" data-segment="0">3 2</span>
<span class="f2" data-bbox="294.1,257.1,129.4,4.4" data-line="5" data-segment="0">e t t</span>
</p>
<p>
<span class="f34" data-bbox="208.6,280.3,64.2,3.7" data-line="0" data-segment="0">0 ! 1</span>
<span class="f34" data-bbox="203.5,294.4,4.2,2.8" data-line="1" data-segment="0">1</span>
</p>
<p>
<span class="f39" data-bbox="369.8,313.6,85.8,9.5" data-line="0" data-segment="0">   </span>
<span class="f11" data-bbox="182.2,317.1,3.0,3.3" data-line="1" data-segment="0">3</span>
<span class="f1" data-bbox="164.3,319.5,285.3,6.8" data-line="2" data-segment="0">1  ! cos t  1 cos t sin t</span>
<span class="f1" data-bbox="288.5,324.8,146.8,6.4" data-line="3" data-segment="0">0:411  3:87 6   +</span>
<span class="f11" data-bbox="388.1,328.0,58.2,3.3" data-line="4" data-segment="0">4 2 3</span>
<span class="f2" data-bbox="385.3,330.3,57.4,4.5" data-line="5" data-segment="0">t 2t t</span>
</p>
<p>
<span class="f34" data-bbox="208.6,353.6,64.2,3.6" data-line="0" data-segment="0">0 ! 1</span>
<span class="f34" data-bbox="203.5,367.7,4.2,2.8" data-line="1" data-segment="0">1</span>
</p>
<p>
<span class="f36" data-bbox="159.6,386.6,6.6,6.4" data-line="0" data-segment="0">p</span>
<span class="f11" data-bbox="184.1,389.2,122.6,9.5" data-line="1" data-segment="0">2    2</span>
<span class="f1" data-bbox="166.2,392.7,255.7,7.1" data-line="2" data-segment="0">1  ! 2   J1 (t)</span>
<span class="f36" data-bbox="329.2,398.1,20.3,6.4" data-line="3" data-segment="0"> 2:67</span>
<span class="f2" data-bbox="294.1,403.5,120.7,4.5" data-line="4" data-segment="0">e 2 t</span>
</p>
<p>
<span class="f34" data-bbox="208.6,426.8,64.2,3.7" data-line="0" data-segment="0">0 ! 1</span>
<span class="f34" data-bbox="203.5,440.9,4.2,2.8" data-line="1" data-segment="0">1</span>
</p>
<p>
<span class="f1" data-bbox="295.6,464.8,154.9,9.5" data-line="0" data-segment="0">1 1    </span>
<span class="f36" data-bbox="326.4,471.3,120.2,6.4" data-line="1" data-segment="0"> 8:69  cos(1   )t  cos t</span>
<span class="f11" data-bbox="295.1,474.4,87.2,3.3" data-line="2" data-segment="0">2  2</span>
<span class="f2" data-bbox="291.6,476.8,87.2,4.4" data-line="3" data-segment="0">e  t</span>
<span class="f35" data-bbox="256.9,489.1,5.4,2.6" data-line="4" data-segment="0"> </span>
<span class="f34" data-bbox="208.6,500.0,64.2,3.7" data-line="5" data-segment="0">0 ! 1</span>
</p>
<p>
<span class="f4" data-bbox="91.9,534.4,427.6,5.6" data-line="0" data-segment="0">The ﬁnal entropy power is the initial entropy power multiplied by the geometric mean gain of the ﬁlter. If</span>
<span class="f4" data-bbox="91.9,546.4,427.5,5.6" data-line="1" data-segment="0">the gain is measured in db, then the output entropy power will be increased by the arithmetic mean db gain</span>
<span class="f4" data-bbox="91.9,558.3,31.4,5.6" data-line="2" data-segment="0">over W .</span>
<span class="f4" data-bbox="106.9,570.3,412.4,5.6" data-line="3" data-segment="0">In Table I the entropy power loss has been calculated (and also expressed in db) for a number of ideal</span>
<span class="f4" data-bbox="91.9,582.3,427.4,5.6" data-line="4" data-segment="0">gain characteristics. The impulsive responses of these ﬁlters are also given for W = 2 , with phase assumed</span>
<span class="f4" data-bbox="91.9,594.2,29.7,5.6" data-line="5" data-segment="0">to be 0.</span>
<span class="f4" data-bbox="106.9,606.2,412.5,5.6" data-line="6" data-segment="0">The entropy loss for many other cases can be obtained from these results. For example the entropy</span>
<span class="f6" data-bbox="160.2,614.5,3.7,4.1" data-line="7" data-segment="0">2</span>
<span class="f4" data-bbox="91.9,618.2,427.5,8.5" data-line="8" data-segment="0">power factor 1=e for the ﬁrst case also applies to any gain characteristic obtain from 1  ! by a measure</span>
<span class="f4" data-bbox="91.9,630.1,427.5,5.6" data-line="9" data-segment="0">preserving transformation of the ! axis. In particular a linearly increasing gain G(!) = !, or a “saw tooth”</span>
<span class="f4" data-bbox="91.9,642.1,427.7,5.6" data-line="10" data-segment="0">characteristic between 0 and 1 have the same entropy loss. The reciprocal gain has the reciprocal factor.</span>
<span class="f6" data-bbox="193.8,650.3,3.7,4.1" data-line="11" data-segment="0">2</span>
<span class="f4" data-bbox="91.9,653.9,352.2,5.6" data-line="12" data-segment="0">Thus 1=! has the factor e . Raising the gain to any power raises the factor to this power.</span>
</p>
<p>
<span class="f4" data-bbox="207.7,674.9,195.8,5.6" data-line="0" data-segment="0">23. ENTROPY OF A SUM OF TWO ENSEMBLES</span>
</p>
<p>
<span class="f4" data-bbox="91.9,693.5,427.6,5.6" data-line="0" data-segment="0">If we have two ensembles of functions f (t) and g (t) we can form a new ensemble by “addition.” Suppose</span>
<span class="f4" data-bbox="91.9,705.5,427.4,5.6" data-line="1" data-segment="0">the ﬁrst ensemble has the probability density function p(x1;:::;xn) and the second q(x1;:::;xn). Then the</span>
</p>
<p>
<span class="f4" data-bbox="300.6,744.5,10.0,5.6" data-line="0" data-segment="0">40</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                                                                                                                                                                                                                    <p>
<span class="f4" data-bbox="91.9,99.9,225.4,5.6" data-line="0" data-segment="0">density function for the sum is given by the convolution:</span>
<span class="f33" data-bbox="229.2,111.3,25.9,10.0" data-line="1" data-segment="0">Z Z</span>
<span class="f7" data-bbox="168.6,123.7,274.0,8.5" data-line="2" data-segment="0">r(x1 ;:::;xn) =     p(y1;:::;yn)q(x1  y1;:::;xn  yn)dy1    dyn:</span>
</p>
<p>
<span class="f4" data-bbox="91.9,147.1,427.8,5.6" data-line="0" data-segment="0">Physically this corresponds to adding the noises or signals represented by the original ensembles of func-</span>
<span class="f4" data-bbox="91.9,159.1,21.9,5.6" data-line="1" data-segment="0">tions.</span>
<span class="f4" data-bbox="106.9,170.9,184.2,5.6" data-line="2" data-segment="0">The following result is derived in Appendix 6.</span>
<span class="f7" data-bbox="106.9,185.9,411.9,5.7" data-line="3" data-segment="0">Theorem 15: Let the average power of two ensembles be N1 and N2 and let their entropy powers be N1</span>
<span class="f4" data-bbox="91.9,197.9,250.9,5.7" data-line="4" data-segment="0">and N2. Then the entropy power of the sum, N3, is bounded by</span>
</p>
<p>
<span class="f7" data-bbox="253.8,218.0,103.6,8.5" data-line="0" data-segment="0">N1 + N2  N3  N1 + N2:</span>
</p>
<p>
<span class="f4" data-bbox="106.9,238.0,412.4,5.6" data-line="0" data-segment="0">White Gaussian noise has the peculiar property that it can absorb any other noise or signal ensemble</span>
<span class="f4" data-bbox="91.9,250.0,427.5,5.6" data-line="1" data-segment="0">which may be added to it with a resultant entropy power approximately equal to the sum of the white noise</span>
<span class="f4" data-bbox="91.9,262.0,427.2,5.6" data-line="2" data-segment="0">power and the signal power (measured from the average signal value, which is normally zero), provided the</span>
<span class="f4" data-bbox="91.9,273.9,238.1,5.6" data-line="3" data-segment="0">signal power is small, in a certain sense, compared to noise.</span>
<span class="f4" data-bbox="106.9,285.9,412.5,5.6" data-line="4" data-segment="0">Consider the function space associated with these ensembles having n dimensions. The white noise</span>
<span class="f4" data-bbox="91.9,297.9,427.9,5.6" data-line="5" data-segment="0">corresponds to the spherical Gaussian distribution in this space. The signal ensemble corresponds to another</span>
<span class="f4" data-bbox="91.9,309.8,427.2,5.6" data-line="6" data-segment="0">probability distribution, not necessarily Gaussian or spherical. Let the second moments of this distribution</span>
<span class="f4" data-bbox="91.9,321.8,363.2,5.6" data-line="7" data-segment="0">about its center of gravity be ai j . That is, if p(x1;:::;xn) is the density distribution function</span>
<span class="f33" data-bbox="242.3,333.9,25.9,10.0" data-line="8" data-segment="0">Z Z</span>
<span class="f7" data-bbox="219.2,346.4,172.3,8.5" data-line="9" data-segment="0">ai j =     p(xi   i)(x j   j )dx1    dxn</span>
</p>
<p>
<span class="f4" data-bbox="91.9,369.8,427.5,5.6" data-line="0" data-segment="0">where the  i are the coordinates of the center of gravity. Now ai j is a positive deﬁnite quadratic form, and</span>
<span class="f4" data-bbox="91.9,381.7,427.6,5.6" data-line="1" data-segment="0">we can rotate our coordinate system to align it with the principal directions of this form. ai j is then reduced</span>
<span class="f4" data-bbox="91.9,393.7,427.5,5.6" data-line="2" data-segment="0">to diagonal form bii. We require that each bii be small compared to N, the squared radius of the spherical</span>
<span class="f4" data-bbox="91.9,405.7,48.5,5.6" data-line="3" data-segment="0">distribution.</span>
<span class="f4" data-bbox="106.9,417.5,412.6,5.6" data-line="4" data-segment="0">In this case the convolution of the noise and signal produce approximately a Gaussian distribution whose</span>
<span class="f4" data-bbox="91.9,429.5,127.6,5.6" data-line="5" data-segment="0">corresponding quadratic form is</span>
<span class="f7" data-bbox="290.5,441.5,30.1,5.5" data-line="6" data-segment="0">N + bii:</span>
</p>
<p>
<span class="f4" data-bbox="91.9,458.3,160.5,5.6" data-line="0" data-segment="0">The entropy power of this distribution is</span>
<span class="f19" data-bbox="271.7,472.6,56.3,14.9" data-line="1" data-segment="0">h i</span>
<span class="f6" data-bbox="328.0,475.1,11.1,4.1" data-line="2" data-segment="0">1=n</span>
<span class="f9" data-bbox="288.2,483.7,35.0,5.5" data-line="3" data-segment="0">(N + b )</span>
<span class="f20" data-bbox="276.4,485.1,42.7,10.2" data-line="4" data-segment="0">∏ ii</span>
</p>
<p>
<span class="f4" data-bbox="91.9,506.1,68.7,5.6" data-line="0" data-segment="0">or approximately</span>
<span class="f19" data-bbox="260.5,520.5,88.6,14.9" data-line="1" data-segment="0">h i</span>
<span class="f6" data-bbox="349.1,522.9,11.1,4.1" data-line="2" data-segment="0">1=n</span>
<span class="f8" data-bbox="280.1,527.3,63.8,4.1" data-line="3" data-segment="0">n n 1</span>
<span class="f9" data-bbox="250.6,531.5,80.2,5.5" data-line="4" data-segment="0">= (N) + bii(N)</span>
<span class="f20" data-bbox="295.1,533.6,10.3,9.4" data-line="5" data-segment="0">∑</span>
</p>
<p>
<span class="f10" data-bbox="278.2,548.6,31.4,6.2" data-line="0" data-segment="0">: 1</span>
<span class="f9" data-bbox="275.6,555.4,59.9,5.5" data-line="1" data-segment="0">= N + bii:</span>
<span class="f20" data-bbox="311.9,557.5,10.3,9.4" data-line="2" data-segment="0">∑</span>
<span class="f7" data-bbox="304.6,562.3,5.0,5.5" data-line="3" data-segment="0">n</span>
<span class="f4" data-bbox="91.9,578.2,265.8,5.6" data-line="4" data-segment="0">The last term is the signal power, while the ﬁrst is the noise power.</span>
</p>
<p>
<span class="f13" data-bbox="198.7,604.1,213.8,6.7" data-line="0" data-segment="0">PART IV: THE CONTINUOUS CHANNEL</span>
</p>
<p>
<span class="f4" data-bbox="199.9,628.0,211.4,5.6" data-line="0" data-segment="0">24. THE CAPACITY OF A CONTINUOUS CHANNEL</span>
</p>
<p>
<span class="f4" data-bbox="91.9,646.7,427.8,5.6" data-line="0" data-segment="0">In a continuous channel the input or transmitted signals will be continuous functions of time f (t) belonging</span>
<span class="f4" data-bbox="91.9,658.7,427.7,5.6" data-line="1" data-segment="0">to a certain set, and the output or received signals will be perturbed versions of these. We will consider</span>
<span class="f4" data-bbox="91.9,670.6,427.5,5.6" data-line="2" data-segment="0">only the case where both transmitted and received signals are limited to a certain band W . They can then</span>
<span class="f4" data-bbox="91.9,682.6,427.6,5.6" data-line="3" data-segment="0">be speciﬁed, for a time T , by 2TW numbers, and their statistical structure by ﬁnite dimensional distribution</span>
<span class="f4" data-bbox="91.9,694.6,302.8,5.6" data-line="4" data-segment="0">functions. Thus the statistics of the transmitted signal will be determined by</span>
</p>
<p>
<span class="f7" data-bbox="265.2,714.7,80.8,5.6" data-line="0" data-segment="0">P(x1;:::;xn) = P(x)</span>
</p>
<p>
<span class="f4" data-bbox="300.6,744.5,10.0,5.6" data-line="0" data-segment="0">41</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           <p>
<span class="f4" data-bbox="91.9,99.9,255.8,5.6" data-line="0" data-segment="0">and those of the noise by the conditional probability distribution</span>
</p>
<p>
<span class="f7" data-bbox="250.8,120.5,109.6,5.6" data-line="0" data-segment="0">Px ;:::;x (y1;:::;yn) = Px(y):</span>
<span class="f11" data-bbox="259.2,123.1,19.9,3.7" data-line="1" data-segment="0">1 n</span>
</p>
<p>
<span class="f4" data-bbox="106.9,141.1,412.5,5.6" data-line="0" data-segment="0">The rate of transmission of information for a continuous channel is deﬁned in a way analogous to that</span>
<span class="f4" data-bbox="91.9,152.9,120.2,5.6" data-line="1" data-segment="0">for a discrete channel, namely</span>
<span class="f7" data-bbox="269.8,164.9,71.6,8.5" data-line="2" data-segment="0">R = H(x) Hy(x)</span>
</p>
<p>
<span class="f4" data-bbox="91.9,182.1,427.4,5.6" data-line="0" data-segment="0">where H(x) is the entropy of the input and Hy(x) the equivocation. The channel capacity C is deﬁned as the</span>
<span class="f4" data-bbox="91.9,194.0,427.8,5.6" data-line="1" data-segment="0">maximum of R when we vary the input over all possible ensembles. This means that in a ﬁnite dimensional</span>
<span class="f4" data-bbox="91.9,206.0,256.2,5.6" data-line="2" data-segment="0">approximation we must vary P(x) = P(x1;:::;xn) and maximize</span>
<span class="f33" data-bbox="213.2,219.8,93.7,10.0" data-line="3" data-segment="0">Z ZZ</span>
<span class="f7" data-bbox="354.8,225.5,26.5,5.5" data-line="4" data-segment="0">P(x;y)</span>
<span class="f16" data-bbox="204.4,232.3,202.6,8.5" data-line="5" data-segment="0">  P(x)log P(x)dx + P(x;y)log dx dy:</span>
<span class="f7" data-bbox="359.0,239.1,18.2,5.5" data-line="6" data-segment="0">P(y)</span>
</p>
<p>
<span class="f4" data-bbox="91.9,257.9,76.7,5.6" data-line="0" data-segment="0">This can be written</span>
<span class="f33" data-bbox="246.6,260.8,10.4,10.0" data-line="1" data-segment="0">ZZ</span>
<span class="f7" data-bbox="309.8,266.6,26.5,5.5" data-line="2" data-segment="0">P(x;y)</span>
<span class="f7" data-bbox="262.2,273.3,102.5,5.6" data-line="3" data-segment="0">P(x;y)log dx dy</span>
<span class="f7" data-bbox="304.9,280.1,36.5,5.5" data-line="4" data-segment="0">P(x)P(y)</span>
<span class="f33" data-bbox="165.6,288.8,114.8,10.0" data-line="5" data-segment="0">ZZ Z</span>
<span class="f4" data-bbox="91.9,301.3,427.4,5.6" data-line="6" data-segment="0">using the fact that P(x;y)log P(x)dx dy = P(x)log P(x)dx. The channel capacity is thus expressed as</span>
</p>
<p>
<span class="f4" data-bbox="91.9,316.5,32.6,5.6" data-line="0" data-segment="0">follows:</span>
<span class="f33" data-bbox="279.0,319.4,10.4,10.0" data-line="1" data-segment="0">ZZ</span>
<span class="f4" data-bbox="270.7,325.0,98.0,5.6" data-line="2" data-segment="0">1 P(x;y)</span>
<span class="f7" data-bbox="211.4,331.7,187.8,5.6" data-line="3" data-segment="0">C = Lim Max P(x;y)log dx dy:</span>
<span class="f8" data-bbox="230.5,338.2,143.3,5.9" data-line="4" data-segment="0">T !∞ P(x) T P(x)P(y)</span>
</p>
<p>
<span class="f4" data-bbox="106.9,354.9,412.7,5.6" data-line="0" data-segment="0">It is obvious in this form that R and C are independent of the coordinate system since the numerator</span>
<span class="f7" data-bbox="193.9,365.5,26.5,5.5" data-line="1" data-segment="0">P(x;y)</span>
<span class="f4" data-bbox="91.9,372.2,427.4,5.6" data-line="2" data-segment="0">and denominator in log will be multiplied by the same factors when x and y are transformed in</span>
<span class="f7" data-bbox="189.0,379.0,36.5,5.5" data-line="3" data-segment="0">P(x)P(y)</span>
<span class="f4" data-bbox="91.9,390.1,427.6,8.5" data-line="4" data-segment="0">any one-to-one way. This integral expression for C is more general than H(x)  Hy(x). Properly interpreted</span>
<span class="f4" data-bbox="91.9,401.9,427.4,8.5" data-line="5" data-segment="0">(see Appendix 7) it will always exist while H(x) Hy(x) may assume an indeterminate form ∞  ∞ in some</span>
<span class="f4" data-bbox="91.9,413.9,427.7,5.6" data-line="6" data-segment="0">cases. This occurs, for example, if x is limited to a surface of fewer dimensions than n in its n dimensional</span>
<span class="f4" data-bbox="91.9,425.9,61.1,5.6" data-line="7" data-segment="0">approximation.</span>
<span class="f4" data-bbox="106.9,437.8,412.5,5.6" data-line="8" data-segment="0">If the logarithmic base used in computing H(x) and Hy(x) is two then C is the maximum number of</span>
<span class="f4" data-bbox="91.9,449.8,427.3,5.6" data-line="9" data-segment="0">binary digits that can be sent per second over the channel with arbitrarily small equivocation, just as in</span>
<span class="f4" data-bbox="91.9,461.7,427.4,5.6" data-line="10" data-segment="0">the discrete case. This can be seen physically by dividing the space of signals into a large number of</span>
<span class="f4" data-bbox="91.9,473.7,427.4,5.6" data-line="11" data-segment="0">small cells, sufﬁciently small so that the probability density Px(y) of signal x being perturbed to point y is</span>
<span class="f4" data-bbox="91.9,485.7,427.3,5.6" data-line="12" data-segment="0">substantially constant over a cell (either of x or y). If the cells are considered as distinct points the situation is</span>
<span class="f4" data-bbox="91.9,497.6,427.4,5.6" data-line="13" data-segment="0">essentially the same as a discrete channel and the proofs used there will apply. But it is clear physically that</span>
<span class="f4" data-bbox="91.9,509.6,427.4,5.6" data-line="14" data-segment="0">this quantizing of the volume into individual points cannot in any practical situation alter the ﬁnal answer</span>
<span class="f4" data-bbox="91.9,521.6,427.4,5.6" data-line="15" data-segment="0">signiﬁcantly, provided the regions are sufﬁciently small. Thus the capacity will be the limit of the capacities</span>
<span class="f4" data-bbox="91.9,533.5,328.8,5.6" data-line="16" data-segment="0">for the discrete subdivisions and this is just the continuous capacity deﬁned above.</span>
<span class="f4" data-bbox="106.9,545.5,412.5,5.6" data-line="17" data-segment="0">On the mathematical side it can be shown ﬁrst (see Appendix 7) that if u is the message, x is the signal,</span>
<span class="f7" data-bbox="91.9,557.3,318.7,5.6" data-line="18" data-segment="0">y is the received signal (perturbed by noise) and v is the recovered message then</span>
</p>
<p>
<span class="f7" data-bbox="245.5,578.0,120.2,8.5" data-line="0" data-segment="0">H(x)  Hy(x)  H(u)  Hv(u)</span>
</p>
<p>
<span class="f4" data-bbox="91.9,598.5,427.4,5.6" data-line="0" data-segment="0">regardless of what operations are performed on u to obtain x or on y to obtain v. Thus no matter how we</span>
<span class="f4" data-bbox="91.9,610.4,427.6,5.6" data-line="1" data-segment="0">encode the binary digits to obtain the signal, or how we decode the received signal to recover the message,</span>
<span class="f4" data-bbox="91.9,622.4,427.5,5.6" data-line="2" data-segment="0">the discrete rate for the binary digits does not exceed the channel capacity we have deﬁned. On the other</span>
<span class="f4" data-bbox="91.9,634.4,427.2,5.6" data-line="3" data-segment="0">hand, it is possible under very general conditions to ﬁnd a coding system for transmitting binary digits at the</span>
<span class="f4" data-bbox="91.9,646.3,427.4,5.6" data-line="4" data-segment="0">rate C with as small an equivocation or frequency of errors as desired. This is true, for example, if, when we</span>
<span class="f4" data-bbox="91.9,658.3,427.4,5.6" data-line="5" data-segment="0">take a ﬁnite dimensional approximating space for the signal functions, P(x;y) is continuous in both x and y</span>
<span class="f4" data-bbox="91.9,670.1,172.8,5.6" data-line="6" data-segment="0">except at a set of points of probability zero.</span>
<span class="f4" data-bbox="106.9,682.1,412.2,5.6" data-line="7" data-segment="0">An important special case occurs when the noise is added to the signal and is independent of it (in the</span>
<span class="f4" data-bbox="91.9,694.1,310.1,8.5" data-line="8" data-segment="0">probability sense). Then Px(y) is a function only of the difference n = (y  x),</span>
</p>
<p>
<span class="f7" data-bbox="271.9,714.7,67.6,8.5" data-line="0" data-segment="0">Px(y) = Q(y  x)</span>
</p>
<p>
<span class="f4" data-bbox="300.6,744.5,10.0,5.6" data-line="0" data-segment="0">42</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                                                                                                                                                                                      <p>
<span class="f4" data-bbox="91.9,99.9,427.6,5.6" data-line="0" data-segment="0">and we can assign a deﬁnite entropy to the noise (independent of the statistics of the signal), namely the</span>
<span class="f4" data-bbox="91.9,111.9,284.5,5.6" data-line="1" data-segment="0">entropy of the distribution Q(n). This entropy will be denoted by H(n).</span>
<span class="f7" data-bbox="106.9,126.9,412.3,5.7" data-line="2" data-segment="0">Theorem 16: If the signal and noise are independent and the received signal is the sum of the transmitted</span>
<span class="f4" data-bbox="91.9,138.8,203.5,5.7" data-line="3" data-segment="0">signal and the noise then the rate of transmission is</span>
</p>
<p>
<span class="f7" data-bbox="269.5,159.4,72.4,8.5" data-line="0" data-segment="0">R = H(y)  H(n);</span>
</p>
<p>
<span class="f4" data-bbox="91.9,180.2,362.9,5.7" data-line="0" data-segment="0">i.e., the entropy of the received signal less the entropy of the noise. The channel capacity is</span>
</p>
<p>
<span class="f7" data-bbox="259.2,200.8,92.5,8.5" data-line="0" data-segment="0">C = Max H(y)  H(n):</span>
<span class="f8" data-bbox="280.7,208.1,13.6,4.1" data-line="1" data-segment="0">P(x)</span>
</p>
<p>
<span class="f4" data-bbox="106.9,227.5,101.4,5.6" data-line="0" data-segment="0">We have, since y = x + n:</span>
<span class="f7" data-bbox="269.5,239.3,72.2,5.5" data-line="1" data-segment="0">H(x;y) = H(x;n):</span>
</p>
<p>
<span class="f4" data-bbox="91.9,256.5,280.1,5.6" data-line="0" data-segment="0">Expanding the left side and using the fact that x and n are independent</span>
</p>
<p>
<span class="f7" data-bbox="245.9,277.3,119.8,5.5" data-line="0" data-segment="0">H(y) + Hy(x) = H(x) + H(n):</span>
</p>
<p>
<span class="f4" data-bbox="91.9,297.9,25.6,5.6" data-line="0" data-segment="0">Hence</span>
<span class="f7" data-bbox="236.8,309.9,138.0,8.5" data-line="1" data-segment="0">R = H(x)  Hy(x) = H(y)  H(n):</span>
</p>
<p>
<span class="f4" data-bbox="106.9,327.1,412.6,5.6" data-line="0" data-segment="0">Since H(n) is independent of P(x), maximizing R requires maximizing H(y), the entropy of the received</span>
<span class="f4" data-bbox="91.9,338.9,427.8,5.6" data-line="1" data-segment="0">signal. If there are certain constraints on the ensemble of transmitted signals, the entropy of the received</span>
<span class="f4" data-bbox="91.9,350.9,216.5,5.6" data-line="2" data-segment="0">signal must be maximized subject to these constraints.</span>
</p>
<p>
<span class="f4" data-bbox="165.8,371.7,279.6,5.6" data-line="0" data-segment="0">25. CHANNEL CAPACITY WITH AN AVERAGE POWER LIMITATION</span>
</p>
<p>
<span class="f4" data-bbox="91.9,390.4,427.5,5.6" data-line="0" data-segment="0">A simple application of Theorem 16 is the case when the noise is a white thermal noise and the transmitted</span>
<span class="f4" data-bbox="91.9,402.4,426.9,5.6" data-line="1" data-segment="0">signals are limited to a certain average power P. Then the received signals have an average power P + N</span>
<span class="f4" data-bbox="91.9,414.3,427.3,5.6" data-line="2" data-segment="0">where N is the average noise power. The maximum entropy for the received signals occurs when they also</span>
<span class="f4" data-bbox="91.9,426.3,427.6,5.6" data-line="3" data-segment="0">form a white noise ensemble since this is the greatest possible entropy for a power P +N and can be obtained</span>
<span class="f4" data-bbox="91.9,438.3,427.5,5.6" data-line="4" data-segment="0">by a suitable choice of transmitted signals, namely if they form a white noise ensemble of power P. The</span>
<span class="f4" data-bbox="91.9,450.2,212.6,5.6" data-line="5" data-segment="0">entropy (per second) of the received ensemble is then</span>
</p>
<p>
<span class="f7" data-bbox="252.6,470.8,106.1,5.6" data-line="0" data-segment="0">H(y) = W log 2 e(P + N);</span>
</p>
<p>
<span class="f4" data-bbox="91.9,491.6,94.5,5.6" data-line="0" data-segment="0">and the noise entropy is</span>
<span class="f7" data-bbox="264.5,503.5,82.2,5.6" data-line="1" data-segment="0">H(n) = W log 2 eN:</span>
</p>
<p>
<span class="f4" data-bbox="91.9,520.6,93.8,5.6" data-line="0" data-segment="0">The channel capacity is</span>
<span class="f7" data-bbox="345.2,530.3,23.4,5.5" data-line="1" data-segment="0">P + N</span>
<span class="f7" data-bbox="237.7,537.1,135.4,8.5" data-line="2" data-segment="0">C = H(y) H(n) = W log :</span>
<span class="f7" data-bbox="353.5,543.9,6.7,5.5" data-line="3" data-segment="0">N</span>
<span class="f4" data-bbox="106.9,557.1,147.4,5.6" data-line="4" data-segment="0">Summarizing we have the following:</span>
<span class="f7" data-bbox="106.9,572.0,412.4,5.7" data-line="5" data-segment="0">Theorem 17: The capacity of a channel of band W perturbed by white thermal noise power N when the</span>
<span class="f4" data-bbox="91.9,584.0,208.0,5.7" data-line="6" data-segment="0">average transmitter power is limited to P is given by</span>
</p>
<p>
<span class="f7" data-bbox="313.6,602.2,23.3,5.5" data-line="0" data-segment="0">P + N</span>
<span class="f7" data-bbox="269.4,609.1,72.0,5.6" data-line="1" data-segment="0">C = W log :</span>
<span class="f7" data-bbox="321.8,615.9,6.7,5.5" data-line="2" data-segment="0">N</span>
</p>
<p>
<span class="f4" data-bbox="106.9,632.6,412.3,5.6" data-line="0" data-segment="0">This means that by sufﬁciently involved encoding systems we can transmit binary digits at the rate</span>
<span class="f7" data-bbox="121.3,642.2,23.3,5.5" data-line="1" data-segment="0">P + N</span>
<span class="f7" data-bbox="91.4,648.9,427.7,5.6" data-line="2" data-segment="0">W log bits per second, with arbitrarily small frequency of errors. It is not possible to transmit at a</span>
<span class="f6" data-bbox="114.8,651.4,3.7,4.1" data-line="3" data-segment="0">2</span>
<span class="f7" data-bbox="129.6,655.7,6.7,5.5" data-line="4" data-segment="0">N</span>
<span class="f4" data-bbox="91.9,663.7,328.5,5.6" data-line="5" data-segment="0">higher rate by any encoding system without a deﬁnite positive frequency of errors.</span>
<span class="f4" data-bbox="106.9,675.7,412.3,5.6" data-line="6" data-segment="0">To approximate this limiting rate of transmission the transmitted signals must approximate, in statistical</span>
<span class="f6" data-bbox="195.4,683.9,3.7,4.1" data-line="7" data-segment="0">6</span>
<span class="f4" data-bbox="91.9,687.5,427.2,5.6" data-line="8" data-segment="0">properties, a white noise. A system which approaches the ideal rate may be described as follows: Let</span>
<span class="f11" data-bbox="102.8,702.3,3.0,3.3" data-line="9" data-segment="0">6</span>
<span class="f1" data-bbox="106.3,705.2,413.1,4.5" data-line="10" data-segment="0">This and other properties of the white noise case are discussed from the geometrical point of view in “Communication in the</span>
<span class="f1" data-bbox="91.9,714.7,88.0,4.5" data-line="11" data-segment="0">Presence of Noise,” loc. cit.</span>
</p>
<p>
<span class="f4" data-bbox="300.6,744.5,10.0,5.6" data-line="0" data-segment="0">43</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 <p>
<span class="f8" data-bbox="117.8,96.3,2.9,4.1" data-line="0" data-segment="0">s</span>
<span class="f7" data-bbox="91.9,99.9,427.6,5.6" data-line="1" data-segment="0">M = 2 samples of white noise be constructed each of duration T . These are assigned binary numbers from</span>
<span class="f4" data-bbox="91.9,111.9,427.6,8.5" data-line="2" data-segment="0">0 to M  1. At the transmitter the message sequences are broken up into groups of s and for each group</span>
<span class="f4" data-bbox="91.9,123.9,427.5,5.6" data-line="3" data-segment="0">the corresponding noise sample is transmitted as the signal. At the receiver the M samples are known and</span>
<span class="f4" data-bbox="91.9,135.8,427.5,5.6" data-line="4" data-segment="0">the actual received signal (perturbed by noise) is compared with each of them. The sample which has the</span>
<span class="f4" data-bbox="91.9,147.8,427.6,5.6" data-line="5" data-segment="0">least R.M.S. discrepancy from the received signal is chosen as the transmitted signal and the corresponding</span>
<span class="f4" data-bbox="91.9,159.8,427.5,5.6" data-line="6" data-segment="0">binary number reconstructed. This process amounts to choosing the most probable (a posteriori) signal.</span>
<span class="f4" data-bbox="91.9,171.7,427.4,5.6" data-line="7" data-segment="0">The number M of noise samples used will depend on the tolerable frequency   of errors, but for almost all</span>
<span class="f4" data-bbox="91.9,183.7,119.7,5.6" data-line="8" data-segment="0">selections of samples we have</span>
</p>
<p>
<span class="f4" data-bbox="268.8,204.2,106.9,5.6" data-line="0" data-segment="0">log M( ;T ) P + N</span>
<span class="f4" data-bbox="231.1,211.0,149.0,5.6" data-line="1" data-segment="0">Lim Lim = W log ;</span>
<span class="f29" data-bbox="232.2,217.4,135.1,6.0" data-line="2" data-segment="0"> !0 T !∞ T N</span>
</p>
<p>
<span class="f4" data-bbox="91.9,236.5,427.2,5.6" data-line="0" data-segment="0">so that no matter how small   is chosen, we can, by taking T sufﬁciently large, transmit as near as we wish</span>
<span class="f7" data-bbox="134.2,246.1,23.3,5.5" data-line="1" data-segment="0">P +N</span>
<span class="f4" data-bbox="91.9,252.8,176.7,5.6" data-line="2" data-segment="0">to T W log binary digits in the time T .</span>
<span class="f7" data-bbox="142.4,259.6,6.7,5.5" data-line="3" data-segment="0">N</span>
<span class="f7" data-bbox="238.1,267.3,23.4,5.5" data-line="4" data-segment="0">P +N</span>
<span class="f4" data-bbox="106.9,274.0,412.6,5.6" data-line="5" data-segment="0">Formulas similar to C = W log for the white noise case have been developed independently</span>
<span class="f7" data-bbox="246.4,280.9,6.7,5.5" data-line="6" data-segment="0">N</span>
<span class="f4" data-bbox="91.9,288.8,427.4,5.6" data-line="7" data-segment="0">by several other writers, although with somewhat different interpretations. We may mention the work of</span>
<span class="f6" data-bbox="135.2,297.2,62.0,4.1" data-line="8" data-segment="0">7 8</span>
<span class="f4" data-bbox="91.9,300.8,246.1,5.6" data-line="9" data-segment="0">N. Wiener, W. G. Tuller, and H. Sullivan in this connection.</span>
<span class="f4" data-bbox="106.9,312.7,412.2,5.6" data-line="10" data-segment="0">In the case of an arbitrary perturbing noise (not necessarily white thermal noise) it does not appear that</span>
<span class="f4" data-bbox="91.9,324.7,427.7,5.6" data-line="11" data-segment="0">the maximizing problem involved in determining the channel capacity C can be solved explicitly. However,</span>
<span class="f4" data-bbox="91.9,336.7,427.4,5.6" data-line="12" data-segment="0">upper and lower bounds can be set for C in terms of the average noise power N the noise entropy power N1.</span>
<span class="f4" data-bbox="91.9,348.5,427.2,5.6" data-line="13" data-segment="0">These bounds are sufﬁciently close together in most practical cases to furnish a satisfactory solution to the</span>
<span class="f4" data-bbox="91.9,360.5,35.9,5.6" data-line="14" data-segment="0">problem.</span>
<span class="f7" data-bbox="106.9,375.4,412.4,5.7" data-line="15" data-segment="0">Theorem 18: The capacity of a channel of band W perturbed by an arbitrary noise is bounded by the</span>
<span class="f4" data-bbox="91.9,387.4,46.1,5.7" data-line="16" data-segment="0">inequalities</span>
<span class="f7" data-bbox="263.5,397.0,107.7,5.6" data-line="17" data-segment="0">P + N1 P + N</span>
<span class="f7" data-bbox="238.0,403.7,107.6,8.5" data-line="18" data-segment="0">W log  C  W log</span>
<span class="f7" data-bbox="271.9,410.6,92.6,5.6" data-line="19" data-segment="0">N1 N1</span>
</p>
<p>
<span class="f4" data-bbox="91.9,425.9,24.5,5.7" data-line="0" data-segment="0">where</span>
</p>
<p>
<span class="f7" data-bbox="244.3,447.8,121.7,5.7" data-line="0" data-segment="0">P = average transmitter power</span>
<span class="f7" data-bbox="243.1,462.8,100.9,5.7" data-line="1" data-segment="0">N = average noise power</span>
<span class="f7" data-bbox="239.8,477.7,131.9,5.7" data-line="2" data-segment="0">N1 = entropy power of the noise.</span>
</p>
<p>
<span class="f4" data-bbox="106.9,499.6,412.4,5.6" data-line="0" data-segment="0">Here again the average power of the perturbed signals will be P + N. The maximum entropy for this</span>
<span class="f4" data-bbox="91.9,511.6,427.5,5.6" data-line="1" data-segment="0">power would occur if the received signal were white noise and would be W log 2 e(P + N). It may not</span>
<span class="f4" data-bbox="91.9,523.5,427.3,5.6" data-line="2" data-segment="0">be possible to achieve this; i.e., there may not be any ensemble of transmitted signals which, added to the</span>
<span class="f4" data-bbox="91.9,535.5,427.4,5.6" data-line="3" data-segment="0">perturbing noise, produce a white thermal noise at the receiver, but at least this sets an upper bound to H(y).</span>
<span class="f4" data-bbox="91.9,547.4,75.3,5.6" data-line="4" data-segment="0">We have, therefore</span>
</p>
<p>
<span class="f7" data-bbox="228.7,569.3,89.8,8.5" data-line="0" data-segment="0">C = Max H(y)  H(n)</span>
<span class="f16" data-bbox="237.8,584.3,144.1,8.5" data-line="1" data-segment="0"> W log 2 e(P + N)  W log 2 eN1:</span>
</p>
<p>
<span class="f4" data-bbox="91.9,606.2,427.3,5.6" data-line="0" data-segment="0">This is the upper limit given in the theorem. The lower limit can be obtained by considering the rate if we</span>
<span class="f4" data-bbox="91.9,618.2,427.4,5.6" data-line="1" data-segment="0">make the transmitted signal a white noise, of power P. In this case the entropy power of the received signal</span>
<span class="f4" data-bbox="91.9,630.2,427.7,5.6" data-line="2" data-segment="0">must be at least as great as that of a white noise of power P + N1 since we have shown in in a previous</span>
<span class="f4" data-bbox="91.9,642.1,428.1,5.6" data-line="3" data-segment="0">theorem that the entropy power of the sum of two ensembles is greater than or equal to the sum of the</span>
<span class="f4" data-bbox="91.9,654.1,135.1,5.6" data-line="4" data-segment="0">individual entropy powers. Hence</span>
</p>
<p>
<span class="f4" data-bbox="242.6,675.9,125.9,8.5" data-line="0" data-segment="0">Max H(y)  W log 2 e(P + N1)</span>
</p>
<p>
<span class="f11" data-bbox="102.8,691.4,3.0,3.3" data-line="0" data-segment="0">7</span>
<span class="f2" data-bbox="106.3,694.3,65.5,4.4" data-line="1" data-segment="0">Cybernetics, loc. cit.</span>
<span class="f11" data-bbox="102.8,701.2,3.0,3.3" data-line="2" data-segment="0">8</span>
<span class="f1" data-bbox="106.3,704.1,413.0,4.5" data-line="3" data-segment="0">“Theoretical Limitations on the Rate of Transmission of Information,” Proceedings of the Institute of Radio Engineers, v. 37,</span>
<span class="f1" data-bbox="91.9,713.6,97.4,4.5" data-line="4" data-segment="0">No. 5, May, 1949, pp. 468–78.</span>
</p>
<p>
<span class="f4" data-bbox="300.6,744.5,10.0,5.6" data-line="0" data-segment="0">44</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                                                                                                                                                                        <p>
<span class="f4" data-bbox="91.9,99.9,14.5,5.6" data-line="0" data-segment="0">and</span>
</p>
<p>
<span class="f7" data-bbox="228.5,120.5,153.3,8.5" data-line="0" data-segment="0">C  W log 2 e(P + N1)  W log 2 eN1</span>
<span class="f7" data-bbox="272.6,134.6,26.7,5.6" data-line="1" data-segment="0">P + N1</span>
<span class="f9" data-bbox="237.5,141.4,66.2,5.6" data-line="2" data-segment="0">= W log :</span>
<span class="f7" data-bbox="280.9,148.3,10.1,5.6" data-line="3" data-segment="0">N1</span>
</p>
<p>
<span class="f4" data-bbox="106.9,166.1,391.3,5.6" data-line="0" data-segment="0">As P increases, the upper and lower bounds approach each other, so we have as an asymptotic rate</span>
</p>
<p>
<span class="f7" data-bbox="304.3,184.5,23.3,5.5" data-line="0" data-segment="0">P + N</span>
<span class="f7" data-bbox="278.6,191.2,53.5,5.6" data-line="1" data-segment="0">W log :</span>
<span class="f7" data-bbox="310.9,198.1,10.1,5.6" data-line="2" data-segment="0">N1</span>
</p>
<p>
<span class="f4" data-bbox="91.9,216.1,361.3,5.6" data-line="0" data-segment="0">If the noise is itself white, N = N1 and the result reduces to the formula proved previously:</span>
<span class="f19" data-bbox="306.5,230.0,37.1,14.9" data-line="1" data-segment="0">   </span>
<span class="f7" data-bbox="329.6,234.3,6.1,5.5" data-line="2" data-segment="0">P</span>
<span class="f7" data-bbox="264.6,241.0,81.6,5.6" data-line="3" data-segment="0">C = W log 1 + :</span>
<span class="f7" data-bbox="329.0,247.9,6.7,5.5" data-line="4" data-segment="0">N</span>
</p>
<p>
<span class="f4" data-bbox="106.9,264.5,412.5,5.6" data-line="0" data-segment="0">If the noise is Gaussian but with a spectrum which is not necessarily ﬂat, N1 is the geometric mean of</span>
<span class="f4" data-bbox="91.9,276.5,261.3,5.6" data-line="1" data-segment="0">the noise power over the various frequencies in the band W . Thus</span>
<span class="f33" data-bbox="300.8,289.3,5.2,10.0" data-line="2" data-segment="0">Z</span>
<span class="f4" data-bbox="291.5,295.0,5.0,5.6" data-line="3" data-segment="0">1</span>
<span class="f7" data-bbox="250.2,301.7,109.2,5.6" data-line="4" data-segment="0">N1 = exp log N( f )d f</span>
<span class="f7" data-bbox="289.0,308.6,22.7,5.5" data-line="5" data-segment="0">W W</span>
</p>
<p>
<span class="f4" data-bbox="91.9,326.8,184.4,5.6" data-line="0" data-segment="0">where N( f ) is the noise power at frequency f .</span>
<span class="f7" data-bbox="106.9,341.8,301.4,5.7" data-line="1" data-segment="0">Theorem 19: If we set the capacity for a given transmitter power P equal to</span>
</p>
<p>
<span class="f7" data-bbox="307.1,360.1,39.4,8.5" data-line="0" data-segment="0">P + N   </span>
<span class="f7" data-bbox="262.9,366.9,41.8,5.6" data-line="1" data-segment="0">C = W log</span>
<span class="f7" data-bbox="321.6,373.7,10.1,5.6" data-line="2" data-segment="0">N1</span>
</p>
<p>
<span class="f4" data-bbox="91.9,391.6,295.9,5.7" data-line="0" data-segment="0">then   is monotonic decreasing as P increases and approaches 0 as a limit.</span>
<span class="f4" data-bbox="106.9,406.6,228.3,5.6" data-line="1" data-segment="0">Suppose that for a given power P1 the channel capacity is</span>
</p>
<p>
<span class="f7" data-bbox="292.9,424.9,46.3,8.5" data-line="0" data-segment="0">P1 + N   1</span>
<span class="f7" data-bbox="267.2,431.6,76.3,5.6" data-line="1" data-segment="0">W log :</span>
<span class="f7" data-bbox="310.9,438.4,10.1,5.6" data-line="2" data-segment="0">N1</span>
</p>
<p>
<span class="f4" data-bbox="91.9,457.1,427.4,5.6" data-line="0" data-segment="0">This means that the best signal distribution, say p(x), when added to the noise distribution q(x), gives a</span>
<span class="f4" data-bbox="91.9,469.0,427.5,8.5" data-line="1" data-segment="0">received distribution r(y) whose entropy power is (P1 + N   1). Let us increase the power to P1 +  P by</span>
<span class="f4" data-bbox="91.9,481.0,388.6,5.6" data-line="2" data-segment="0">adding a white noise of power  P to the signal. The entropy of the received signal is now at least</span>
</p>
<p>
<span class="f7" data-bbox="230.2,501.7,151.1,8.5" data-line="0" data-segment="0">H(y) = W log 2 e(P1 + N   1 +  P)</span>
</p>
<p>
<span class="f4" data-bbox="91.9,522.3,427.3,5.6" data-line="0" data-segment="0">by application of the theorem on the minimum entropy power of a sum. Hence, since we can attain the</span>
<span class="f7" data-bbox="91.9,534.3,427.8,5.6" data-line="1" data-segment="0">H indicated, the entropy of the maximizing distribution must be at least as great and   must be monotonic</span>
<span class="f4" data-bbox="91.9,546.2,427.5,8.5" data-line="2" data-segment="0">decreasing. To show that   ! 0 as P ! ∞ consider a signal which is white noise with a large P. Whatever</span>
<span class="f4" data-bbox="91.9,558.2,427.4,5.6" data-line="3" data-segment="0">the perturbing noise, the received signal will be approximately a white noise, if P is sufﬁciently large, in the</span>
<span class="f4" data-bbox="91.9,570.2,214.3,5.6" data-line="4" data-segment="0">sense of having an entropy power approaching P + N.</span>
</p>
<p>
<span class="f4" data-bbox="167.4,590.9,276.5,5.6" data-line="0" data-segment="0">26. THE CHANNEL CAPACITY WITH A PEAK POWER LIMITATION</span>
</p>
<p>
<span class="f4" data-bbox="91.9,609.7,427.6,5.6" data-line="0" data-segment="0">In some applications the transmitter is limited not by the average power output but by the peak instantaneous</span>
<span class="f4" data-bbox="91.9,621.5,427.4,5.6" data-line="1" data-segment="0">power. The problem of calculating the channel capacity is then that of maximizing (by variation of the</span>
<span class="f4" data-bbox="91.9,633.5,135.0,5.6" data-line="2" data-segment="0">ensemble of transmitted symbols)</span>
<span class="f7" data-bbox="280.1,645.4,51.4,8.5" data-line="3" data-segment="0">H(y)  H(n)</span>
<span class="f16" data-bbox="458.0,654.3,8.3,8.5" data-line="4" data-segment="0">p</span>
<span class="f4" data-bbox="91.9,662.7,427.5,5.6" data-line="5" data-segment="0">subject to the constraint that all the functions f (t) in the ensemble be less than or equal to S, say, for all</span>
<span class="f7" data-bbox="91.7,674.6,427.6,5.6" data-line="6" data-segment="0">t. A constraint of this type does not work out as well mathematically as the average power limitation. The</span>
<span class="f7" data-bbox="362.0,684.4,5.0,5.5" data-line="7" data-segment="0">S</span>
<span class="f4" data-bbox="91.9,691.1,427.5,5.6" data-line="8" data-segment="0">most we have obtained for this case is a lower bound valid for all , an “asymptotic” upper bound (valid</span>
<span class="f7" data-bbox="361.0,698.0,6.7,5.5" data-line="9" data-segment="0">N</span>
<span class="f7" data-bbox="130.6,705.9,153.0,5.5" data-line="10" data-segment="0">S S</span>
<span class="f4" data-bbox="91.9,712.6,220.5,5.6" data-line="11" data-segment="0">for large ) and an asymptotic value of C for small.</span>
<span class="f7" data-bbox="129.5,719.5,154.6,5.5" data-line="12" data-segment="0">N N</span>
</p>
<p>
<span class="f4" data-bbox="300.6,744.5,10.0,5.6" data-line="0" data-segment="0">45</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                                                                                                                                                                                       <p>
<span class="f7" data-bbox="106.9,99.9,412.3,5.7" data-line="0" data-segment="0">Theorem 20: The channel capacity C for a band W perturbed by white thermal noise of power N is</span>
</p>
<p>
<span class="f4" data-bbox="91.9,111.9,46.9,5.7" data-line="0" data-segment="0">bounded by</span>
<span class="f4" data-bbox="318.2,121.7,18.2,5.6" data-line="1" data-segment="0">2 S</span>
<span class="f7" data-bbox="269.2,128.5,72.5,8.5" data-line="2" data-segment="0">C  W log ;</span>
<span class="f6" data-bbox="323.8,132.4,3.7,4.1" data-line="3" data-segment="0">3</span>
<span class="f10" data-bbox="313.3,135.3,23.7,5.5" data-line="4" data-segment="0"> e N</span>
</p>
<p>
<span class="f7" data-bbox="368.3,149.2,5.0,5.5" data-line="0" data-segment="0">S</span>
<span class="f4" data-bbox="91.9,155.9,271.8,5.7" data-line="1" data-segment="0">where S is the peak allowed transmitter power. For sufﬁciently large</span>
<span class="f7" data-bbox="367.2,162.8,6.7,5.5" data-line="2" data-segment="0">N</span>
</p>
<p>
<span class="f6" data-bbox="300.0,178.7,3.7,4.1" data-line="0" data-segment="0">2</span>
<span class="f7" data-bbox="307.1,182.7,22.4,5.5" data-line="1" data-segment="0">S + N</span>
<span class="f29" data-bbox="297.7,186.2,8.2,4.1" data-line="2" data-segment="0"> e</span>
<span class="f7" data-bbox="252.4,190.1,106.2,8.5" data-line="3" data-segment="0">C  W log (1 +  )</span>
<span class="f7" data-bbox="309.6,197.0,6.7,5.5" data-line="4" data-segment="0">N</span>
</p>
<p>
<span class="f7" data-bbox="219.7,214.7,5.0,5.5" data-line="0" data-segment="0">S</span>
<span class="f4" data-bbox="91.9,221.5,304.6,8.5" data-line="1" data-segment="0">where   is arbitrarily small. As ! 0 (and provided the band W starts at 0)</span>
<span class="f7" data-bbox="218.6,228.3,6.7,5.5" data-line="2" data-segment="0">N</span>
</p>
<p>
<span class="f19" data-bbox="293.9,239.8,39.7,14.9" data-line="0" data-segment="0">   </span>
<span class="f19" data-bbox="261.8,242.8,8.0,14.9" data-line="1" data-segment="0">.</span>
<span class="f7" data-bbox="319.0,247.1,5.0,5.5" data-line="2" data-segment="0">S</span>
<span class="f7" data-bbox="255.0,253.9,100.8,8.5" data-line="3" data-segment="0">C W log 1 + !1:</span>
<span class="f7" data-bbox="317.9,260.7,6.7,5.5" data-line="4" data-segment="0">N</span>
</p>
<p>
<span class="f7" data-bbox="348.0,281.0,5.0,5.5" data-line="0" data-segment="0">S</span>
<span class="f4" data-bbox="106.9,287.8,412.4,5.6" data-line="1" data-segment="0">We wish to maximize the entropy of the received signal. If is large this will occur very nearly when</span>
<span class="f7" data-bbox="346.9,294.5,6.7,5.5" data-line="2" data-segment="0">N</span>
<span class="f4" data-bbox="91.9,302.6,216.4,5.6" data-line="3" data-segment="0">we maximize the entropy of the transmitted ensemble.</span>
</p>
<p>
<span class="f4" data-bbox="106.9,314.5,412.5,5.6" data-line="0" data-segment="0">The asymptotic upper bound is obtained by relaxing the conditions on the ensemble. Let us suppose that</span>
</p>
<p>
<span class="f4" data-bbox="91.9,326.5,427.5,5.6" data-line="0" data-segment="0">the power is limited to S not at every instant of time, but only at the sample points. The maximum entropy of</span>
</p>
<p>
<span class="f4" data-bbox="91.9,338.5,427.2,5.6" data-line="0" data-segment="0">the transmitted ensemble under these weakened conditions is certainly greater than or equal to that under the</span>
</p>
<p>
<span class="f4" data-bbox="91.9,350.3,427.5,5.6" data-line="0" data-segment="0">original conditions. This altered problem can be solved easily. The maximum entropy occurs if the different</span>
<span class="f16" data-bbox="419.2,354.1,41.6,8.5" data-line="1" data-segment="0">p p</span>
<span class="f4" data-bbox="91.9,362.3,427.6,8.5" data-line="2" data-segment="0">samples are independent and have a distribution function which is constant from   S to + S. The entropy</span>
</p>
<p>
<span class="f4" data-bbox="91.9,374.3,79.3,5.6" data-line="0" data-segment="0">can be calculated as</span>
</p>
<p>
<span class="f7" data-bbox="286.8,386.2,37.3,5.6" data-line="0" data-segment="0">W log 4S:</span>
</p>
<p>
<span class="f4" data-bbox="91.9,404.2,218.2,5.6" data-line="0" data-segment="0">The received signal will then have an entropy less than</span>
</p>
<p>
<span class="f7" data-bbox="254.6,426.1,101.9,5.6" data-line="0" data-segment="0">W log(4S + 2 eN)(1 +  )</span>
</p>
<p>
<span class="f7" data-bbox="153.2,446.2,5.0,5.5" data-line="0" data-segment="0">S</span>
<span class="f4" data-bbox="91.9,452.9,427.7,8.5" data-line="1" data-segment="0">with   ! 0 as ! ∞ and the channel capacity is obtained by subtracting the entropy of the white noise,</span>
<span class="f7" data-bbox="152.2,459.8,6.7,5.5" data-line="2" data-segment="0">N</span>
<span class="f7" data-bbox="91.4,467.7,49.9,5.6" data-line="3" data-segment="0">W log 2 eN:</span>
<span class="f6" data-bbox="377.9,475.9,3.7,4.1" data-line="4" data-segment="0">2</span>
<span class="f7" data-bbox="385.0,479.8,22.4,5.5" data-line="5" data-segment="0">S + N</span>
<span class="f29" data-bbox="375.6,483.2,8.2,4.1" data-line="6" data-segment="0"> e</span>
<span class="f7" data-bbox="171.7,487.1,267.5,8.5" data-line="7" data-segment="0">W log(4S + 2 eN)(1 +  )  W log(2 eN) = W log (1 +  ):</span>
<span class="f7" data-bbox="387.5,494.0,6.7,5.5" data-line="8" data-segment="0">N</span>
</p>
<p>
<span class="f4" data-bbox="91.9,507.9,221.4,5.6" data-line="0" data-segment="0">This is the desired upper bound to the channel capacity.</span>
</p>
<p>
<span class="f4" data-bbox="106.9,519.9,411.8,5.6" data-line="0" data-segment="0">To obtain a lower bound consider the same ensemble of functions. Let these functions be passed through</span>
</p>
<p>
<span class="f4" data-bbox="91.9,531.9,427.1,5.6" data-line="0" data-segment="0">an ideal ﬁlter with a triangular transfer characteristic. The gain is to be unity at frequency 0 and decline</span>
</p>
<p>
<span class="f4" data-bbox="91.9,543.8,427.5,5.6" data-line="0" data-segment="0">linearly down to gain 0 at frequency W . We ﬁrst show that the output functions of the ﬁlter have a peak</span>
<span class="f4" data-bbox="438.2,553.7,35.4,5.6" data-line="1" data-segment="0">sin 2 W t</span>
<span class="f4" data-bbox="91.9,560.5,427.5,5.6" data-line="2" data-segment="0">power limitation S at all times (not just the sample points). First we note that a pulse going into</span>
<span class="f4" data-bbox="444.6,567.3,22.6,5.6" data-line="3" data-segment="0">2 W t</span>
<span class="f4" data-bbox="91.9,575.3,72.2,5.6" data-line="4" data-segment="0">the ﬁlter produces</span>
<span class="f6" data-bbox="303.4,583.4,3.7,4.1" data-line="5" data-segment="0">2</span>
<span class="f4" data-bbox="284.4,587.3,41.9,5.6" data-line="6" data-segment="0">1 sin  W t</span>
<span class="f6" data-bbox="320.3,598.0,3.7,4.1" data-line="7" data-segment="0">2</span>
<span class="f4" data-bbox="284.4,600.9,35.9,5.6" data-line="8" data-segment="0">2 ( W t)</span>
</p>
<p>
<span class="f4" data-bbox="91.9,617.2,427.6,5.6" data-line="0" data-segment="0">in the output. This function is never negative. The input function (in the general case) can be thought of as</span>
</p>
<p>
<span class="f4" data-bbox="91.9,629.1,154.4,5.6" data-line="0" data-segment="0">the sum of a series of shifted functions</span>
<span class="f4" data-bbox="290.2,637.0,35.3,5.6" data-line="1" data-segment="0">sin 2 W t</span>
<span class="f7" data-bbox="283.9,643.7,5.0,5.5" data-line="2" data-segment="0">a</span>
<span class="f4" data-bbox="296.5,650.6,22.6,5.6" data-line="3" data-segment="0">2 W t</span>
<span class="f16" data-bbox="313.4,658.5,8.3,8.5" data-line="4" data-segment="0">p</span>
<span class="f4" data-bbox="91.9,666.8,427.7,5.6" data-line="5" data-segment="0">where a, the amplitude of the sample, is not greater than S. Hence the output is the sum of shifted functions</span>
</p>
<p>
<span class="f4" data-bbox="91.9,678.8,428.5,5.6" data-line="0" data-segment="0">of the non-negative form above with the same coefﬁcients. These functions being non-negative, the greatest</span>
<span class="f16" data-bbox="503.5,682.5,8.3,8.5" data-line="1" data-segment="0">p</span>
<span class="f4" data-bbox="91.9,690.8,427.4,5.6" data-line="2" data-segment="0">positive value for any t is obtained when all the coefﬁcients a have their maximum positive values, i.e., S.</span>
<span class="f16" data-bbox="325.2,694.4,8.3,8.5" data-line="3" data-segment="0">p</span>
<span class="f4" data-bbox="91.9,702.7,427.4,5.6" data-line="4" data-segment="0">In this case the input function was a constant of amplitude S and since the ﬁlter has unit gain for D.C., the</span>
</p>
<p>
<span class="f4" data-bbox="91.9,714.7,267.2,5.6" data-line="0" data-segment="0">output is the same. Hence the output ensemble has a peak power S.</span>
</p>
<p>
<span class="f4" data-bbox="300.6,744.5,10.0,5.6" data-line="0" data-segment="0">46</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                                                                                                                                                                                <p>
<span class="f4" data-bbox="106.9,99.9,412.4,5.6" data-line="0" data-segment="0">The entropy of the output ensemble can be calculated from that of the input ensemble by using the</span>
</p>
<p>
<span class="f4" data-bbox="91.9,111.9,428.9,5.6" data-line="0" data-segment="0">theorem dealing with such a situation. The output entropy is equal to the input entropy plus the geometrical</span>
</p>
<p>
<span class="f4" data-bbox="91.9,123.9,90.8,5.6" data-line="0" data-segment="0">mean gain of the ﬁlter:</span>
<span class="f33" data-bbox="211.0,128.9,137.0,16.4" data-line="1" data-segment="0">Z Z    </span>
<span class="f8" data-bbox="219.8,131.8,131.9,5.1" data-line="2" data-segment="0">W W 2</span>
<span class="f7" data-bbox="315.1,134.7,24.3,8.5" data-line="3" data-segment="0">W   f</span>
<span class="f6" data-bbox="249.5,137.3,3.7,4.1" data-line="4" data-segment="0">2</span>
<span class="f4" data-bbox="228.5,141.4,172.0,8.5" data-line="5" data-segment="0">log G d f = log d f =  2W:</span>
<span class="f6" data-bbox="216.1,148.3,115.4,5.5" data-line="6" data-segment="0">0 0 W</span>
</p>
<p>
<span class="f4" data-bbox="91.9,163.1,110.1,5.6" data-line="0" data-segment="0">Hence the output entropy is</span>
<span class="f4" data-bbox="347.9,173.0,10.0,5.6" data-line="1" data-segment="0">4S</span>
<span class="f7" data-bbox="251.6,179.7,94.0,8.5" data-line="2" data-segment="0">W log 4S  2W = W log</span>
<span class="f6" data-bbox="353.0,183.7,3.7,4.1" data-line="3" data-segment="0">2</span>
<span class="f7" data-bbox="348.6,186.5,4.4,5.5" data-line="4" data-segment="0">e</span>
</p>
<p>
<span class="f4" data-bbox="91.9,200.3,157.3,5.6" data-line="0" data-segment="0">and the channel capacity is greater than</span>
<span class="f4" data-bbox="308.9,210.2,18.3,5.6" data-line="1" data-segment="0">2 S</span>
<span class="f7" data-bbox="278.4,216.9,53.9,5.6" data-line="2" data-segment="0">W log :</span>
<span class="f6" data-bbox="314.5,220.9,3.7,4.1" data-line="3" data-segment="0">3</span>
<span class="f10" data-bbox="304.1,223.7,23.7,5.5" data-line="4" data-segment="0"> e N</span>
</p>
<p>
<span class="f7" data-bbox="254.6,237.5,5.0,5.5" data-line="0" data-segment="0">S</span>
<span class="f4" data-bbox="106.9,244.3,412.5,5.6" data-line="1" data-segment="0">We now wish to show that, for small (peak signal power over average white noise power), the channel</span>
<span class="f7" data-bbox="253.6,251.1,6.7,5.5" data-line="2" data-segment="0">N</span>
<span class="f4" data-bbox="91.9,259.1,102.7,5.6" data-line="3" data-segment="0">capacity is approximately</span>
<span class="f19" data-bbox="305.0,262.6,39.8,14.9" data-line="4" data-segment="0">   </span>
<span class="f7" data-bbox="330.1,269.9,5.0,5.5" data-line="5" data-segment="0">S</span>
<span class="f7" data-bbox="263.2,276.7,84.5,5.6" data-line="6" data-segment="0">C = W log 1 + :</span>
<span class="f7" data-bbox="329.0,283.5,6.7,5.5" data-line="7" data-segment="0">N</span>
<span class="f19" data-bbox="193.8,293.5,40.0,14.9" data-line="8" data-segment="0">   </span>
<span class="f19" data-bbox="161.5,296.5,8.0,14.9" data-line="9" data-segment="0">.</span>
<span class="f7" data-bbox="219.1,300.8,55.6,5.5" data-line="10" data-segment="0">S S</span>
<span class="f4" data-bbox="91.9,307.5,427.5,8.5" data-line="11" data-segment="0">More precisely C W log 1 + ! 1 as ! 0. Since the average signal power P is less than or equal</span>
<span class="f7" data-bbox="218.0,314.3,57.3,5.5" data-line="12" data-segment="0">N N</span>
<span class="f7" data-bbox="234.6,324.7,5.0,5.5" data-line="13" data-segment="0">S</span>
<span class="f4" data-bbox="91.9,331.5,137.9,5.6" data-line="14" data-segment="0">to the peak S, it follows that for all</span>
<span class="f7" data-bbox="233.5,338.3,6.7,5.5" data-line="15" data-segment="0">N</span>
</p>
<p>
<span class="f19" data-bbox="267.6,349.7,114.6,14.9" data-line="0" data-segment="0">       </span>
<span class="f7" data-bbox="292.2,357.1,80.4,5.5" data-line="1" data-segment="0">P S</span>
<span class="f7" data-bbox="225.7,363.8,159.2,8.5" data-line="2" data-segment="0">C  W log 1 +  W log 1 + :</span>
<span class="f7" data-bbox="291.6,370.6,81.5,5.5" data-line="3" data-segment="0">N N</span>
</p>
<p>
<span class="f19" data-bbox="480.0,384.5,39.4,14.9" data-line="0" data-segment="0">   </span>
<span class="f7" data-bbox="504.6,391.9,5.0,5.5" data-line="1" data-segment="0">S</span>
<span class="f4" data-bbox="91.9,398.6,409.4,5.6" data-line="2" data-segment="0">Therefore, if we can ﬁnd an ensemble of functions such that they correspond to a rate nearly W log 1 +</span>
<span class="f7" data-bbox="503.5,405.4,6.7,5.5" data-line="3" data-segment="0">N</span>
<span class="f4" data-bbox="91.9,415.9,427.4,5.6" data-line="4" data-segment="0">and are limited to band W and peak S the result will be proved. Consider the ensemble of functions of the</span>
<span class="f16" data-bbox="356.3,419.6,42.1,8.5" data-line="5" data-segment="0">p p</span>
<span class="f4" data-bbox="91.9,427.9,427.5,8.5" data-line="6" data-segment="0">following type. A series of t samples have the same value, either + S or   S, then the next t samples have</span>
<span class="f16" data-bbox="428.3,431.5,72.8,8.5" data-line="7" data-segment="0">p p</span>
<span class="f6" data-bbox="398.4,435.9,68.1,4.1" data-line="8" data-segment="0">1 1</span>
<span class="f4" data-bbox="91.9,439.7,427.4,8.5" data-line="9" data-segment="0">the same value, etc. The value for a series is chosen at random, probability for + S and for   S. If</span>
<span class="f6" data-bbox="398.4,443.2,68.1,4.1" data-line="10" data-segment="0">2 2</span>
<span class="f4" data-bbox="91.9,451.7,427.3,5.6" data-line="11" data-segment="0">this ensemble be passed through a ﬁlter with triangular gain characteristic (unit gain at D.C.), the output is</span>
</p>
<p>
<span class="f4" data-bbox="91.9,463.7,426.7,8.5" data-line="0" data-segment="0">peak limited to  S. Furthermore the average power is nearly S and can be made to approach this by taking t</span>
</p>
<p>
<span class="f4" data-bbox="91.9,475.6,427.7,5.6" data-line="0" data-segment="0">sufﬁciently large. The entropy of the sum of this and the thermal noise can be found by applying the theorem</span>
</p>
<p>
<span class="f4" data-bbox="91.9,487.6,268.8,5.6" data-line="0" data-segment="0">on the sum of a noise and a small signal. This theorem will apply if</span>
</p>
<p>
<span class="f16" data-bbox="295.1,505.9,18.8,8.5" data-line="0" data-segment="0">p S</span>
<span class="f7" data-bbox="303.1,514.1,2.8,5.5" data-line="1" data-segment="0">t</span>
<span class="f7" data-bbox="307.8,520.9,6.7,5.5" data-line="2" data-segment="0">N</span>
</p>
<p>
<span class="f7" data-bbox="301.0,538.6,5.0,5.5" data-line="0" data-segment="0">S</span>
<span class="f4" data-bbox="91.9,545.5,427.5,5.6" data-line="1" data-segment="0">is sufﬁciently small. This can be ensured by taking small enough (after t is chosen). The entropy power</span>
<span class="f7" data-bbox="299.9,552.3,6.7,5.5" data-line="2" data-segment="0">N</span>
<span class="f4" data-bbox="91.9,560.2,427.3,5.6" data-line="3" data-segment="0">will be S +N to as close an approximation as desired, and hence the rate of transmission as near as we wish</span>
</p>
<p>
<span class="f4" data-bbox="91.9,572.2,243.7,16.4" data-line="0" data-segment="0">to    </span>
<span class="f7" data-bbox="304.3,581.0,22.4,5.5" data-line="1" data-segment="0">S + N</span>
<span class="f7" data-bbox="272.4,587.7,66.0,5.6" data-line="2" data-segment="0">W log :</span>
<span class="f7" data-bbox="312.0,594.5,6.7,5.5" data-line="3" data-segment="0">N</span>
</p>
<p>
<span class="f13" data-bbox="170.3,624.2,270.5,6.7" data-line="0" data-segment="0">PART V: THE RATE FOR A CONTINUOUS SOURCE</span>
</p>
<p>
<span class="f4" data-bbox="220.6,648.2,170.1,5.6" data-line="0" data-segment="0">27. FIDELITY EVALUATION FUNCTIONS</span>
</p>
<p>
<span class="f4" data-bbox="91.9,666.8,427.5,5.6" data-line="0" data-segment="0">In the case of a discrete source of information we were able to determine a deﬁnite rate of generating</span>
</p>
<p>
<span class="f4" data-bbox="91.9,678.8,426.6,5.6" data-line="0" data-segment="0">information, namely the entropy of the underlying stochastic process. With a continuous source the situation</span>
</p>
<p>
<span class="f4" data-bbox="91.9,690.8,427.4,5.6" data-line="0" data-segment="0">is considerably more involved. In the ﬁrst place a continuously variable quantity can assume an inﬁnite</span>
</p>
<p>
<span class="f4" data-bbox="91.9,702.7,427.4,5.6" data-line="0" data-segment="0">number of values and requires, therefore, an inﬁnite number of binary digits for exact speciﬁcation. This</span>
</p>
<p>
<span class="f4" data-bbox="91.9,714.7,427.6,5.6" data-line="0" data-segment="0">means that to transmit the output of a continuous source with exact recovery at the receiving point requires,</span>
</p>
<p>
<span class="f4" data-bbox="300.6,744.5,10.0,5.6" data-line="0" data-segment="0">47</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          <p>
<span class="f4" data-bbox="91.9,99.9,427.4,5.6" data-line="0" data-segment="0">in general, a channel of inﬁnite capacity (in bits per second). Since, ordinarily, channels have a certain</span>
<span class="f4" data-bbox="91.9,111.9,322.6,5.6" data-line="1" data-segment="0">amount of noise, and therefore a ﬁnite capacity, exact transmission is impossible.</span>
<span class="f4" data-bbox="106.9,123.9,412.5,5.6" data-line="2" data-segment="0">This, however, evades the real issue. Practically, we are not interested in exact transmission when we</span>
<span class="f4" data-bbox="91.9,135.8,427.3,5.6" data-line="3" data-segment="0">have a continuous source, but only in transmission to within a certain tolerance. The question is, can we</span>
<span class="f4" data-bbox="91.9,147.8,427.4,5.6" data-line="4" data-segment="0">assign a deﬁnite rate to a continuous source when we require only a certain ﬁdelity of recovery, measured in</span>
<span class="f4" data-bbox="91.9,159.8,427.4,5.6" data-line="5" data-segment="0">a suitable way. Of course, as the ﬁdelity requirements are increased the rate will increase. It will be shown</span>
<span class="f4" data-bbox="91.9,171.7,427.7,5.6" data-line="6" data-segment="0">that we can, in very general cases, deﬁne such a rate, having the property that it is possible, by properly</span>
<span class="f4" data-bbox="91.9,183.7,427.7,5.6" data-line="7" data-segment="0">encoding the information, to transmit it over a channel whose capacity is equal to the rate in question, and</span>
<span class="f4" data-bbox="91.9,195.5,309.3,5.6" data-line="8" data-segment="0">satisfy the ﬁdelity requirements. A channel of smaller capacity is insufﬁcient.</span>
<span class="f4" data-bbox="106.9,207.5,412.4,5.6" data-line="9" data-segment="0">It is ﬁrst necessary to give a general mathematical formulation of the idea of ﬁdelity of transmission.</span>
<span class="f4" data-bbox="91.9,219.5,427.5,5.6" data-line="10" data-segment="0">Consider the set of messages of a long duration, say T seconds. The source is described by giving the</span>
<span class="f4" data-bbox="91.9,231.4,427.6,5.6" data-line="11" data-segment="0">probability density, in the associated space, that the source will select the message in question P(x). A given</span>
<span class="f4" data-bbox="91.9,243.4,427.9,5.6" data-line="12" data-segment="0">communication system is described (from the external point of view) by giving the conditional probability</span>
<span class="f7" data-bbox="91.9,255.4,427.5,5.6" data-line="13" data-segment="0">Px(y) that if message x is produced by the source the recovered message at the receiving point will be y. The</span>
<span class="f4" data-bbox="91.9,267.3,427.3,5.6" data-line="14" data-segment="0">system as a whole (including source and transmission system) is described by the probability function P(x;y)</span>
<span class="f4" data-bbox="91.9,279.3,427.4,5.6" data-line="15" data-segment="0">of having message x and ﬁnal output y. If this function is known, the complete characteristics of the system</span>
<span class="f4" data-bbox="91.9,291.2,427.5,5.6" data-line="16" data-segment="0">from the point of view of ﬁdelity are known. Any evaluation of ﬁdelity must correspond mathematically</span>
<span class="f4" data-bbox="91.9,303.2,427.5,5.6" data-line="17" data-segment="0">to an operation applied to P(x;y). This operation must at least have the properties of a simple ordering of</span>
<span class="f4" data-bbox="91.9,315.2,427.4,5.6" data-line="18" data-segment="0">systems; i.e., it must be possible to say of two systems represented by P1(x;y) and P2(x;y) that, according to</span>
<span class="f4" data-bbox="91.9,327.1,427.6,5.6" data-line="19" data-segment="0">our ﬁdelity criterion, either (1) the ﬁrst has higher ﬁdelity, (2) the second has higher ﬁdelity, or (3) they have</span>
<span class="f4" data-bbox="91.9,339.1,418.4,5.6" data-line="20" data-segment="0">equal ﬁdelity. This means that a criterion of ﬁdelity can be represented by a numerically valued function:</span>
<span class="f19" data-bbox="290.0,350.0,35.6,14.9" data-line="21" data-segment="0">   </span>
<span class="f7" data-bbox="285.6,358.0,35.5,5.5" data-line="22" data-segment="0">v P(x;y)</span>
</p>
<p>
<span class="f4" data-bbox="91.9,377.1,266.2,5.6" data-line="0" data-segment="0">whose argument ranges over possible probability functions P(x;y).</span>
<span class="f19" data-bbox="454.2,380.9,35.6,14.9" data-line="1" data-segment="0">   </span>
<span class="f4" data-bbox="106.9,389.1,412.5,5.6" data-line="2" data-segment="0">We will now show that under very general and reasonable assumptions the function v P(x;y) can be</span>
<span class="f4" data-bbox="91.9,401.0,427.3,5.6" data-line="3" data-segment="0">written in a seemingly much more specialized form, namely as an average of a function  (x;y) over the set</span>
<span class="f4" data-bbox="91.9,413.0,115.5,5.6" data-line="4" data-segment="0">of possible values of x and y:</span>
<span class="f33" data-bbox="285.7,416.1,10.4,10.0" data-line="5" data-segment="0">ZZ</span>
<span class="f19" data-bbox="237.8,420.5,35.6,14.9" data-line="6" data-segment="0">   </span>
<span class="f7" data-bbox="233.4,428.6,144.2,5.5" data-line="7" data-segment="0">v P(x;y) = P(x;y) (x;y)dx dy:</span>
</p>
<p>
<span class="f4" data-bbox="91.9,448.0,427.5,5.6" data-line="0" data-segment="0">To obtain this we need only assume (1) that the source and system are ergodic so that a very long sample</span>
<span class="f4" data-bbox="91.9,460.0,427.4,5.6" data-line="1" data-segment="0">will be, with probability nearly 1, typical of the ensemble, and (2) that the evaluation is “reasonable” in the</span>
<span class="f4" data-bbox="91.9,472.0,427.6,5.6" data-line="2" data-segment="0">sense that it is possible, by observing a typical input and output x1 and y1, to form a tentative evaluation</span>
<span class="f4" data-bbox="91.9,483.9,427.4,5.6" data-line="3" data-segment="0">on the basis of these samples; and if these samples are increased in duration the tentative evaluation will,</span>
<span class="f4" data-bbox="91.9,495.9,427.5,5.6" data-line="4" data-segment="0">with probability 1, approach the exact evaluation based on a full knowledge of P(x;y). Let the tentative</span>
<span class="f4" data-bbox="91.9,507.8,427.5,8.5" data-line="5" data-segment="0">evaluation be  (x;y). Then the function  (x;y) approaches (as T !∞) a constant for almost all (x;y) which</span>
<span class="f4" data-bbox="91.9,519.8,248.7,5.6" data-line="6" data-segment="0">are in the high probability region corresponding to the system:</span>
<span class="f19" data-bbox="310.0,530.7,35.6,14.9" data-line="7" data-segment="0">   </span>
<span class="f10" data-bbox="265.6,538.9,75.5,8.5" data-line="8" data-segment="0"> (x;y) !v P(x;y)</span>
</p>
<p>
<span class="f4" data-bbox="91.9,557.8,89.6,5.6" data-line="0" data-segment="0">and we may also write</span>
<span class="f33" data-bbox="280.7,560.9,10.4,10.0" data-line="1" data-segment="0">ZZ</span>
<span class="f10" data-bbox="240.7,573.4,129.7,8.5" data-line="2" data-segment="0"> (x;y) ! P(x;y) (x;y)dx dy</span>
</p>
<p>
<span class="f4" data-bbox="91.9,593.0,182.0,11.0" data-line="0" data-segment="0">since ZZ</span>
<span class="f7" data-bbox="279.1,606.5,68.6,5.6" data-line="1" data-segment="0">P(x;y)dx dy = 1:</span>
</p>
<p>
<span class="f4" data-bbox="91.9,626.0,136.5,5.6" data-line="0" data-segment="0">This establishes the desired result.</span>
<span class="f6" data-bbox="400.2,634.4,3.7,4.1" data-line="1" data-segment="0">9</span>
<span class="f4" data-bbox="106.9,638.0,412.6,5.6" data-line="2" data-segment="0">The function  (x;y) has the general nature of a “distance” between x and y. It measures how undesirable</span>
<span class="f4" data-bbox="91.9,649.9,427.5,5.6" data-line="3" data-segment="0">it is (according to our ﬁdelity criterion) to receive y when x is transmitted. The general result given above</span>
<span class="f4" data-bbox="91.9,661.9,427.9,5.6" data-line="4" data-segment="0">can be restated as follows: Any reasonable evaluation can be represented as an average of a distance function</span>
<span class="f4" data-bbox="91.9,673.9,427.5,5.6" data-line="5" data-segment="0">over the set of messages and recovered messages x and y weighted according to the probability P(x;y) of</span>
<span class="f4" data-bbox="91.9,685.7,380.2,5.6" data-line="6" data-segment="0">getting the pair in question, provided the duration T of the messages be taken sufﬁciently large.</span>
<span class="f4" data-bbox="106.9,697.7,237.0,5.6" data-line="7" data-segment="0">The following are simple examples of evaluation functions:</span>
<span class="f11" data-bbox="102.8,711.8,3.0,3.3" data-line="8" data-segment="0">9</span>
<span class="f1" data-bbox="106.3,714.7,412.9,6.4" data-line="9" data-segment="0">It is not a “metric” in the strict sense, however, since in general it does not satisfy either  (x; y) = (y; x) or  (x; y)+ (y; z)   (x; z).</span>
</p>
<p>
<span class="f4" data-bbox="300.6,744.5,10.0,5.6" data-line="0" data-segment="0">48</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                                                                                                                                                      <p>
<span class="f4" data-bbox="104.4,99.9,79.9,5.6" data-line="0" data-segment="0">1. R.M.S. criterion.</span>
<span class="f19" data-bbox="297.7,105.5,50.5,14.9" data-line="1" data-segment="0">   </span>
<span class="f6" data-bbox="348.1,107.9,3.7,4.1" data-line="2" data-segment="0">2</span>
<span class="f7" data-bbox="281.2,113.6,73.9,8.5" data-line="3" data-segment="0">v = x(t)  y(t) :</span>
</p>
<p>
<span class="f4" data-bbox="116.9,130.9,402.6,5.6" data-line="0" data-segment="0">In this very commonly used measure of ﬁdelity the distance function  (x;y) is (apart from a constant</span>
<span class="f4" data-bbox="116.9,142.9,402.4,5.6" data-line="1" data-segment="0">factor) the square of the ordinary Euclidean distance between the points x and y in the associated</span>
<span class="f4" data-bbox="116.9,154.9,60.3,5.6" data-line="2" data-segment="0">function space.</span>
<span class="f33" data-bbox="301.7,159.9,5.2,10.0" data-line="3" data-segment="0">Z</span>
<span class="f8" data-bbox="310.8,162.8,4.1,4.1" data-line="4" data-segment="0">T</span>
<span class="f4" data-bbox="293.4,164.3,76.2,14.9" data-line="5" data-segment="0">1    2</span>
<span class="f10" data-bbox="253.6,172.4,129.1,8.5" data-line="6" data-segment="0"> (x;y) = x(t)  y(t) dt:</span>
<span class="f7" data-bbox="292.6,179.2,17.9,5.5" data-line="7" data-segment="0">T 0</span>
</p>
<p>
<span class="f4" data-bbox="104.4,197.2,415.0,5.6" data-line="0" data-segment="0">2. Frequency weighted R.M.S. criterion. More generally one can apply different weights to the different</span>
<span class="f4" data-bbox="116.9,209.1,402.6,5.6" data-line="1" data-segment="0">frequency components before using an R.M.S. measure of ﬁdelity. This is equivalent to passing the</span>
<span class="f4" data-bbox="116.9,221.1,402.7,8.5" data-line="2" data-segment="0">difference x(t)  y(t) through a shaping ﬁlter and then determining the average power in the output.</span>
<span class="f4" data-bbox="116.9,233.1,32.3,5.6" data-line="3" data-segment="0">Thus let</span>
<span class="f7" data-bbox="283.7,245.0,68.9,8.5" data-line="4" data-segment="0">e(t) = x(t)  y(t)</span>
</p>
<p>
<span class="f4" data-bbox="116.9,262.3,181.3,11.3" data-line="0" data-segment="0">and Z</span>
<span class="f15" data-bbox="302.2,266.5,5.3,4.8" data-line="1" data-segment="0">∞</span>
<span class="f7" data-bbox="265.7,276.1,105.1,8.5" data-line="2" data-segment="0">f (t) = e( )k(t   )d </span>
<span class="f14" data-bbox="298.1,283.9,11.0,4.8" data-line="3" data-segment="0"> ∞</span>
</p>
<p>
<span class="f4" data-bbox="116.9,297.7,17.2,5.6" data-line="0" data-segment="0">then</span>
<span class="f33" data-bbox="317.5,300.7,5.2,10.0" data-line="1" data-segment="0">Z</span>
<span class="f8" data-bbox="326.8,303.5,4.1,4.1" data-line="2" data-segment="0">T</span>
<span class="f4" data-bbox="309.4,306.4,5.0,5.6" data-line="3" data-segment="0">1</span>
<span class="f6" data-bbox="349.9,308.9,3.7,4.1" data-line="4" data-segment="0">2</span>
<span class="f10" data-bbox="269.4,313.1,97.4,5.5" data-line="5" data-segment="0"> (x;y) = f (t) dt:</span>
<span class="f7" data-bbox="308.4,320.0,18.0,5.5" data-line="6" data-segment="0">T 0</span>
</p>
<p>
<span class="f4" data-bbox="104.4,338.0,109.1,5.6" data-line="0" data-segment="0">3. Absolute error criterion.</span>
<span class="f33" data-bbox="304.6,341.0,5.2,10.0" data-line="1" data-segment="0">Z</span>
<span class="f8" data-bbox="313.7,343.9,4.1,4.1" data-line="2" data-segment="0">T</span>
<span class="f4" data-bbox="296.4,344.9,70.9,14.9" data-line="3" data-segment="0">1    </span>
<span class="f19" data-bbox="319.3,350.9,48.0,14.9" data-line="4" data-segment="0">   </span>
<span class="f10" data-bbox="256.4,353.5,123.4,8.5" data-line="5" data-segment="0"> (x;y) = x(t) y(t) dt:</span>
<span class="f7" data-bbox="295.4,360.3,17.9,5.5" data-line="6" data-segment="0">T 0</span>
</p>
<p>
<span class="f4" data-bbox="104.4,378.3,415.3,5.6" data-line="0" data-segment="0">4. The structure of the ear and brain determine implicitly an evaluation, or rather a number of evaluations,</span>
<span class="f4" data-bbox="116.9,390.3,402.2,5.6" data-line="1" data-segment="0">appropriate in the case of speech or music transmission. There is, for example, an “intelligibility”</span>
<span class="f4" data-bbox="116.9,402.2,402.4,5.6" data-line="2" data-segment="0">criterion in which  (x;y) is equal to the relative frequency of incorrectly interpreted words when</span>
<span class="f4" data-bbox="116.9,414.2,402.5,5.6" data-line="3" data-segment="0">message x(t) is received as y(t). Although we cannot give an explicit representation of  (x;y) in these</span>
<span class="f4" data-bbox="116.9,426.1,402.4,5.6" data-line="4" data-segment="0">cases it could, in principle, be determined by sufﬁcient experimentation. Some of its properties follow</span>
<span class="f4" data-bbox="116.9,438.1,402.2,5.6" data-line="5" data-segment="0">from well-known experimental results in hearing, e.g., the ear is relatively insensitive to phase and the</span>
<span class="f4" data-bbox="116.9,450.1,246.1,5.6" data-line="6" data-segment="0">sensitivity to amplitude and frequency is roughly logarithmic.</span>
</p>
<p>
<span class="f4" data-bbox="104.4,469.5,415.1,5.6" data-line="0" data-segment="0">5. The discrete case can be considered as a specialization in which we have tacitly assumed an evaluation</span>
<span class="f4" data-bbox="116.9,481.5,402.4,5.6" data-line="1" data-segment="0">based on the frequency of errors. The function  (x;y) is then deﬁned as the number of symbols in the</span>
<span class="f4" data-bbox="116.9,493.4,402.4,5.6" data-line="2" data-segment="0">sequence y differing from the corresponding symbols in x divided by the total number of symbols in</span>
<span class="f7" data-bbox="116.9,505.4,6.9,5.6" data-line="3" data-segment="0">x.</span>
</p>
<p>
<span class="f4" data-bbox="156.4,526.3,298.6,5.6" data-line="0" data-segment="0">28. THE RATE FOR A SOURCE RELATIVE TO A FIDELITY EVALUATION</span>
</p>
<p>
<span class="f4" data-bbox="91.9,544.9,427.4,5.6" data-line="0" data-segment="0">We are now in a position to deﬁne a rate of generating information for a continuous source. We are given</span>
<span class="f7" data-bbox="91.9,556.9,427.5,5.6" data-line="1" data-segment="0">P(x) for the source and an evaluation v determined by a distance function  (x;y) which will be assumed</span>
<span class="f4" data-bbox="91.9,568.9,344.0,5.6" data-line="2" data-segment="0">continuous in both x and y. With a particular system P(x;y) the quality is measured by</span>
<span class="f33" data-bbox="267.8,581.2,10.6,10.0" data-line="3" data-segment="0">ZZ</span>
<span class="f7" data-bbox="251.3,593.6,108.5,5.5" data-line="4" data-segment="0">v =  (x;y)P(x;y)dx dy:</span>
</p>
<p>
<span class="f4" data-bbox="91.9,618.4,284.0,5.6" data-line="0" data-segment="0">Furthermore the rate of ﬂow of binary digits corresponding to P(x;y) is</span>
</p>
<p>
<span class="f33" data-bbox="254.6,632.6,10.4,10.0" data-line="0" data-segment="0">ZZ</span>
<span class="f7" data-bbox="317.9,638.3,26.5,5.5" data-line="1" data-segment="0">P(x;y)</span>
<span class="f7" data-bbox="236.3,645.1,138.6,5.6" data-line="2" data-segment="0">R = P(x;y)log dx dy:</span>
<span class="f7" data-bbox="313.0,651.9,36.5,5.5" data-line="3" data-segment="0">P(x)P(y)</span>
</p>
<p>
<span class="f4" data-bbox="91.9,671.1,427.5,5.6" data-line="0" data-segment="0">We deﬁne the rate R1 of generating information for a given quality v1 of reproduction to be the minimum of</span>
<span class="f7" data-bbox="91.9,683.0,211.5,5.6" data-line="1" data-segment="0">R when we keep v ﬁxed at v1 and vary Px(y). That is:</span>
</p>
<p>
<span class="f33" data-bbox="266.6,697.1,10.4,10.0" data-line="0" data-segment="0">ZZ</span>
<span class="f7" data-bbox="330.0,702.9,26.5,5.5" data-line="1" data-segment="0">P(x;y)</span>
<span class="f7" data-bbox="226.4,709.6,158.4,5.6" data-line="2" data-segment="0">R1 = Min P(x;y)log dx dy</span>
<span class="f8" data-bbox="249.2,716.5,112.2,5.5" data-line="3" data-segment="0">Px (y) P(x)P(y)</span>
</p>
<p>
<span class="f4" data-bbox="300.6,744.5,10.0,5.6" data-line="0" data-segment="0">49</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     <p>
<span class="f4" data-bbox="91.9,99.9,97.9,5.6" data-line="0" data-segment="0">subject to the constraint:</span>
<span class="f33" data-bbox="270.0,103.1,10.4,10.0" data-line="1" data-segment="0">ZZ</span>
<span class="f7" data-bbox="249.2,115.5,112.7,5.6" data-line="2" data-segment="0">v1 = P(x;y) (x;y)dx dy:</span>
</p>
<p>
<span class="f4" data-bbox="106.9,134.9,412.4,5.6" data-line="0" data-segment="0">This means that we consider, in effect, all the communication systems that might be used and that</span>
<span class="f4" data-bbox="91.9,146.8,427.3,5.6" data-line="1" data-segment="0">transmit with the required ﬁdelity. The rate of transmission in bits per second is calculated for each one</span>
<span class="f4" data-bbox="91.9,158.8,427.4,5.6" data-line="2" data-segment="0">and we choose that having the least rate. This latter rate is the rate we assign the source for the ﬁdelity in</span>
<span class="f4" data-bbox="91.9,170.8,36.4,5.6" data-line="3" data-segment="0">question.</span>
<span class="f4" data-bbox="106.9,182.7,243.1,5.6" data-line="4" data-segment="0">The justiﬁcation of this deﬁnition lies in the following result:</span>
</p>
<p>
<span class="f7" data-bbox="106.9,197.7,412.5,5.7" data-line="0" data-segment="0">Theorem 21: If a source has a rate R1 for a valuation v1 it is possible to encode the output of the source</span>
<span class="f4" data-bbox="91.9,209.6,427.6,8.5" data-line="1" data-segment="0">and transmit it over a channel of capacity C with ﬁdelity as near v1 as desired provided R1  C. This is not</span>
<span class="f4" data-bbox="91.9,221.6,75.0,5.7" data-line="2" data-segment="0">possible if R1 &gt; C.</span>
</p>
<p>
<span class="f4" data-bbox="106.9,236.6,412.4,5.6" data-line="0" data-segment="0">The last statement in the theorem follows immediately from the deﬁnition of R1 and previous results. If</span>
<span class="f4" data-bbox="91.9,248.5,427.6,5.6" data-line="1" data-segment="0">it were not true we could transmit more than C bits per second over a channel of capacity C. The ﬁrst part</span>
<span class="f4" data-bbox="91.9,260.5,428.5,5.6" data-line="2" data-segment="0">of the theorem is proved by a method analogous to that used for Theorem 11. We may, in the ﬁrst place,</span>
<span class="f4" data-bbox="91.9,272.3,427.4,5.6" data-line="3" data-segment="0">divide the (x;y) space into a large number of small cells and represent the situation as a discrete case. This</span>
<span class="f4" data-bbox="91.9,284.3,427.4,5.6" data-line="4" data-segment="0">will not change the evaluation function by more than an arbitrarily small amount (when the cells are very</span>
<span class="f4" data-bbox="91.9,296.3,427.4,5.6" data-line="5" data-segment="0">small) because of the continuity assumed for  (x;y). Suppose that P1(x;y) is the particular system which</span>
<span class="f4" data-bbox="91.9,308.2,399.5,5.6" data-line="6" data-segment="0">minimizes the rate and gives R1. We choose from the high probability y’s a set at random containing</span>
</p>
<p>
<span class="f18" data-bbox="294.0,322.9,26.9,4.8" data-line="0" data-segment="0">(R1+ )T</span>
<span class="f4" data-bbox="289.0,327.1,5.0,5.6" data-line="1" data-segment="0">2</span>
</p>
<p>
<span class="f4" data-bbox="91.9,345.8,427.6,8.5" data-line="0" data-segment="0">members where   ! 0 as T ! ∞. With large T each chosen point will be connected by a high probability</span>
<span class="f4" data-bbox="91.9,357.8,427.3,5.6" data-line="1" data-segment="0">line (as in Fig. 10) to a set of x’s. A calculation similar to that used in proving Theorem 11 shows that with</span>
<span class="f4" data-bbox="91.9,369.7,427.5,5.6" data-line="2" data-segment="0">large T almost all x’s are covered by the fans from the chosen y points for almost all choices of the y’s. The</span>
<span class="f4" data-bbox="91.9,381.7,427.5,5.6" data-line="3" data-segment="0">communication system to be used operates as follows: The selected points are assigned binary numbers.</span>
<span class="f4" data-bbox="91.9,393.7,427.5,8.5" data-line="4" data-segment="0">When a message x is originated it will (with probability approaching 1 as T ! ∞) lie within at least one</span>
<span class="f4" data-bbox="91.9,405.5,427.6,5.6" data-line="5" data-segment="0">of the fans. The corresponding binary number is transmitted (or one of them chosen arbitrarily if there are</span>
<span class="f4" data-bbox="91.9,417.5,427.4,8.5" data-line="6" data-segment="0">several) over the channel by suitable coding means to give a small probability of error. Since R1  C this is</span>
<span class="f4" data-bbox="91.9,429.4,415.0,5.6" data-line="7" data-segment="0">possible. At the receiving point the corresponding y is reconstructed and used as the recovered message.</span>
<span class="f14" data-bbox="175.3,437.8,2.0,2.5" data-line="8" data-segment="0">0</span>
<span class="f4" data-bbox="106.9,441.4,412.5,5.6" data-line="9" data-segment="0">The evaluation v for this system can be made arbitrarily close to v1 by taking T sufﬁciently large.</span>
<span class="f6" data-bbox="175.3,444.3,3.7,4.1" data-line="10" data-segment="0">1</span>
<span class="f4" data-bbox="91.9,453.4,427.6,5.6" data-line="11" data-segment="0">This is due to the fact that for each long sample of message x(t) and recovered message y(t) the evaluation</span>
<span class="f4" data-bbox="91.9,465.3,139.0,5.6" data-line="12" data-segment="0">approaches v1 (with probability 1).</span>
<span class="f4" data-bbox="106.9,477.3,412.1,5.6" data-line="13" data-segment="0">It is interesting to note that, in this system, the noise in the recovered message is actually produced by a</span>
<span class="f4" data-bbox="91.9,489.3,427.5,5.6" data-line="14" data-segment="0">kind of general quantizing at the transmitter and not produced by the noise in the channel. It is more or less</span>
<span class="f4" data-bbox="91.9,501.2,169.3,5.6" data-line="15" data-segment="0">analogous to the quantizing noise in PCM.</span>
</p>
<p>
<span class="f4" data-bbox="232.7,521.8,145.9,5.6" data-line="0" data-segment="0">29. THE CALCULATION OF RATES</span>
</p>
<p>
<span class="f4" data-bbox="91.9,540.5,410.9,5.6" data-line="0" data-segment="0">The deﬁnition of the rate is similar in many respects to the deﬁnition of channel capacity. In the former</span>
<span class="f33" data-bbox="264.6,552.3,10.4,10.0" data-line="1" data-segment="0">ZZ</span>
<span class="f7" data-bbox="327.8,557.9,26.5,5.5" data-line="2" data-segment="0">P(x;y)</span>
<span class="f7" data-bbox="228.6,564.7,154.1,5.6" data-line="3" data-segment="0">R = Min P(x;y)log dx dy</span>
<span class="f8" data-bbox="247.2,571.5,112.2,5.5" data-line="4" data-segment="0">Px (y) P(x)P(y)</span>
<span class="f33" data-bbox="170.5,582.5,10.6,10.0" data-line="5" data-segment="0">ZZ</span>
<span class="f4" data-bbox="91.9,595.0,242.3,5.6" data-line="6" data-segment="0">with P(x) and v1 = P(x;y) (x;y)dx dy ﬁxed. In the latter</span>
</p>
<p>
<span class="f33" data-bbox="265.6,612.1,10.4,10.0" data-line="0" data-segment="0">ZZ</span>
<span class="f7" data-bbox="328.8,617.7,26.5,5.5" data-line="1" data-segment="0">P(x;y)</span>
<span class="f7" data-bbox="227.2,624.5,156.5,5.6" data-line="2" data-segment="0">C = Max P(x;y)log dx dy</span>
<span class="f8" data-bbox="248.5,631.4,111.8,5.5" data-line="3" data-segment="0">P(x) P(x)P(y)</span>
</p>
<p>
<span class="f4" data-bbox="91.9,649.9,427.5,5.6" data-line="0" data-segment="0">with Px(y) ﬁxed and possibly one or more other constraints (e.g., an average power limitation) of the form</span>
<span class="f33" data-bbox="111.2,654.4,8.3,10.0" data-line="1" data-segment="0">RR</span>
<span class="f7" data-bbox="91.9,661.9,107.7,5.6" data-line="2" data-segment="0">K = P(x;y) (x;y)dx dy.</span>
<span class="f4" data-bbox="106.9,673.7,412.5,5.6" data-line="3" data-segment="0">A partial solution of the general maximizing problem for determining the rate of a source can be given.</span>
<span class="f4" data-bbox="91.9,685.7,153.4,5.6" data-line="4" data-segment="0">Using Lagrange’s method we consider</span>
<span class="f33" data-bbox="179.0,696.1,229.0,14.9" data-line="5" data-segment="0">ZZ    </span>
<span class="f7" data-bbox="246.5,703.4,26.5,5.5" data-line="6" data-segment="0">P(x;y)</span>
<span class="f7" data-bbox="198.7,710.2,233.5,5.6" data-line="7" data-segment="0">P(x;y)log +  P(x;y) (x;y) +  (x)P(x;y) dx dy:</span>
<span class="f7" data-bbox="241.4,717.1,36.5,5.5" data-line="8" data-segment="0">P(x)P(y)</span>
</p>
<p>
<span class="f4" data-bbox="300.6,744.5,10.0,5.6" data-line="0" data-segment="0">50</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                                                                                                                                   <p>
<span class="f4" data-bbox="91.9,99.9,305.9,5.6" data-line="0" data-segment="0">The variational equation (when we take the ﬁrst variation on P(x;y)) leads to</span>
</p>
<p>
<span class="f14" data-bbox="318.8,119.0,28.8,4.1" data-line="0" data-segment="0">   (x;y)</span>
<span class="f7" data-bbox="262.9,123.1,55.9,5.5" data-line="1" data-segment="0">Py(x) = B(x)e</span>
</p>
<p>
<span class="f4" data-bbox="91.9,145.0,315.4,5.6" data-line="0" data-segment="0">where   is determined to give the required ﬁdelity and B(x) is chosen to satisfy</span>
<span class="f33" data-bbox="259.0,158.5,5.2,10.0" data-line="1" data-segment="0">Z</span>
<span class="f14" data-bbox="292.0,166.7,28.8,4.1" data-line="2" data-segment="0">   (x;y)</span>
<span class="f7" data-bbox="269.3,170.8,83.0,5.6" data-line="3" data-segment="0">B(x)e dx = 1:</span>
</p>
<p>
<span class="f4" data-bbox="106.9,196.1,412.5,5.6" data-line="0" data-segment="0">This shows that, with best encoding, the conditional probability of a certain cause for various received</span>
<span class="f7" data-bbox="91.9,208.0,400.4,5.6" data-line="1" data-segment="0">y, Py(x) will decline exponentially with the distance function  (x;y) between the x and y in question.</span>
<span class="f4" data-bbox="106.9,220.0,412.4,5.6" data-line="2" data-segment="0">In the special case where the distance function  (x;y) depends only on the (vector) difference between x</span>
<span class="f4" data-bbox="91.9,231.9,23.9,5.6" data-line="3" data-segment="0">and y,</span>
<span class="f10" data-bbox="270.6,243.9,70.2,8.5" data-line="4" data-segment="0"> (x;y) =  (x  y)</span>
</p>
<p>
<span class="f4" data-bbox="91.9,261.8,170.3,11.2" data-line="0" data-segment="0">we have Z</span>
<span class="f14" data-bbox="290.0,271.3,32.5,4.1" data-line="1" data-segment="0">   (x y)</span>
<span class="f7" data-bbox="267.4,275.5,86.9,5.6" data-line="2" data-segment="0">B(x)e dx = 1:</span>
</p>
<p>
<span class="f4" data-bbox="91.9,297.3,137.7,5.6" data-line="0" data-segment="0">Hence B(x) is constant, say  , and</span>
<span class="f14" data-bbox="309.7,306.3,32.5,4.1" data-line="1" data-segment="0">   (x y)</span>
<span class="f7" data-bbox="265.7,310.5,79.9,5.5" data-line="2" data-segment="0">Py(x) =  e :</span>
</p>
<p>
<span class="f4" data-bbox="91.9,328.4,427.4,5.6" data-line="0" data-segment="0">Unfortunately these formal solutions are difﬁcult to evaluate in particular cases and seem to be of little value.</span>
<span class="f4" data-bbox="91.9,340.4,359.6,5.6" data-line="1" data-segment="0">In fact, the actual calculation of rates has been carried out in only a few very simple cases.</span>
<span class="f4" data-bbox="106.9,352.3,412.5,5.6" data-line="2" data-segment="0">If the distance function  (x;y) is the mean square discrepancy between x and y and the message ensemble</span>
<span class="f4" data-bbox="91.9,364.3,251.0,5.6" data-line="3" data-segment="0">is white noise, the rate can be determined. In that case we have</span>
<span class="f19" data-bbox="249.7,378.1,61.7,14.9" data-line="4" data-segment="0">   </span>
<span class="f7" data-bbox="214.8,386.2,181.6,8.5" data-line="5" data-segment="0">R = Min H(x)  Hy(x) = H(x)  Max Hy(x)</span>
</p>
<p>
<span class="f6" data-bbox="158.4,407.6,3.7,4.1" data-line="0" data-segment="0">2</span>
<span class="f4" data-bbox="91.9,410.5,427.5,8.5" data-line="1" data-segment="0">with N = (x  y) . But the Max Hy(x) occurs when y  x is a white noise, and is equal to W1 log 2 eN where</span>
<span class="f7" data-bbox="91.4,422.3,227.2,5.6" data-line="2" data-segment="0">W1 is the bandwidth of the message ensemble. Therefore</span>
</p>
<p>
<span class="f7" data-bbox="242.2,444.3,126.4,8.5" data-line="0" data-segment="0">R = W1 log 2 eQ  W1 log 2 eN</span>
<span class="f7" data-bbox="288.0,458.6,7.2,5.5" data-line="1" data-segment="0">Q</span>
<span class="f9" data-bbox="250.4,465.3,35.4,5.6" data-line="2" data-segment="0">= W1 log</span>
<span class="f7" data-bbox="288.0,472.1,6.7,5.5" data-line="3" data-segment="0">N</span>
</p>
<p>
<span class="f4" data-bbox="91.9,490.0,264.5,5.6" data-line="0" data-segment="0">where Q is the average message power. This proves the following:</span>
</p>
<p>
<span class="f7" data-bbox="106.9,505.0,412.5,5.7" data-line="0" data-segment="0">Theorem 22: The rate for a white noise source of power Q and band W1 relative to an R.M.S. measure</span>
<span class="f4" data-bbox="91.9,516.9,48.2,5.7" data-line="1" data-segment="0">of ﬁdelity is</span>
<span class="f7" data-bbox="324.4,526.7,7.2,5.5" data-line="2" data-segment="0">Q</span>
<span class="f7" data-bbox="278.5,533.6,43.6,5.6" data-line="3" data-segment="0">R = W1 log</span>
<span class="f7" data-bbox="324.4,540.4,6.7,5.5" data-line="4" data-segment="0">N</span>
<span class="f4" data-bbox="91.9,554.3,336.1,5.7" data-line="5" data-segment="0">where N is the allowed mean square error between original and recovered messages.</span>
<span class="f4" data-bbox="106.9,569.2,412.5,5.6" data-line="6" data-segment="0">More generally with any message source we can obtain inequalities bounding the rate relative to a mean</span>
<span class="f4" data-bbox="91.9,581.2,86.9,5.6" data-line="7" data-segment="0">square error criterion.</span>
<span class="f7" data-bbox="106.9,596.1,253.0,5.7" data-line="8" data-segment="0">Theorem 23: The rate for any source of band W1 is bounded by</span>
</p>
<p>
<span class="f7" data-bbox="280.0,616.0,77.8,5.6" data-line="0" data-segment="0">Q1 Q</span>
<span class="f7" data-bbox="251.8,622.7,96.6,8.5" data-line="1" data-segment="0">W1 log  R  W1 log</span>
<span class="f7" data-bbox="282.0,629.6,75.2,5.5" data-line="2" data-segment="0">N N</span>
</p>
<p>
<span class="f4" data-bbox="91.9,647.5,411.8,5.7" data-line="0" data-segment="0">where Q is the average power of the source, Q1 its entropy power and N the allowed mean square error.</span>
<span class="f6" data-bbox="416.3,660.5,3.7,4.1" data-line="1" data-segment="0">2</span>
<span class="f4" data-bbox="106.9,663.4,412.4,8.5" data-line="2" data-segment="0">The lower bound follows from the fact that the Max Hy(x) for a given (x  y) = N occurs in the white</span>
<span class="f4" data-bbox="91.9,675.4,427.5,5.6" data-line="3" data-segment="0">noise case. The upper bound results if we place points (used in the proof of Theorem 21) not in the best way</span>
<span class="f16" data-bbox="232.8,679.9,8.3,8.5" data-line="4" data-segment="0">p</span>
<span class="f4" data-bbox="91.9,687.3,176.6,8.5" data-line="5" data-segment="0">but at random in a sphere of radius Q  N.</span>
</p>
<p>
<span class="f4" data-bbox="300.6,744.5,10.0,5.6" data-line="0" data-segment="0">51</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                                                                                                                                         <p>
<span class="f4" data-bbox="259.7,99.9,92.0,5.6" data-line="0" data-segment="0">ACKNOWLEDGMENTS</span>
</p>
<p>
<span class="f4" data-bbox="91.9,118.6,427.3,5.6" data-line="0" data-segment="0">The writer is indebted to his colleagues at the Laboratories, particularly to Dr. H. W. Bode, Dr. J. R. Pierce,</span>
<span class="f4" data-bbox="91.9,130.6,427.4,5.6" data-line="1" data-segment="0">Dr. B. McMillan, and Dr. B. M. Oliver for many helpful suggestions and criticisms during the course of this</span>
<span class="f4" data-bbox="91.9,142.5,427.5,5.6" data-line="2" data-segment="0">work. Credit should also be given to Professor N. Wiener, whose elegant solution of the problems of ﬁltering</span>
<span class="f4" data-bbox="91.9,154.5,400.0,5.6" data-line="3" data-segment="0">and prediction of stationary ensembles has considerably inﬂuenced the writer’s thinking in this ﬁeld.</span>
</p>
<p>
<span class="f4" data-bbox="275.3,178.4,60.7,5.6" data-line="0" data-segment="0">APPENDIX 5</span>
</p>
<p>
<span class="f4" data-bbox="91.9,202.4,426.9,5.6" data-line="0" data-segment="0">Let S1 be any measurable subset of the g ensemble, and S2 the subset of the f ensemble which gives S1</span>
<span class="f4" data-bbox="91.9,214.3,113.0,5.6" data-line="1" data-segment="0">under the operation T . Then</span>
<span class="f7" data-bbox="285.6,226.3,40.1,5.6" data-line="2" data-segment="0">S1 = T S2:</span>
<span class="f29" data-bbox="115.6,239.3,4.7,3.2" data-line="3" data-segment="0"> </span>
<span class="f4" data-bbox="91.9,242.9,306.5,5.6" data-line="4" data-segment="0">Let H be the operator which shifts all functions in a set by the time  . Then</span>
<span class="f29" data-bbox="261.1,258.7,87.2,3.2" data-line="5" data-segment="0">     </span>
<span class="f7" data-bbox="253.2,262.9,104.4,5.6" data-line="6" data-segment="0">H S1 = H T S2 = T H S2</span>
<span class="f29" data-bbox="297.5,279.1,4.7,3.2" data-line="7" data-segment="0"> </span>
<span class="f4" data-bbox="91.9,282.7,422.8,5.6" data-line="8" data-segment="0">since T is invariant and therefore commutes with H . Hence if m[S] is the probability measure of the set S</span>
<span class="f29" data-bbox="255.5,298.5,105.7,3.2" data-line="9" data-segment="0">     </span>
<span class="f7" data-bbox="237.6,302.6,136.1,5.6" data-line="10" data-segment="0">m[H S1] = m[T H S2] = m[H S2]</span>
<span class="f9" data-bbox="274.8,317.6,65.9,5.6" data-line="11" data-segment="0">= m[S2] = m[S1]</span>
</p>
<p>
<span class="f4" data-bbox="91.9,337.4,427.4,5.6" data-line="0" data-segment="0">where the second equality is by deﬁnition of measure in the g space, the third since the f ensemble is</span>
<span class="f4" data-bbox="91.9,349.4,222.8,5.6" data-line="1" data-segment="0">stationary, and the last by deﬁnition of g measure again.</span>
<span class="f4" data-bbox="106.9,361.3,412.4,5.6" data-line="2" data-segment="0">To prove that the ergodic property is preserved under invariant operations, let S1 be a subset of the g</span>
<span class="f29" data-bbox="237.7,369.7,4.7,3.2" data-line="3" data-segment="0"> </span>
<span class="f4" data-bbox="91.9,373.3,427.5,5.6" data-line="4" data-segment="0">ensemble which is invariant under H , and let S2 be the set of all functions f which transform into S1. Then</span>
<span class="f29" data-bbox="250.4,389.0,87.2,3.2" data-line="5" data-segment="0">     </span>
<span class="f7" data-bbox="242.5,393.2,125.7,5.6" data-line="6" data-segment="0">H S1 = H T S2 = T H S2 = S1</span>
<span class="f29" data-bbox="128.6,409.4,4.7,3.2" data-line="7" data-segment="0"> </span>
<span class="f4" data-bbox="91.9,413.0,203.3,5.6" data-line="8" data-segment="0">so that H S2 is included in S2 for all  . Now, since</span>
<span class="f29" data-bbox="289.0,428.8,4.7,3.2" data-line="9" data-segment="0"> </span>
<span class="f7" data-bbox="271.1,432.9,69.1,5.6" data-line="10" data-segment="0">m[H S2] = m[S1]</span>
</p>
<p>
<span class="f4" data-bbox="91.9,452.8,46.4,5.6" data-line="0" data-segment="0">this implies</span>
<span class="f29" data-bbox="291.7,460.6,4.7,3.2" data-line="1" data-segment="0"> </span>
<span class="f7" data-bbox="283.8,464.7,43.2,5.6" data-line="2" data-segment="0">H S2 = S2</span>
</p>
<p>
<span class="f4" data-bbox="91.9,481.5,297.2,8.5" data-line="0" data-segment="0">for all   with m[S2] 6= 0;1. This contradiction shows that S1 does not exist.</span>
</p>
<p>
<span class="f4" data-bbox="275.3,505.4,60.7,5.6" data-line="0" data-segment="0">APPENDIX 6</span>
</p>
<p>
<span class="f4" data-bbox="91.9,529.3,426.9,8.5" data-line="0" data-segment="0">The upper bound, N3   N1 +N2, is due to the fact that the maximum possible entropy for a power N1 + N2</span>
<span class="f4" data-bbox="91.9,541.3,365.3,5.6" data-line="1" data-segment="0">occurs when we have a white noise of this power. In this case the entropy power is N1 + N2.</span>
<span class="f4" data-bbox="106.9,553.1,412.4,5.6" data-line="2" data-segment="0">To obtain the lower bound, suppose we have two distributions in n dimensions p(xi) and q(xi) with</span>
<span class="f4" data-bbox="91.9,565.1,427.5,5.6" data-line="3" data-segment="0">entropy powers N1 and N2. What form should p and q have to minimize the entropy power N3 of their</span>
<span class="f4" data-bbox="91.9,577.1,71.1,5.6" data-line="4" data-segment="0">convolution r(xi ):</span>
<span class="f33" data-bbox="279.0,580.6,5.2,10.0" data-line="5" data-segment="0">Z</span>
<span class="f7" data-bbox="248.0,593.0,115.2,8.5" data-line="6" data-segment="0">r(xi) = p(yi)q(xi  yi)dyi:</span>
</p>
<p>
<span class="f4" data-bbox="106.9,613.0,125.1,5.6" data-line="0" data-segment="0">The entropy H3 of r is given by</span>
<span class="f33" data-bbox="282.1,624.2,5.2,10.0" data-line="1" data-segment="0">Z</span>
<span class="f7" data-bbox="250.0,636.5,111.4,8.5" data-line="2" data-segment="0">H3 =   r(xi)log r(xi)dxi :</span>
</p>
<p>
<span class="f4" data-bbox="91.9,659.7,203.1,5.6" data-line="0" data-segment="0">We wish to minimize this subject to the constraints</span>
<span class="f33" data-bbox="281.3,670.9,5.2,10.0" data-line="1" data-segment="0">Z</span>
<span class="f7" data-bbox="249.1,683.3,111.3,8.5" data-line="2" data-segment="0">H1 =   p(xi)log p(xi)dxi</span>
<span class="f33" data-bbox="281.3,694.7,5.2,10.0" data-line="3" data-segment="0">Z</span>
<span class="f7" data-bbox="249.1,707.2,113.0,8.5" data-line="4" data-segment="0">H2 =   q(xi)log q(xi)dxi:</span>
</p>
<p>
<span class="f4" data-bbox="300.6,744.5,10.0,5.6" data-line="0" data-segment="0">52</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                              <p>
<span class="f4" data-bbox="91.9,99.9,69.0,5.6" data-line="0" data-segment="0">We consider then</span>
<span class="f33" data-bbox="179.5,110.1,5.2,10.0" data-line="1" data-segment="0">Z</span>
<span class="f19" data-bbox="188.6,114.4,188.8,14.9" data-line="2" data-segment="0">   </span>
<span class="f7" data-bbox="150.4,122.5,238.1,8.5" data-line="3" data-segment="0">U =   r(x)log r(x) +  p(x)log p(x) +  q(x)log q(x) dx</span>
<span class="f33" data-bbox="179.5,134.0,5.2,10.0" data-line="4" data-segment="0">Z</span>
<span class="f19" data-bbox="188.6,138.3,262.8,14.9" data-line="5" data-segment="0">   </span>
<span class="f10" data-bbox="146.0,146.5,319.2,8.5" data-line="6" data-segment="0"> U =   [1 +log r(x)] r(x) +  [1 + log p(x)] p(x) +  [1 + logq(x)] q(x) dx:</span>
</p>
<p>
<span class="f4" data-bbox="106.9,171.3,281.4,5.6" data-line="0" data-segment="0">If p(x) is varied at a particular argument xi = si , the variation in r(x) is</span>
</p>
<p>
<span class="f10" data-bbox="270.7,192.3,69.8,8.5" data-line="0" data-segment="0"> r(x) = q(xi  si)</span>
</p>
<p>
<span class="f4" data-bbox="91.9,213.2,154.3,11.2" data-line="0" data-segment="0">and Z</span>
</p>
<p>
<span class="f10" data-bbox="207.6,226.7,196.0,8.5" data-line="0" data-segment="0"> U =   q(xi  si)log r(xi )dxi   log p(si) = 0</span>
</p>
<p>
<span class="f4" data-bbox="91.9,247.4,286.8,5.6" data-line="0" data-segment="0">and similarly when q is varied. Hence the conditions for a minimum are</span>
<span class="f33" data-bbox="227.5,259.6,5.2,10.0" data-line="1" data-segment="0">Z</span>
<span class="f7" data-bbox="237.8,272.0,144.5,8.5" data-line="2" data-segment="0">q(xi  si)log r(xi )dxi =   log p(si )</span>
<span class="f33" data-bbox="226.8,283.5,5.2,10.0" data-line="3" data-segment="0">Z</span>
<span class="f7" data-bbox="237.7,295.9,146.8,8.5" data-line="4" data-segment="0">p(xi  si)log r(xi )dxi =   log q(si):</span>
</p>
<p>
<span class="f4" data-bbox="91.9,320.8,386.0,5.6" data-line="0" data-segment="0">If we multiply the ﬁrst by p(si) and the second by q(si) and integrate with respect to si we obtain</span>
</p>
<p>
<span class="f7" data-bbox="281.8,341.8,47.5,8.5" data-line="0" data-segment="0">H3 =   H1</span>
</p>
<p>
<span class="f7" data-bbox="281.5,356.7,47.7,8.5" data-line="0" data-segment="0">H3 =   H2</span>
</p>
<p>
<span class="f4" data-bbox="91.9,377.7,210.1,5.6" data-line="0" data-segment="0">or solving for   and   and replacing in the equations</span>
<span class="f33" data-bbox="231.0,389.8,5.2,10.0" data-line="1" data-segment="0">Z</span>
<span class="f7" data-bbox="218.8,402.3,172.3,8.5" data-line="2" data-segment="0">H1 q(xi  si)log r(xi)dxi =  H3 log p(si)</span>
<span class="f33" data-bbox="230.3,413.7,5.2,10.0" data-line="3" data-segment="0">Z</span>
<span class="f7" data-bbox="218.0,426.2,175.1,8.5" data-line="4" data-segment="0">H2 p(xi  si)log r(xi)dxi =  H3 log q(si):</span>
</p>
<p>
<span class="f4" data-bbox="91.9,451.0,161.3,5.6" data-line="0" data-segment="0">Now suppose p(xi) and q(xi) are normal</span>
</p>
<p>
<span class="f8" data-bbox="291.0,469.0,11.1,4.1" data-line="0" data-segment="0">n=2</span>
<span class="f16" data-bbox="273.5,472.6,17.5,8.5" data-line="1" data-segment="0">jAi j j</span>
<span class="f6" data-bbox="329.8,475.6,3.7,4.1" data-line="2" data-segment="0">1</span>
<span class="f7" data-bbox="239.8,479.6,133.8,8.5" data-line="3" data-segment="0">p(xi) = exp   Ai j xix j</span>
<span class="f20" data-bbox="335.6,481.7,10.3,9.4" data-line="4" data-segment="0">∑</span>
<span class="f8" data-bbox="291.6,483.1,41.9,5.1" data-line="5" data-segment="0">n=2 2</span>
<span class="f9" data-bbox="272.9,486.9,18.7,5.6" data-line="6" data-segment="0">(2 )</span>
<span class="f8" data-bbox="288.2,498.9,11.1,4.1" data-line="7" data-segment="0">n=2</span>
<span class="f16" data-bbox="270.7,502.5,17.5,8.5" data-line="8" data-segment="0">jBi j j</span>
<span class="f6" data-bbox="327.0,505.5,3.7,4.1" data-line="9" data-segment="0">1</span>
<span class="f7" data-bbox="237.0,509.3,137.3,8.5" data-line="10" data-segment="0">q(xi) = exp   Bi j xix j :</span>
<span class="f20" data-bbox="332.9,511.5,10.3,9.4" data-line="11" data-segment="0">∑</span>
<span class="f8" data-bbox="288.8,512.8,41.9,5.1" data-line="12" data-segment="0">n=2 2</span>
<span class="f9" data-bbox="270.1,516.8,18.7,5.6" data-line="13" data-segment="0">(2 )</span>
</p>
<p>
<span class="f4" data-bbox="91.9,536.7,416.6,5.6" data-line="0" data-segment="0">Then r(xi ) will also be normal with quadratic form Ci j . If the inverses of these forms are ai j , bi j , ci j then</span>
</p>
<p>
<span class="f7" data-bbox="276.8,557.6,57.5,5.5" data-line="0" data-segment="0">ci j = ai j + bi j:</span>
</p>
<p>
<span class="f4" data-bbox="91.9,578.6,427.5,5.6" data-line="0" data-segment="0">We wish to show that these functions satisfy the minimizing conditions if and only if ai j = Kbi j and thus</span>
<span class="f4" data-bbox="91.9,590.5,230.9,5.6" data-line="1" data-segment="0">give the minimum H3 under the constraints. First we have</span>
</p>
<p>
<span class="f7" data-bbox="274.7,609.3,30.3,5.6" data-line="0" data-segment="0">n 1</span>
<span class="f6" data-bbox="338.3,612.2,3.7,4.1" data-line="1" data-segment="0">1</span>
<span class="f4" data-bbox="228.7,616.0,153.1,8.5" data-line="2" data-segment="0">log r(xi ) = log jCi j j  Ci j xix j</span>
<span class="f6" data-bbox="338.3,618.2,16.1,9.4" data-line="3" data-segment="0">2 ∑</span>
<span class="f4" data-bbox="274.7,622.9,33.0,5.6" data-line="4" data-segment="0">2 2 </span>
<span class="f33" data-bbox="171.2,627.8,5.2,10.0" data-line="5" data-segment="0">Z</span>
<span class="f7" data-bbox="278.9,633.5,30.3,5.6" data-line="6" data-segment="0">n 1</span>
<span class="f6" data-bbox="342.4,636.4,58.7,4.1" data-line="7" data-segment="0">1 1</span>
<span class="f7" data-bbox="181.6,640.3,258.5,8.5" data-line="8" data-segment="0">q(xi  si)log r(xi )dxi = log jCi j j  Ci j sis j   Ci j bi j :</span>
<span class="f6" data-bbox="342.4,642.4,71.2,9.4" data-line="9" data-segment="0">2 ∑ 2 ∑</span>
<span class="f4" data-bbox="278.9,647.1,33.0,5.6" data-line="10" data-segment="0">2 2 </span>
</p>
<p>
<span class="f4" data-bbox="91.9,664.5,71.0,5.6" data-line="0" data-segment="0">This should equal</span>
<span class="f19" data-bbox="253.0,668.1,118.9,14.9" data-line="1" data-segment="0">   </span>
<span class="f7" data-bbox="240.6,675.4,49.2,5.6" data-line="2" data-segment="0">H3 n 1</span>
<span class="f6" data-bbox="323.2,678.2,3.7,4.1" data-line="3" data-segment="0">1</span>
<span class="f4" data-bbox="266.6,682.1,99.1,8.5" data-line="4" data-segment="0">log jAi j j  Ai j sis j</span>
<span class="f6" data-bbox="323.2,684.3,16.3,9.4" data-line="5" data-segment="0">2 ∑</span>
<span class="f7" data-bbox="240.6,689.0,51.8,5.6" data-line="6" data-segment="0">H1 2 2 </span>
</p>
<p>
<span class="f7" data-bbox="178.7,704.5,113.0,5.6" data-line="0" data-segment="0">H1 H1</span>
<span class="f4" data-bbox="91.9,711.2,372.0,5.6" data-line="1" data-segment="0">which requires Ai j = Ci j . In this case Ai j = Bi j and both equations reduce to identities.</span>
<span class="f7" data-bbox="178.7,718.0,113.0,5.6" data-line="2" data-segment="0">H3 H2</span>
</p>
<p>
<span class="f4" data-bbox="300.6,744.5,10.0,5.6" data-line="0" data-segment="0">53</span>
</p>
</div>
<div class="page">
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  <p>
<span class="f4" data-bbox="275.3,99.9,60.7,5.6" data-line="0" data-segment="0">APPENDIX 7</span>
</p>
<p>
<span class="f4" data-bbox="91.9,123.9,427.7,5.6" data-line="0" data-segment="0">The following will indicate a more general and more rigorous approach to the central deﬁnitions of commu-</span>
<span class="f4" data-bbox="91.9,135.8,427.6,5.6" data-line="1" data-segment="0">nication theory. Consider a probability measure space whose elements are ordered pairs (x;y). The variables</span>
<span class="f7" data-bbox="91.9,147.8,427.5,5.6" data-line="2" data-segment="0">x, y are to be identiﬁed as the possible transmitted and received signals of some long duration T . Let us call</span>
<span class="f4" data-bbox="91.9,159.8,427.6,5.6" data-line="3" data-segment="0">the set of all points whose x belongs to a subset S1 of x points the strip over S1, and similarly the set whose</span>
<span class="f7" data-bbox="91.9,171.7,427.6,5.6" data-line="4" data-segment="0">y belong to S2 the strip over S2. We divide x and y into a collection of non-overlapping measurable subsets</span>
<span class="f7" data-bbox="91.9,183.7,214.3,5.6" data-line="5" data-segment="0">Xi and Yi approximate to the rate of transmission R by</span>
</p>
<p>
<span class="f4" data-bbox="261.4,203.5,106.9,5.6" data-line="0" data-segment="0">1 P(Xi;Yi)</span>
<span class="f7" data-bbox="236.8,210.2,91.2,5.6" data-line="1" data-segment="0">R1 = P(Xi;Yi )log</span>
<span class="f20" data-bbox="269.5,212.3,10.3,9.4" data-line="2" data-segment="0">∑</span>
<span class="f7" data-bbox="260.4,217.0,112.8,5.5" data-line="3" data-segment="0">T P(Xi)P(Yi)</span>
<span class="f8" data-bbox="273.6,220.3,2.1,4.1" data-line="4" data-segment="0">i</span>
</p>
<p>
<span class="f4" data-bbox="91.9,238.3,24.5,5.6" data-line="0" data-segment="0">where</span>
</p>
<p>
<span class="f7" data-bbox="180.0,259.4,211.5,5.6" data-line="0" data-segment="0">P(Xi) is the probability measure of the strip over Xi</span>
</p>
<p>
<span class="f7" data-bbox="181.3,274.4,208.8,5.6" data-line="0" data-segment="0">P(Yi) is the probability measure of the strip over Yi</span>
</p>
<p>
<span class="f7" data-bbox="169.1,289.3,273.1,5.6" data-line="0" data-segment="0">P(Xi;Yi) is the probability measure of the intersection of the strips:</span>
</p>
<p>
<span class="f14" data-bbox="405.0,306.8,24.6,2.5" data-line="0" data-segment="0">0 00</span>
<span class="f4" data-bbox="91.9,310.4,367.4,5.6" data-line="1" data-segment="0">A further subdivision can never decrease R1. For let X1 be divided into X1 = X + X and let</span>
<span class="f6" data-bbox="403.8,313.4,24.3,4.1" data-line="2" data-segment="0">1 1</span>
</p>
<p>
<span class="f7" data-bbox="233.0,331.9,149.3,5.6" data-line="0" data-segment="0">P(Y1) = a P(X1) = b + c</span>
<span class="f14" data-bbox="248.8,342.7,83.9,2.5" data-line="1" data-segment="0">0 0</span>
<span class="f7" data-bbox="231.7,346.7,135.4,5.6" data-line="2" data-segment="0">P(X ) = b P(X ;Y1) = d</span>
<span class="f6" data-bbox="247.6,349.3,85.5,4.1" data-line="3" data-segment="0">1 1</span>
<span class="f14" data-bbox="247.3,357.7,85.8,2.5" data-line="4" data-segment="0">00 00</span>
<span class="f7" data-bbox="230.3,361.7,136.3,5.6" data-line="5" data-segment="0">P(X ) = c P(X ;Y1) = e</span>
<span class="f6" data-bbox="246.1,364.1,85.5,4.1" data-line="6" data-segment="0">1 1</span>
<span class="f7" data-bbox="269.8,377.7,71.9,5.6" data-line="7" data-segment="0">P(X1;Y1) = d + e:</span>
</p>
<p>
<span class="f4" data-bbox="91.9,398.8,246.6,5.6" data-line="0" data-segment="0">Then in the sum we have replaced (for the X1, Y1 intersection)</span>
</p>
<p>
<span class="f7" data-bbox="263.4,417.9,128.4,5.5" data-line="0" data-segment="0">d + e d e</span>
<span class="f9" data-bbox="213.0,424.7,185.3,5.6" data-line="1" data-segment="0">(d +e)log by d log + e log :</span>
<span class="f7" data-bbox="257.3,431.6,137.0,5.5" data-line="2" data-segment="0">a(b + c) ab ac</span>
</p>
<p>
<span class="f4" data-bbox="91.9,451.0,249.6,5.6" data-line="0" data-segment="0">It is easily shown that with the limitation we have on b, c, d, e,</span>
</p>
<p>
<span class="f19" data-bbox="265.8,466.0,33.4,14.9" data-line="0" data-segment="0">   </span>
<span class="f8" data-bbox="299.0,468.4,44.8,5.3" data-line="1" data-segment="0">d+e d e</span>
<span class="f7" data-bbox="272.2,473.3,68.4,5.5" data-line="2" data-segment="0">d + e d e</span>
<span class="f16" data-bbox="314.9,480.1,7.8,8.5" data-line="3" data-segment="0"> </span>
<span class="f8" data-bbox="331.3,484.0,12.3,4.1" data-line="4" data-segment="0">d e</span>
<span class="f7" data-bbox="272.4,486.9,67.9,5.5" data-line="5" data-segment="0">b + c b c</span>
</p>
<p>
<span class="f4" data-bbox="91.9,506.5,427.6,5.6" data-line="0" data-segment="0">and consequently the sum is increased. Thus the various possible subdivisions form a directed set, with</span>
<span class="f7" data-bbox="91.9,518.3,427.4,5.6" data-line="1" data-segment="0">R monotonic increasing with reﬁnement of the subdivision. We may deﬁne R unambiguously as the least</span>
<span class="f4" data-bbox="91.9,530.3,124.8,5.6" data-line="2" data-segment="0">upper bound for R1 and write it</span>
<span class="f33" data-bbox="259.7,544.4,10.6,10.0" data-line="3" data-segment="0">ZZ</span>
<span class="f4" data-bbox="251.5,550.1,98.0,5.6" data-line="4" data-segment="0">1 P(x;y)</span>
<span class="f7" data-bbox="231.1,556.9,148.9,5.6" data-line="5" data-segment="0">R = P(x;y)log dx dy:</span>
<span class="f7" data-bbox="250.7,563.7,103.9,5.5" data-line="6" data-segment="0">T P(x)P(y)</span>
</p>
<p>
<span class="f4" data-bbox="91.9,583.1,427.3,5.6" data-line="0" data-segment="0">This integral, understood in the above sense, includes both the continuous and discrete cases and of course</span>
<span class="f4" data-bbox="91.9,595.1,427.4,5.6" data-line="1" data-segment="0">many others which cannot be represented in either form. It is trivial in this formulation that if x and u are</span>
<span class="f4" data-bbox="91.9,607.0,427.6,5.6" data-line="2" data-segment="0">in one-to-one correspondence, the rate from u to y is equal to that from x to y. If v is any function of y (not</span>
<span class="f4" data-bbox="91.9,619.0,427.4,5.6" data-line="3" data-segment="0">necessarily with an inverse) then the rate from x to y is greater than or equal to that from x to v since, in</span>
<span class="f4" data-bbox="91.9,631.0,427.6,5.6" data-line="4" data-segment="0">the calculation of the approximations, the subdivisions of y are essentially a ﬁner subdivision of those for</span>
<span class="f7" data-bbox="91.9,642.9,427.4,5.6" data-line="5" data-segment="0">v. More generally if y and v are related not functionally but statistically, i.e., we have a probability measure</span>
<span class="f4" data-bbox="91.9,654.9,427.4,8.5" data-line="6" data-segment="0">space (y;v), then R(x;v)  R(x;y). This means that any operation applied to the received signal, even though</span>
<span class="f4" data-bbox="91.9,666.8,203.7,5.6" data-line="7" data-segment="0">it involves statistical elements, does not increase R.</span>
<span class="f4" data-bbox="106.9,678.8,412.4,5.6" data-line="8" data-segment="0">Another notion which should be deﬁned precisely in an abstract formulation of the theory is that of</span>
<span class="f4" data-bbox="91.9,690.8,427.4,5.6" data-line="9" data-segment="0">“dimension rate,” that is the average number of dimensions required per second to specify a member of</span>
<span class="f4" data-bbox="91.9,702.7,427.5,5.6" data-line="10" data-segment="0">an ensemble. In the band limited case 2W numbers per second are sufﬁcient. A general deﬁnition can be</span>
<span class="f4" data-bbox="91.9,714.7,427.7,5.6" data-line="11" data-segment="0">framed as follows. Let f (t) be an ensemble of functions and let  T [f (t); f (t)] be a metric measuring</span>
</p>
<p>
<span class="f4" data-bbox="300.6,744.5,10.0,5.6" data-line="0" data-segment="0">54</span>
</p>
</div>
<div class="page">
                                                                                                                  <p>
<span class="f4" data-bbox="91.9,99.9,427.4,5.6" data-line="0" data-segment="0">the “distance” from f  to f  over the time T (for example the R.M.S. discrepancy over this interval.) Let</span>
<span class="f7" data-bbox="91.9,111.9,427.7,5.6" data-line="1" data-segment="0">N( ; ;T ) be the least number of elements f which can be chosen such that all elements of the ensemble</span>
<span class="f4" data-bbox="91.9,123.9,427.8,5.6" data-line="2" data-segment="0">apart from a set of measure   are within the distance   of at least one of those chosen. Thus we are covering</span>
<span class="f4" data-bbox="91.9,135.8,427.5,5.6" data-line="3" data-segment="0">the space to within   apart from a set of small measure  . We deﬁne the dimension rate   for the ensemble</span>
<span class="f4" data-bbox="91.9,147.8,69.5,5.6" data-line="4" data-segment="0">by the triple limit</span>
<span class="f4" data-bbox="315.4,158.5,52.2,5.6" data-line="5" data-segment="0">log N( ; ;T )</span>
<span class="f10" data-bbox="239.8,165.2,131.8,5.6" data-line="6" data-segment="0">  = Lim Lim Lim :</span>
<span class="f29" data-bbox="258.6,171.5,95.8,6.1" data-line="7" data-segment="0"> !0  !0 T !∞ T log  </span>
</p>
<p>
<span class="f4" data-bbox="91.9,187.9,427.3,5.6" data-line="0" data-segment="0">This is a generalization of the measure type deﬁnitions of dimension in topology, and agrees with the intu-</span>
<span class="f4" data-bbox="91.9,199.9,309.0,5.6" data-line="1" data-segment="0">itive dimension rate for simple ensembles where the desired result is obvious.</span>
</p>
<p>
<span class="f4" data-bbox="300.6,744.5,10.0,5.6" data-line="0" data-segment="0">55</span>
</p>
</div>
<style type="text/css">
  .f1 { font-family: Times-Roman; font-size: 8.0px; font-style: normal; font-weight: normal; color: #000000; }
  .f2 { font-family: Times-Italic; font-size: 8.0px; font-style: italic; font-weight: normal; color: #000000; }
  .f3 { font-family: Times-Roman; font-size: 14.0px; font-style: normal; font-weight: normal; color: #000000; }
  .f4 { font-family: Times-Roman; font-size: 10.0px; font-style: normal; font-weight: normal; color: #000000; }
  .f5 { font-family: T24; font-size: 0.0px; font-style: normal; font-weight: normal; color: #000000; }
  .f6 { font-family: Times-Roman; font-size: 7.0px; font-style: normal; font-weight: normal; color: #000000; }
  .f7 { font-family: Times-Italic; font-size: 10.0px; font-style: italic; font-weight: normal; color: #000000; }
  .f8 { font-family: Times-Italic; font-size: 7.0px; font-style: italic; font-weight: normal; color: #000000; }
  .f9 { font-family: T22; font-size: 0.0px; font-style: normal; font-weight: normal; color: #000000; }
  .f10 { font-family: T23; font-size: 0.0px; font-style: normal; font-weight: normal; color: #000000; }
  .f11 { font-family: Times-Roman; font-size: 6.0px; font-style: normal; font-weight: normal; color: #000000; }
  .f12 { font-family: Times-Roman; font-size: 9.0px; font-style: normal; font-weight: normal; color: #000000; }
  .f13 { font-family: Times-Roman; font-size: 12.0px; font-style: normal; font-weight: normal; color: #000000; }
  .f14 { font-family: T19; font-size: 0.0px; font-style: normal; font-weight: normal; color: #000000; }
  .f15 { font-family: Symbol; font-size: 7.0px; font-style: normal; font-weight: normal; color: #000000; }
  .f16 { font-family: T18; font-size: 0.0px; font-style: normal; font-weight: normal; color: #000000; }
  .f17 { font-family: Times-Italic; font-size: 6.0px; font-style: italic; font-weight: normal; color: #000000; }
  .f18 { font-family: T20; font-size: 0.0px; font-style: normal; font-weight: normal; color: #000000; }
  .f19 { font-family: T16; font-size: 0.0px; font-style: normal; font-weight: normal; color: #000000; }
  .f20 { font-family: Symbol; font-size: 14.0px; font-style: normal; font-weight: normal; color: #000000; }
  .f21 { font-family: T17; font-size: 0.0px; font-style: normal; font-weight: normal; color: #000000; }
  .f22 { font-family: T21; font-size: 0.0px; font-style: normal; font-weight: normal; color: #000000; }
  .f23 { font-family: T14; font-size: 0.0px; font-style: normal; font-weight: normal; color: #000000; }
  .f24 { font-family: T13; font-size: 0.0px; font-style: normal; font-weight: normal; color: #000000; }
  .f25 { font-family: Symbol; font-size: 10.0px; font-style: normal; font-weight: normal; color: #000000; }
  .f26 { font-family: Times-Italic; font-size: 9.0px; font-style: italic; font-weight: normal; color: #000000; }
  .f27 { font-family: T11; font-size: 0.0px; font-style: normal; font-weight: normal; color: #000000; }
  .f28 { font-family: T10; font-size: 0.0px; font-style: normal; font-weight: normal; color: #000000; }
  .f29 { font-family: T9; font-size: 0.0px; font-style: normal; font-weight: normal; color: #000000; }
  .f30 { font-family: T8; font-size: 0.0px; font-style: normal; font-weight: normal; color: #000000; }
  .f31 { font-family: T7; font-size: 0.0px; font-style: normal; font-weight: normal; color: #000000; }
  .f32 { font-family: Times-Italic; font-size: 5.0px; font-style: italic; font-weight: normal; color: #000000; }
  .f33 { font-family: T15; font-size: 0.0px; font-style: normal; font-weight: normal; color: #000000; }
  .f34 { font-family: T6; font-size: 0.0px; font-style: normal; font-weight: normal; color: #000000; }
  .f35 { font-family: T5; font-size: 0.0px; font-style: normal; font-weight: normal; color: #000000; }
  .f36 { font-family: T3; font-size: 0.0px; font-style: normal; font-weight: normal; color: #000000; }
  .f37 { font-family: T12; font-size: 0.0px; font-style: normal; font-weight: normal; color: #000000; }
  .f38 { font-family: T4; font-size: 0.0px; font-style: normal; font-weight: normal; color: #000000; }
  .f39 { font-family: T2; font-size: 0.0px; font-style: normal; font-weight: normal; color: #000000; }
  .f40 { font-family: T1; font-size: 0.0px; font-style: normal; font-weight: normal; color: #000000; }
</style></body></html>